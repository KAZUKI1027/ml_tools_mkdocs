{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"General \u00b6 \u672c\u30b5\u30a4\u30c8\u306e\u5229\u7528\u306b\u95a2\u3057\u3066 \u00b6 \u672c\u30b5\u30a4\u30c8\u306f\u30d6\u30ed\u30b0\u300c brain snacks \u300d\u3092\u904b\u55b6\u3059\u308b\u8457\u8005\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u95a2\u3059\u308b\u30ce\u30a6\u30cf\u30a6\u3092\u96c6\u7d04\u3059\u308b\u76ee\u7684\u3067\u958b\u8a2d\u3057\u307e\u3057\u305f\u3002 \u4e3b\u306b\u4ee5\u4e0b\u306b\u95a2\u3057\u3066\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u53d6\u308a\u307e\u3068\u3081\u3066\u3044\u307e\u3059\u3002 Git Command\u306b\u95a2\u3057\u3066 \u30c7\u30fc\u30bf\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0(Numpy/Pandas) \u30c7\u30fc\u30bf\u53ef\u8996\u5316\u306e\u624b\u6cd5(Matplotlib/Seaborn) \u6a5f\u68b0\u5b66\u7fd2/\u6df1\u5c64\u5b66\u7fd2 (\u30b3\u30f3\u30da\u53c2\u52a0\u306e\u8a18\u9332) \u56e0\u679c\u63a8\u8ad6\u3001WebScraping\u95a2\u9023 \u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u7121\u65ad\u8ee2\u8f09\u3092\u9664\u304d\u3001\u5229\u7528\u306b\u95a2\u3057\u3066\u5236\u9650\u306f\u3042\u308a\u307e\u305b\u3093\u306e\u3067\u3054\u81ea\u7531\u306b\u3054\u6d3b\u7528\u304f\u3060\u3055\u3044\u3002 \u304a\u554f\u3044\u5408\u308f\u305b \u00b6 \u304a\u4ed5\u4e8b\u306e\u3054\u4f9d\u983c\u3001\u554f\u3044\u5408\u308f\u305b\u306f\u300c \u554f\u3044\u5408\u308f\u305b\u30d5\u30a9\u30fc\u30e0 \u300d\u304b\u3089\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002","title":"General"},{"location":"#general","text":"","title":"General"},{"location":"#_1","text":"\u672c\u30b5\u30a4\u30c8\u306f\u30d6\u30ed\u30b0\u300c brain snacks \u300d\u3092\u904b\u55b6\u3059\u308b\u8457\u8005\u306e\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9\u306b\u95a2\u3059\u308b\u30ce\u30a6\u30cf\u30a6\u3092\u96c6\u7d04\u3059\u308b\u76ee\u7684\u3067\u958b\u8a2d\u3057\u307e\u3057\u305f\u3002 \u4e3b\u306b\u4ee5\u4e0b\u306b\u95a2\u3057\u3066\u306e\u30b5\u30f3\u30d7\u30eb\u30b3\u30fc\u30c9\u3092\u53d6\u308a\u307e\u3068\u3081\u3066\u3044\u307e\u3059\u3002 Git Command\u306b\u95a2\u3057\u3066 \u30c7\u30fc\u30bf\u30cf\u30f3\u30c9\u30ea\u30f3\u30b0(Numpy/Pandas) \u30c7\u30fc\u30bf\u53ef\u8996\u5316\u306e\u624b\u6cd5(Matplotlib/Seaborn) \u6a5f\u68b0\u5b66\u7fd2/\u6df1\u5c64\u5b66\u7fd2 (\u30b3\u30f3\u30da\u53c2\u52a0\u306e\u8a18\u9332) \u56e0\u679c\u63a8\u8ad6\u3001WebScraping\u95a2\u9023 \u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u7121\u65ad\u8ee2\u8f09\u3092\u9664\u304d\u3001\u5229\u7528\u306b\u95a2\u3057\u3066\u5236\u9650\u306f\u3042\u308a\u307e\u305b\u3093\u306e\u3067\u3054\u81ea\u7531\u306b\u3054\u6d3b\u7528\u304f\u3060\u3055\u3044\u3002","title":"\u672c\u30b5\u30a4\u30c8\u306e\u5229\u7528\u306b\u95a2\u3057\u3066"},{"location":"#_2","text":"\u304a\u4ed5\u4e8b\u306e\u3054\u4f9d\u983c\u3001\u554f\u3044\u5408\u308f\u305b\u306f\u300c \u554f\u3044\u5408\u308f\u305b\u30d5\u30a9\u30fc\u30e0 \u300d\u304b\u3089\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002","title":"\u304a\u554f\u3044\u5408\u308f\u305b"},{"location":"cas_inf/","text":"Stats Model \u00b6 \u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f \u00b6 import numpy as np import scipy as sp import pandas as pd from pandas import Series , DataFrame from scipy import stats import statsmodels.api as sm from sklearn.linear_model import LogisticRegression , LinearRegression import random import japanize_matplotlib import copy from sklearn.covariance import GraphicalLasso from sklearn.covariance import GraphicalLassoCV import pydot from IPython.display import Image , display # \u53ef\u8996\u5316\u30e9\u30a4\u30d6\u30e9\u30ea import matplotlib.pyplot as plt import matplotlib as mpl import seaborn as sns from statsmodels.graphics.factorplots import interaction_plot % matplotlib inline pd . set_option ( 'display.max_columns' , 100 ) #plt.style.use('seaborn-darkgrid') from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.metrics import classification_report , roc_curve , confusion_matrix , roc_auc_score from sklearn.metrics import mean_squared_error , silhouette_samples from sklearn.cluster import KMeans # \u5c0f\u6570\u7b2c3\u4f4d\u307e\u3067\u8868\u793a % precision 3 # \u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u306e\u56fa\u5b9a np . random . seed ( 0 ) \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f \u00b6 input_dir = \"/content/drive/MyDrive/11_Python/casual_inference/input/\" output_dir = \"/content/drive/MyDrive/11_Python/casual_inference/output/\" pd . set_option ( \"display.max_colwidth\" , 50 ) df = pd . read_csv ( input_dir + \"sample.csv\" ) print ( df . shape ) df . head () \u57fa\u790e\u96c6\u8a08 \u00b6 sns . set ( font = \"IPAexGothic\" ) fig = plt . figure ( figsize = ( 7 , 7 )) ax = fig . add_subplot ( 1 , 1 , 1 ) ax = sns . barplot ( x = 'cm_dummy' , y = 'gamesecond' , data = df , ax = ax ) ax . set_xticklabels ([ 'CM\u898b\u3066\u306a\u3044(z=0)' , 'CM\u898b\u305f(z=1)' ]) ax . set_xlabel ( '' ) ax . set_ylabel ( '\u5e73\u5747\u30b2\u30fc\u30e0\u5229\u7528\u6642\u9593 [\u79d2]' ); \u30e2\u30c7\u30eb \u00b6 target_col = 'cm_dummy' tags_covariate = [ 'gamedummy' , 'area_kanto' , 'area_keihan' , 'area_tokai' , 'area_keihanshin' , 'age' , 'sex' , 'marry_dummy' , 'job_dummy1' , 'job_dummy2' , 'job_dummy3' , 'job_dummy4' , 'job_dummy5' , 'job_dummy6' , 'job_dummy7' , 'job_dummy8' , 'inc' , 'pmoney' , 'fam_str_dummy1' , 'fam_str_dummy2' , 'fam_str_dummy3' , 'fam_str_dummy4' , 'fam_str_dummy5' , 'child_dummy' , 'TVwatch_day' , 'gamesecond' ] df_cm_0 = df [ df . cm_dummy == 0 ] . reset_index ( drop = True ) print ( df_cm_0 . shape ) df_cm_1 = df [ df . cm_dummy == 1 ] . reset_index ( drop = True ) print ( df_cm_1 . shape ) AXES = [ \"ax_ {} \" . format ( i ) for i in range ( len ( tags_covariate ))] figsize = ( 21 , 5 * len ( tags_covariate )) fig = plt . figure ( figsize = figsize ) for i in range ( len ( tags_covariate )): array0 = df_cm_0 [ tags_covariate [ i ]] . values array1 = df_cm_1 [ tags_covariate [ i ]] . values AXES [ i ] = plt . subplot2grid (( len ( tags_covariate ), 1 ), ( i , 0 ), rowspan = 1 , colspan = 1 ) dum_ax = AXES [ i ] dum_ax . hist ( array0 , density = True , bins = 50 , alpha = 0.4 , label = \"with CM\" ) dum_ax . hist ( array1 , density = True , bins = 50 , alpha = 0.4 , label = \"w/o CM\" ) dum_ax . set_title ( tags_covariate [ i ], fontsize = 16 ) dum_ax . legend ( fontsize = 16 ) \u50be\u5411\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0 (\u30ed\u30b8\u30b9\u30c6\u30a3\u30af\u56de\u5e30\uff09 \u00b6 X = df [ tags_covariate ] . values y = df . cm_dummy . values clf = LogisticRegression ( solver = \"lbfgs\" , C = 0.1 ) #clf = GradientBoostingClassifier() X = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) clf . fit ( X , y ) y_pred = clf . predict ( X ) y_pred_proba = clf . predict_proba ( X ) fpr , tpr , thresholds = roc_curve ( y , y_pred_proba [:, 1 ]) #print(classification_report(y, y_pred)) print ( confusion_matrix ( y , y_pred , labels = [ 0 , 1 ])) fig , ax = plt . subplots ( ncols = 1 , figsize = ( 7 , 7 )) ax . plot ( fpr , tpr , marker = \"o\" ) ax . set_xlim ([ 0.0 , 1.0 ]) ax . set_ylim ([ 0.0 , 1.0 ]) ax . set_xlabel ( \"False Positive\" , fontsize = 16 ) ax . set_ylabel ( \"True Positive\" , fontsize = 16 ) plt . title ( \"AUC : {} \" . format ( roc_auc_score ( y , y_pred_proba [:, 1 ])), fontsize = 16 ) tags_ps = [ 'cm_dummy' , 'gamedummy' , 'area_kanto' , 'area_keihan' , 'area_tokai' , 'area_keihanshin' , 'age' , 'sex' , 'marry_dummy' , 'job_dummy1' , 'job_dummy2' , 'job_dummy3' , 'job_dummy4' , 'job_dummy5' , 'job_dummy6' , 'job_dummy7' , 'job_dummy8' , 'inc' , 'pmoney' , 'fam_str_dummy1' , 'fam_str_dummy2' , 'fam_str_dummy3' , 'fam_str_dummy4' , 'fam_str_dummy5' , 'child_dummy' , 'TVwatch_day' , 'gamesecond' ] df = df [ tags_ps ] . copy () df [ \"ps\" ] = clf . predict_proba ( X )[:, 1 ] df . head () figsize = ( 10 , 8 ) fig = plt . figure ( figsize = figsize ) #\u53ef\u8996\u5316 propensity0 = df . ps [ df . cm_dummy == 0 ] propensity1 = df . ps [ df . cm_dummy == 1 ] bins = np . arange ( 0 , 1.05 , 0.05 ) top0 , _ = np . histogram ( propensity0 , bins = bins ) top1 , _ = np . histogram ( propensity1 , bins = bins ) plt . ylim ( - 1000 , 1000 ) plt . axhline ( 0 , c = \"black\" ) plt . bar ( bins [: - 1 ] + 0.025 , top0 , width = 0.04 , facecolor = \"tomato\" , label = \"0\" ) plt . bar ( bins [: - 1 ] + 0.025 , - top1 , width = 0.04 , facecolor = \"cornflowerblue\" , label = \"1\" ) plt . axhline ( 0 , c = \"black\" ) for x , y in zip ( bins , top0 ): plt . text ( x + 0.025 , y + 10 , str ( y ), ha = \"center\" , va = \"bottom\" ) for x , y in zip ( bins , top1 ): plt . text ( x + 0.025 , - y - 10 , str ( y ), ha = \"center\" , va = \"top\" ) plt . title ( \"Propensity Score\" , fontsize = 20 ) plt . legend ( fontsize = 15 ) \u30de\u30c3\u30c1\u30f3\u30b0 \u00b6 #\u30de\u30c3\u30c1\u30f3\u30b0\u306e\u7bc4\u56f2\u3092\u6307\u5b9a inter = np . arange ( 0 , 1 , 0.005 ) #\u30de\u30c3\u30c1\u30f3\u30b0\u5f8c\u306e\u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b\u305f\u3081\u306e\u304b\u3089\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0 dat_match = pd . DataFrame () #\u4e00\u3064\u76ee\u306e\u6761\u4ef6\u3067\u3001CM0\u3068CM1\u306e\u3069\u3061\u3089\u304b\u4e00\u65b9\u304c0\u4eba\u306e\u5834\u5408\u3092\u9664\u5916\u3059\u308b\u3002 for i in inter : match0 = df [( df . cm_dummy == 0 ) & ( i - 0.005 < df . ps ) & ( df . ps < i )] match1 = df [( df . cm_dummy == 1 ) & ( i - 0.005 < df . ps ) & ( df . ps < i )] #\u4e8c\u3064\u76ee\u306e\u6761\u4ef6\u3067\u4eba\u6570\u304c\u5c11\u306a\u3044\u65b9\u306b\u5408\u308f\u305b\u3066\u30e9\u30f3\u30c0\u30e0\u306b\u30de\u30c3\u30c1\u30f3\u30b0\u3059\u308b if ( match0 . shape [ 0 ] != 0 ) & ( match1 . shape [ 0 ] != 0 ): if match0 . shape [ 0 ] <= match1 . shape [ 0 ]: match1 = match1 . iloc [ random . sample ( range ( 0 , match1 . shape [ 0 ]), match0 . shape [ 0 ]),:] dat_match = pd . concat ([ dat_match , match0 , match1 ]) else : match0 = match0 . iloc [ random . sample ( range ( 0 , match0 . shape [ 0 ]), match1 . shape [ 0 ]),:] dat_match = pd . concat ([ dat_match , match0 , match1 ]) figsize = ( 10 , 8 ) fig = plt . figure ( figsize = figsize ) propensity0 = dat_match . ps [ dat_match . cm_dummy == 0 ] propensity1 = dat_match . ps [ dat_match . cm_dummy == 1 ] bins = np . arange ( 0 , 1.05 , 0.05 ) top0 , _ = np . histogram ( propensity0 , bins = bins ) top1 , _ = np . histogram ( propensity1 , bins = bins ) plt . ylim ( - 1000 , 1000 ) plt . axhline ( 0 , c = \"black\" ) plt . bar ( bins [: - 1 ] + 0.025 , top0 , width = 0.04 , facecolor = \"tomato\" , label = \"0\" ) plt . bar ( bins [: - 1 ] + 0.025 , - top1 , width = 0.04 , facecolor = \"cornflowerblue\" , label = \"1\" ) plt . axhline ( 0 , c = \"black\" ) for x , y in zip ( bins , top0 ): plt . text ( x + 0.025 , y + 10 , str ( y ), ha = \"center\" , va = \"bottom\" ) for x , y in zip ( bins , top1 ): plt . text ( x + 0.025 , - y - 10 , str ( y ), ha = \"center\" , va = \"top\" ) plt . title ( \"Propensity Score\" , fontsize = 20 ) plt . legend ( fontsize = 15 ) dat_match [ \"ps_gp\" ] = round ( dat_match . ps / 0.1 ) dat0 = dat_match [ dat_match . cm_dummy == 0 ] dat1 = dat_match [ dat_match . cm_dummy == 1 ] for i in range ( 2 , 10 ): res = stats . ttest_ind ( dat0 . gamesecond [ dat0 . ps_gp == i ], dat1 . gamesecond [ dat1 . ps_gp == i ]) diff = np . mean ( dat0 . gamesecond [ dat0 . ps_gp == i ]) - np . mean ( dat1 . gamesecond [ dat1 . ps_gp == i ]) col = \"ps\" + str ( i ) print ( \" {:} {:5.1f} (p: {:1.3f} )\" . format ( col , diff , res . pvalue ))","title":"Casual Inference"},{"location":"cas_inf/#stats-model","text":"","title":"Stats Model"},{"location":"cas_inf/#_1","text":"import numpy as np import scipy as sp import pandas as pd from pandas import Series , DataFrame from scipy import stats import statsmodels.api as sm from sklearn.linear_model import LogisticRegression , LinearRegression import random import japanize_matplotlib import copy from sklearn.covariance import GraphicalLasso from sklearn.covariance import GraphicalLassoCV import pydot from IPython.display import Image , display # \u53ef\u8996\u5316\u30e9\u30a4\u30d6\u30e9\u30ea import matplotlib.pyplot as plt import matplotlib as mpl import seaborn as sns from statsmodels.graphics.factorplots import interaction_plot % matplotlib inline pd . set_option ( 'display.max_columns' , 100 ) #plt.style.use('seaborn-darkgrid') from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import GradientBoostingClassifier from sklearn.linear_model import LinearRegression , LogisticRegression from sklearn.metrics import classification_report , roc_curve , confusion_matrix , roc_auc_score from sklearn.metrics import mean_squared_error , silhouette_samples from sklearn.cluster import KMeans # \u5c0f\u6570\u7b2c3\u4f4d\u307e\u3067\u8868\u793a % precision 3 # \u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u306e\u56fa\u5b9a np . random . seed ( 0 )","title":"\u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"cas_inf/#_2","text":"input_dir = \"/content/drive/MyDrive/11_Python/casual_inference/input/\" output_dir = \"/content/drive/MyDrive/11_Python/casual_inference/output/\" pd . set_option ( \"display.max_colwidth\" , 50 ) df = pd . read_csv ( input_dir + \"sample.csv\" ) print ( df . shape ) df . head ()","title":"\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f"},{"location":"cas_inf/#_3","text":"sns . set ( font = \"IPAexGothic\" ) fig = plt . figure ( figsize = ( 7 , 7 )) ax = fig . add_subplot ( 1 , 1 , 1 ) ax = sns . barplot ( x = 'cm_dummy' , y = 'gamesecond' , data = df , ax = ax ) ax . set_xticklabels ([ 'CM\u898b\u3066\u306a\u3044(z=0)' , 'CM\u898b\u305f(z=1)' ]) ax . set_xlabel ( '' ) ax . set_ylabel ( '\u5e73\u5747\u30b2\u30fc\u30e0\u5229\u7528\u6642\u9593 [\u79d2]' );","title":"\u57fa\u790e\u96c6\u8a08"},{"location":"cas_inf/#_4","text":"target_col = 'cm_dummy' tags_covariate = [ 'gamedummy' , 'area_kanto' , 'area_keihan' , 'area_tokai' , 'area_keihanshin' , 'age' , 'sex' , 'marry_dummy' , 'job_dummy1' , 'job_dummy2' , 'job_dummy3' , 'job_dummy4' , 'job_dummy5' , 'job_dummy6' , 'job_dummy7' , 'job_dummy8' , 'inc' , 'pmoney' , 'fam_str_dummy1' , 'fam_str_dummy2' , 'fam_str_dummy3' , 'fam_str_dummy4' , 'fam_str_dummy5' , 'child_dummy' , 'TVwatch_day' , 'gamesecond' ] df_cm_0 = df [ df . cm_dummy == 0 ] . reset_index ( drop = True ) print ( df_cm_0 . shape ) df_cm_1 = df [ df . cm_dummy == 1 ] . reset_index ( drop = True ) print ( df_cm_1 . shape ) AXES = [ \"ax_ {} \" . format ( i ) for i in range ( len ( tags_covariate ))] figsize = ( 21 , 5 * len ( tags_covariate )) fig = plt . figure ( figsize = figsize ) for i in range ( len ( tags_covariate )): array0 = df_cm_0 [ tags_covariate [ i ]] . values array1 = df_cm_1 [ tags_covariate [ i ]] . values AXES [ i ] = plt . subplot2grid (( len ( tags_covariate ), 1 ), ( i , 0 ), rowspan = 1 , colspan = 1 ) dum_ax = AXES [ i ] dum_ax . hist ( array0 , density = True , bins = 50 , alpha = 0.4 , label = \"with CM\" ) dum_ax . hist ( array1 , density = True , bins = 50 , alpha = 0.4 , label = \"w/o CM\" ) dum_ax . set_title ( tags_covariate [ i ], fontsize = 16 ) dum_ax . legend ( fontsize = 16 )","title":"\u30e2\u30c7\u30eb"},{"location":"cas_inf/#_5","text":"X = df [ tags_covariate ] . values y = df . cm_dummy . values clf = LogisticRegression ( solver = \"lbfgs\" , C = 0.1 ) #clf = GradientBoostingClassifier() X = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) clf . fit ( X , y ) y_pred = clf . predict ( X ) y_pred_proba = clf . predict_proba ( X ) fpr , tpr , thresholds = roc_curve ( y , y_pred_proba [:, 1 ]) #print(classification_report(y, y_pred)) print ( confusion_matrix ( y , y_pred , labels = [ 0 , 1 ])) fig , ax = plt . subplots ( ncols = 1 , figsize = ( 7 , 7 )) ax . plot ( fpr , tpr , marker = \"o\" ) ax . set_xlim ([ 0.0 , 1.0 ]) ax . set_ylim ([ 0.0 , 1.0 ]) ax . set_xlabel ( \"False Positive\" , fontsize = 16 ) ax . set_ylabel ( \"True Positive\" , fontsize = 16 ) plt . title ( \"AUC : {} \" . format ( roc_auc_score ( y , y_pred_proba [:, 1 ])), fontsize = 16 ) tags_ps = [ 'cm_dummy' , 'gamedummy' , 'area_kanto' , 'area_keihan' , 'area_tokai' , 'area_keihanshin' , 'age' , 'sex' , 'marry_dummy' , 'job_dummy1' , 'job_dummy2' , 'job_dummy3' , 'job_dummy4' , 'job_dummy5' , 'job_dummy6' , 'job_dummy7' , 'job_dummy8' , 'inc' , 'pmoney' , 'fam_str_dummy1' , 'fam_str_dummy2' , 'fam_str_dummy3' , 'fam_str_dummy4' , 'fam_str_dummy5' , 'child_dummy' , 'TVwatch_day' , 'gamesecond' ] df = df [ tags_ps ] . copy () df [ \"ps\" ] = clf . predict_proba ( X )[:, 1 ] df . head () figsize = ( 10 , 8 ) fig = plt . figure ( figsize = figsize ) #\u53ef\u8996\u5316 propensity0 = df . ps [ df . cm_dummy == 0 ] propensity1 = df . ps [ df . cm_dummy == 1 ] bins = np . arange ( 0 , 1.05 , 0.05 ) top0 , _ = np . histogram ( propensity0 , bins = bins ) top1 , _ = np . histogram ( propensity1 , bins = bins ) plt . ylim ( - 1000 , 1000 ) plt . axhline ( 0 , c = \"black\" ) plt . bar ( bins [: - 1 ] + 0.025 , top0 , width = 0.04 , facecolor = \"tomato\" , label = \"0\" ) plt . bar ( bins [: - 1 ] + 0.025 , - top1 , width = 0.04 , facecolor = \"cornflowerblue\" , label = \"1\" ) plt . axhline ( 0 , c = \"black\" ) for x , y in zip ( bins , top0 ): plt . text ( x + 0.025 , y + 10 , str ( y ), ha = \"center\" , va = \"bottom\" ) for x , y in zip ( bins , top1 ): plt . text ( x + 0.025 , - y - 10 , str ( y ), ha = \"center\" , va = \"top\" ) plt . title ( \"Propensity Score\" , fontsize = 20 ) plt . legend ( fontsize = 15 )","title":"\u50be\u5411\u30b9\u30b3\u30a2\u30de\u30c3\u30c1\u30f3\u30b0 (\u30ed\u30b8\u30b9\u30c6\u30a3\u30af\u56de\u5e30\uff09"},{"location":"cas_inf/#_6","text":"#\u30de\u30c3\u30c1\u30f3\u30b0\u306e\u7bc4\u56f2\u3092\u6307\u5b9a inter = np . arange ( 0 , 1 , 0.005 ) #\u30de\u30c3\u30c1\u30f3\u30b0\u5f8c\u306e\u30c7\u30fc\u30bf\u3092\u5165\u308c\u308b\u305f\u3081\u306e\u304b\u3089\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0 dat_match = pd . DataFrame () #\u4e00\u3064\u76ee\u306e\u6761\u4ef6\u3067\u3001CM0\u3068CM1\u306e\u3069\u3061\u3089\u304b\u4e00\u65b9\u304c0\u4eba\u306e\u5834\u5408\u3092\u9664\u5916\u3059\u308b\u3002 for i in inter : match0 = df [( df . cm_dummy == 0 ) & ( i - 0.005 < df . ps ) & ( df . ps < i )] match1 = df [( df . cm_dummy == 1 ) & ( i - 0.005 < df . ps ) & ( df . ps < i )] #\u4e8c\u3064\u76ee\u306e\u6761\u4ef6\u3067\u4eba\u6570\u304c\u5c11\u306a\u3044\u65b9\u306b\u5408\u308f\u305b\u3066\u30e9\u30f3\u30c0\u30e0\u306b\u30de\u30c3\u30c1\u30f3\u30b0\u3059\u308b if ( match0 . shape [ 0 ] != 0 ) & ( match1 . shape [ 0 ] != 0 ): if match0 . shape [ 0 ] <= match1 . shape [ 0 ]: match1 = match1 . iloc [ random . sample ( range ( 0 , match1 . shape [ 0 ]), match0 . shape [ 0 ]),:] dat_match = pd . concat ([ dat_match , match0 , match1 ]) else : match0 = match0 . iloc [ random . sample ( range ( 0 , match0 . shape [ 0 ]), match1 . shape [ 0 ]),:] dat_match = pd . concat ([ dat_match , match0 , match1 ]) figsize = ( 10 , 8 ) fig = plt . figure ( figsize = figsize ) propensity0 = dat_match . ps [ dat_match . cm_dummy == 0 ] propensity1 = dat_match . ps [ dat_match . cm_dummy == 1 ] bins = np . arange ( 0 , 1.05 , 0.05 ) top0 , _ = np . histogram ( propensity0 , bins = bins ) top1 , _ = np . histogram ( propensity1 , bins = bins ) plt . ylim ( - 1000 , 1000 ) plt . axhline ( 0 , c = \"black\" ) plt . bar ( bins [: - 1 ] + 0.025 , top0 , width = 0.04 , facecolor = \"tomato\" , label = \"0\" ) plt . bar ( bins [: - 1 ] + 0.025 , - top1 , width = 0.04 , facecolor = \"cornflowerblue\" , label = \"1\" ) plt . axhline ( 0 , c = \"black\" ) for x , y in zip ( bins , top0 ): plt . text ( x + 0.025 , y + 10 , str ( y ), ha = \"center\" , va = \"bottom\" ) for x , y in zip ( bins , top1 ): plt . text ( x + 0.025 , - y - 10 , str ( y ), ha = \"center\" , va = \"top\" ) plt . title ( \"Propensity Score\" , fontsize = 20 ) plt . legend ( fontsize = 15 ) dat_match [ \"ps_gp\" ] = round ( dat_match . ps / 0.1 ) dat0 = dat_match [ dat_match . cm_dummy == 0 ] dat1 = dat_match [ dat_match . cm_dummy == 1 ] for i in range ( 2 , 10 ): res = stats . ttest_ind ( dat0 . gamesecond [ dat0 . ps_gp == i ], dat1 . gamesecond [ dat1 . ps_gp == i ]) diff = np . mean ( dat0 . gamesecond [ dat0 . ps_gp == i ]) - np . mean ( dat1 . gamesecond [ dat1 . ps_gp == i ]) col = \"ps\" + str ( i ) print ( \" {:} {:5.1f} (p: {:1.3f} )\" . format ( col , diff , res . pvalue ))","title":"\u30de\u30c3\u30c1\u30f3\u30b0"},{"location":"general_data_analysis/","text":"General Data Analysis \u00b6 \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u53d6\u5f97 \u00b6 \u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u00b6 # import module import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import japanize_matplotlib from matplotlib_venn import venn2 import missingno as msno import math from scipy import stats import warnings \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u53d6\u5f97 (california housing price) \u00b6 \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3068\u3057\u3066 scikit learn\u306ecalifornia housing \u306e\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u8aad\u307f\u8fbc\u3080\u3002 #Calfornia hosing data\u306e\u8aad\u307f\u8fbc\u307f # \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u53d6\u5f97 data = sklearn . datasets . fetch_california_housing () df_1 = pd . DataFrame ( data . data , columns = data . feature_names ) df_1 [ \"Price\" ] = data . target #\u30c7\u30fc\u30bf\u5206\u5272 X_train_1 , X_test_1 , y_train_1 , y_test_1 = train_test_split ( df_1 . iloc [:,: - 1 ], df_1 . Price , test_size = 0.3 , random_state = 42 ) #\u30e2\u30c7\u30eb\u4f5c\u6210 tree_reg = DecisionTreeRegressor ( random_state = 0 ) tree_reg . fit ( X_train_1 , y_train_1 ) #\u4e88\u6e2c y_obs_1 = y_test_1 y_pred_1 = tree_reg . predict ( X_test \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u53d6\u5f97 (taitanic) \u00b6 \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3068\u3057\u3066taitanic\u306e\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u8aad\u307f\u8fbc\u3080\u3002 #Taitanic\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f #\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u4f5c\u6210 titanic = fetch_openml ( data_id = 40945 , as_frame = True ) df_2 = titanic . data #\u30c7\u30fc\u30bf\u5206\u5272 train_2 , test_2 = train_test_split ( df_2 , test_size = 0.5 ) \u53ef\u8996\u5316\u30e2\u30b8\u30e5\u30fc\u30eb \u00b6 \u6b20\u640d\u5024\u306e\u53ef\u8996\u5316 \u00b6 \u6b20\u640d\u5024\u3092msno\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u7528\u3057\u3066\u53ef\u8996\u5316\u3059\u308b\u3002\u3056\u3063\u304f\u308a\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u5168\u4f53\u306e\u6b20\u640d\u5024\u3001\u6b20\u640d\u5024\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u7406\u89e3\u3059\u308b\u969b\u306b\u4f7f\u7528\u3002 def missing_plot ( df : pd . DataFrame , type = \"matrix\" ): \"\"\"show missing plot by misno Args: df (pd.DataFrame): target_df type: matrix/bar/heatmap/dendrogram \"\"\" if type == \"matrix\" : msno . matrix ( df ) if type == \"bar\" : msno . bar ( df ) if type == \"heatmap\" : msno . heatmap ( df ) if type == \"dendrogram\" : msno . dendrogram ( df ) missing_plot ( df_2 , type = \"matrix\" ) missing_plot ( df_2 , type = \"bar\" ) missing_plot ( df_2 , type = \"heatmap\" ) missing_plot ( df_2 , type = \"dendrogram\" ) Matrix Bar Heatmap Dendrogram \u6b20\u640d\u5024\u306e\u53ef\u8996\u5316 \u00b6 \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3067\u6b20\u640d\u5024\u304c\u3042\u308b\u30ab\u30e9\u30e0\u540d\u3068\u305d\u306e\u6b20\u640d\u5024\u5272\u5408\u3092\u8868\u793a\u3059\u308b\u3002 def get_null_df ( df : pd . DataFrame ): \"\"\"Calculate missing data rate Args: df (pd.DataFrame): target_df Returns: col_null_df: dataframe of null_feature_rate \"\"\" col_null_df = pd . DataFrame ( columns = [ 'Column' , 'Type' , 'Total NaN' , '%' ]) col_null = df . columns [ df . isna () . any ()] . to_list () L = len ( df ) for col in col_null : T = 0 if df [ col ] . dtype == \"float64\" : T = \"Numerical\" elif df [ col ] . dtype == \"int64\" : T = \"Numerical\" else : T = \"Categorical\" nulls = len ( df [ df [ col ] . isna () == True ][ col ]) col_null_df = col_null_df . append ({ 'Column' : col , 'Type' : T , 'Total NaN' : nulls , '%' : ( nulls / L ) * 100 }, ignore_index = True ) return col_null_df get_null_df ( df_2 ) \u30e6\u30cb\u30fc\u30af\u5024\u306e\u53ef\u8996\u5316 \u00b6 \u30e6\u30cb\u30fc\u30af\u306a\u5024\u3068\u305d\u306e\u500b\u6570\u3092\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u5f62\u5f0f\u3067\u53d6\u5f97\u3059\u308b\u3002\u30c7\u30fc\u30bf\u5168\u4f53\u306e\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u5168\u4f53\u50cf\u3092\u628a\u63e1\u3057\u305f\u3044\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def get_unique_df ( df : pd . DataFrame ): \"\"\"Show unique column data Args: df (pd.dataFrame):target_df Returns: unique_df: unique_df_columns \"\"\" unique_df = pd . DataFrame ( columns = [ 'Feature' , 'Unique' , 'Count' ]) for col in df . columns : v = df [ col ] . unique () l = len ( v ) unique_df = unique_df . append ({ 'Feature' : col , 'Unique' : v , 'Count' : l }, ignore_index = True ) return unique_df get_unique_df ( df_2 ) \u30d9\u30f3\u56f3\u306e\u4f5c\u6210 \u00b6 train, test\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u3042\u308b\u5834\u5408\u305d\u308c\u305e\u308c\u306e\u9805\u76ee\u306b\u5bfe\u3059\u308b\u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5206\u5e03\u3092\u30d9\u30f3\u56f3\u3067\u53ef\u8996\u5316\u3059\u308b\u3002 \u3069\u3061\u3089\u304b\u4e00\u65b9\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3060\u3051\u306b\u3057\u304b\u306a\u3044\u9805\u76ee\u3092\u898b\u3064\u3051\u308b\u306e\u306b\u6700\u9069\u306a\u624b\u6cd5\u3002 def get_venn_plot ( train : pd . DataFrame , test : pd . DataFrame ): \"\"\"show venn plot from train/test_dataset Args: train (pd.DataFrame): target_train_df test (pd.DataFrame): target_test_df \"\"\" columns = test . columns columns_num = len ( columns ) n_cols = 4 n_rows = columns_num // n_cols + 1 fig , axes = plt . subplots ( figsize = ( n_cols * 3 , n_rows * 3 ), ncols = n_cols , nrows = n_rows ) for col , ax in zip ( columns , axes . ravel ()): venn2 ( subsets = ( set ( train [ col ] . unique ()), set ( test [ col ] . unique ())), set_labels = ( 'Train' , 'Test' ), ax = ax ) ax . set_title ( col ) fig . tight_layout () get_venn_plot ( train_2 , test_2 ) \u76f8\u95a2\u56f3\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u306e\u4f5c\u6210 \u00b6 seaborn\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3092\u5229\u7528\u3057\u3066Numeric\u30c7\u30fc\u30bf\u9593\u306e\u76f8\u95a2\u3092\u53ef\u8996\u5316\u3059\u308b\u3002 \u8aac\u660e\u5909\u6570\u9593\u306e\u95a2\u4fc2\u3092\u628a\u63e1\u3057\u305f\u3044\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def get_corr_heatmap ( train : pd . DataFrame , test : pd . DataFrame ): \"\"\"show correlation heat map Args: train (pd.DataFrame): target_train_df test (pd.DataFrame): target_test_df \"\"\" fig , axs = plt . subplots ( nrows = 2 , figsize = ( 15 , 15 )) plt . subplots_adjust ( wspace = 0.4 , hspace = 0.6 ) sns . heatmap ( train . corr ( ), ax = axs [ 0 ], annot = True , square = True , cmap = 'coolwarm' , annot_kws = { 'size' : 14 }, fmt = \"1.2f\" ) sns . heatmap ( test . corr (), ax = axs [ 1 ], annot = True , square = True , cmap = 'coolwarm' , annot_kws = { 'size' : 14 }, fmt = \"1.2f\" ) for i in range ( 2 ): axs [ i ] . tick_params ( axis = 'x' , labelsize = 14 ) axs [ i ] . tick_params ( axis = 'y' , labelsize = 14 ) axs [ 0 ] . set_title ( 'Training Set Correldef get_check_norm(x:pd.Series): \"\"\"show norm feature with Sapiro-Wilk test Args: x (pd.Series): Pandas Series \"\"\" plt . hist ( x , bins = 100 ) #\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u8868\u793a plt . pause ( .01 ) stats . probplot ( x , dist = \"norm\" , plot = plt ) #QQ\u30d7\u30ed\u30c3\u30c8\u8868\u793a plt . pause ( .01 ) p = stats . shapiro ( x ) #\u30b7\u30e3\u30d4\u30ed\u30a6\u30a3\u30eb\u30af\u691c\u5b9a print ( p ) ations ', size=15) axs [ 1 ] . set_title ( 'Test Set Correlations' , size = 15 ) plt . show () get_corr_heatmap ( X_train_1 , X_test_1 ) \u6b63\u898f\u5206\u5e03\u306e\u78ba\u8a8d \u00b6 \u6b63\u898f\u5206\u5e03\u306b\u3069\u306e\u7a0b\u5ea6\u5f93\u3063\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002 \u30b7\u30e3\u30d4\u30ed\u30a6\u30a3\u30eb\u30af\u691c\u5b9a\u306b\u3066\u6b63\u898f\u5206\u5e03\u306b\u3069\u306e\u7a0b\u5ea6\u5f93\u3046\u306e\u304b\u5b9a\u91cf\u5316\u3059\u308b\u4e8b\u304c\u3067\u304d\u308b\u3002 def get_check_norm ( x : pd . Series ): \"\"\"show norm feature with Sapiro-Wilk test Args: x (pd.Series): Pandas Series \"\"\" plt . hist ( x , bins = 100 ) #\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u8868\u793a plt . pause ( .01 ) stats . probplot ( x , dist = \"norm\" , plot = plt ) #QQ\u30d7\u30ed\u30c3\u30c8\u8868\u793a plt . pause ( .01 ) p = stats . shapiro ( x ) #\u30b7\u30e3\u30d4\u30ed\u30a6\u30a3\u30eb\u30af\u691c\u5b9a print ( p ) get_check_norm ( df_1 . Population ) yyplot \u4f5c\u6210\u95a2\u6570 \u00b6 \u4e88\u5b9f\u5224\u5b9a\u3068\u3057\u3066\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\u306e\u6bd4\u8f03\u3092\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def get_yyplot ( y_obs : list , y_pred : list ): \"\"\"show yyplot Args: y_obs (list): actual y y_pred (list]): predicted y \"\"\" yvalues = np . concatenate ([ y_obs , y_pred ]) ymin , ymax , yrange = np . amin ( yvalues ), np . amax ( yvalues ), np . ptp ( yvalues ) fig = plt . figure ( figsize = ( 8 , 8 )) plt . scatter ( y_obs , y_pred ) plt . plot ([ ymin - yrange * 0.01 , ymax + yrange * 0.01 ], [ ymin - yrange * 0.01 , ymax + yrange * 0.01 ]) plt . xlabel ( 'y_observed' , fontsize = 24 ) plt . ylabel ( 'y_predicted' , fontsize = 24 ) plt . title ( 'Observed-Predicted Plot' , fontsize = 24 ) plt . tick_params ( labelsize = 16 ) get_yyplot ( y_obs = y_obs_1 , y_pred = y_pred_1 ) \u6570\u5024\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u53ef\u8996\u5316 \u00b6 train/test\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067Numeric\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3092\u78ba\u8a8d\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 train/test\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u5927\u304d\u304f\u5206\u5e03\u304c\u7570\u306a\u308b\u5834\u5408\u306f\u8a13\u7df4\u6642\u306e\u30c7\u30fc\u30bf\u9078\u629e\u306b\u7559\u610f\u3059\u308b\u3002 def get_numeric_features_plot ( train : pd . DataFrame , test : pd . DataFrame , cont_features : list , height , figsize , hspace = .3 ): \"\"\"Show Numeric Features Distribution Args: train (pd.DataFrame): train_df test (pd.DataFrame): test_df cont_features (list): target_features height ([float]): plot_height figsize ([float]): plot_size hspace (float, optional): space of figs. Defaults to .3. \"\"\" ncols = 2 nrows = int ( math . ceil ( len ( cont_features ) / 2 )) fig , axs = plt . subplots ( ncols = ncols , nrows = nrows , figsize = ( height * 2 , height * nrows )) plt . subplots_adjust ( right = 1.5 , hspace = hspace ) for i , feature in enumerate ( cont_features ): plt . subplot ( nrows , ncols , i + 1 ) # Distribution of target features sns . distplot ( train [ feature ], label = 'Train' , hist = True , color = '#e74c3c' ) sns . distplot ( test [ feature ], label = 'Test' , hist = True , color = '#2ecc71' ) plt . xlabel ( ' {} ' . format ( feature ), size = figsize , labelpad = 15 ) plt . ylabel ( 'Density' , size = figsize , labelpad = 15 ) plt . tick_params ( axis = 'x' , labelsize = figsize ) plt . tick_params ( axis = 'y' , labelsize = figsize ) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . title ( 'Distribution of {} Feature' . format ( feature ), size = figsize , y = 1.05 ) plt . show () get_numeric_features_plot ( train = X_train_1 , test = X_test_1 , cont_features = X_train_1 . columns , height = 5 , figsize = 12 , hspace = .4 ) \u30ab\u30c6\u30b4\u30ea\u30fc\u30c7\u30fc\u30bf\u306e\u30ab\u30a6\u30f3\u30c8\u53ef\u8996\u5316 \u00b6 train/test\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067category\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3092\u78ba\u8a8d\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 def categorical_count_plot ( train : pd . DataFrame , test : pd . DataFrame , cat_features : list , height , figsize , hspace = .3 ): \"\"\"Show Numeric Features Distribution Args: train (pd.DataFrame): train_df test (pd.DataFrame): test_df cat_features (list): target_features height ([float]): plot_height figsize ([float]): plot_size hspace (float, optional): space of figs. Defaults to .3. \"\"\" ncols = 2 nrows = int ( math . ceil ( len ( cat_features ) / 2 )) train [ \"type\" ] = \"train\" test [ \"type\" ] = \"test\" whole_df = pd . concat ([ train , test ], axis = 0 ) . reset_index ( drop = True ) fig , axs = plt . subplots ( ncols = ncols , nrows = nrows , figsize = ( height * 2 , height * nrows )) plt . subplots_adjust ( right = 1.5 , hspace = hspace ) for i , feature in enumerate ( cat_features ): plt . subplot ( nrows , ncols , i + 1 ) # Distribution of target features ax = sns . countplot ( data = whole_df , x = feature , hue = \"type\" ) ax . set_xticklabels ( ax . get_xticklabels (), rotation = 90 ) plt . xlabel ( ' {} ' . format ( feature ), size = figsize , labelpad = 15 ) plt . ylabel ( 'Density' , size = figsize , labelpad = 15 ) plt . tick_params ( axis = 'x' , labelsize = figsize ) plt . tick_params ( axis = 'y' , labelsize = figsize ) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . title ( 'Count of {} Feature' . format ( feature ), size = figsize , y = 1.05 ) plt . show () #category item\u306e\u6307\u5b9a cat_items = [ 'sex' , 'boat' ] categorical_count_plot ( train = train_2 , test = test_2 , cat_features = cat_items , height = 5 , figsize = 12 , hspace = .4 ) \u5186\u30b0\u30e9\u30d5\u306e\u4f5c\u6210 \u00b6 category\u30c7\u30fc\u30bf\u306e\u69cb\u6210\u5272\u5408\u3092\u5186\u30b0\u30e9\u30d5\u3067\u53ef\u8996\u5316\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def get_pie_plot ( df : pd . DataFrame , target_feature : str , column : str , title : str ): \"\"\"show target_feature rate Args: df (pd.DataFrame): target df target_feature ([str]): target feature to show rate column ([str]): column to count target_feature title ([str]): fig titles \"\"\" plt . style . use ( 'default' ) sns . set_style ( 'whitegrid' ) sns . set ( font = \"IPAexGothic\" ) sns . set_palette ( 'Set2' ) target_df = df . groupby ( target_feature )[ column ] . count () . reset_index () labels = [] target = target_df . copy () for i in range ( len ( target )): label = target [ target_feature ][ i ] + \" [\" + str ( round ( target [ column ][ i ] * 100 / target [ column ] . sum (), 1 )) + \" %\" + \"]\" labels . append ( label ) x = labels x = target [ target_feature ] y = target [ column ] fig = plt . figure ( figsize = ( 50 , 5 )) ax = fig . add_subplot ( 1 , 2 , 1 ) ax . pie ( y , counterclock = False , startangle = 90 , rotatelabels = False , labels = x ); plt . title ( title ) plt . legend ( labels , loc = \"lower left\" ); get_pie_plot ( df = df_2 , target_feature = \"target\" , column = \"sex\" , title = \"Rate of sex\" ) \u30ab\u30c6\u30b4\u30ea\u30fc\u306e\u30c7\u30fc\u30bf\u306eBoxPlot\u4f5c\u6210 \u00b6 \u30ab\u30c6\u30b4\u30ea\u30fc\u5909\u6570\u306b\u5bfe\u3059\u308btarget(Numeric)\u5024\u306e\u7bc4\u56f2\u3092boxplot\u3067\u53ef\u8996\u5316\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def get_cat_boxplot ( df , x , y , title , order : list , height , hue = None ): \"\"\"Show box plot Args: df ([type]): target_df x ([type]): target_x y ([type]): target_y title ([type]): fig title order (list): BoxPlot_order hue ([type], optional): show hue. Defaults to None. \"\"\" fig = plt . figure ( figsize = ( height * 1.5 , height )) plt . style . use ( 'default' ) sns . set_style ( 'whitegrid' ) sns . set ( font = \"IPAexGothic\" ) sns . set_palette ( 'Set2' ) sns . set_style ( { \"xtick.major.size\" : 4 , \"ytick.major.size\" : 4 }) ax = plt . gca () if hue is None : g = sns . boxplot ( data = df , x = x , y = y , sym = \"\" , order = order ) else : g = sns . boxplot ( data = df , x = x , y = y , sym = \"\" , order = order , hue = hue ) g . set_title ( title ) labels = ax . get_xticklabels () plt . setp ( labels , rotation = 90 ); get_cat_boxplot ( df = df_2 , x = \"sex\" , y = \"age\" , title = \"age boxplot of each sex\" , order = [ \"male\" , \"female\" ], height = 8 ) \u524d\u51e6\u7406\u30e2\u30b8\u30e5\u30fc\u30eb \u00b6 \u6a19\u6e96\u5316 \u00b6 Numeric\u30c7\u30fc\u30bf\u3092\u6a19\u6e96\u5316\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 def StandardScaler ( input_df : pd . DataFrame , target_cols : list ): \"\"\"Standard Scaling Args: input_df (pd.DataFrame): input dataframe target_cols (list): target columns Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] scaler = preprocessing . StandardScaler () . fit ( features . values ) features = scaler . transform ( features . values ) input_df [ target_cols ] = features output_df = input_df . copy () return output_df #target columns target_cols = df_1 . columns StandardScaler ( input_df = df_1 , target_cols = target_cols ) \u6b63\u898f\u5316 (Min/Max) \u00b6 Numeric\u30c7\u30fc\u30bf\u3092\u6b63\u898f\u5316\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 def MinMaxScaler ( input_df : pd . DataFrame , target_cols : list ): \"\"\"MinMax Scaling Args: input_df (pd.DataFrame): input dataframe target_cols (list): target columns Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] scaler = preprocessing . MinMaxScaler () . fit ( features . values ) features = scaler . transform ( features . values ) input_df [ target_cols ] = features output_df = input_df . copy () return output_df #target columns target_cols = [ \"MedInc\" , \"HouseAge\" , \"AveRooms\" ] df_minmax = df_1 [ target_cols ] . copy () MinMaxScaler ( input_df = df_minmax , target_cols = target_cols ) \u6b63\u898f\u5316 (L1/L2 Norm) \u00b6 Numeric\u30c7\u30fc\u30bf\u3092(L1/L2 Norm)\u3067\u6b63\u898f\u5316\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 def Normalizer ( input_df : pd . DataFrame , target_cols : list , norm = \"l2\" ): \"\"\"_summary_ Args: input_df (pd.DataFrame): input dataframe target_cols (list): target columns norm (list, optional): select [\"l2\", \"l2\", \"max\"]. Default is \"l2\" Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] scaler = preprocessing . Normalizer ( norm = norm ) . fit ( features . values ) features = scaler . transform ( features . values ) input_df [ target_cols ] = features output_df = input_df . copy () return output_df #target columns target_cols = [ \"MedInc\" , \"HouseAge\" , \"AveRooms\" ] df_nor = df_1 [ target_cols ] . copy () Normalizer ( input_df = df_nor , target_cols = target_cols , norm = \"l1\" ) \u6a19\u6e96\u5316 (Robust Scaler) \u00b6 Numeric\u30c7\u30fc\u30bf\u3092Robust Scaler\u3067\u6a19\u6e96\u5316\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 \u5916\u308c\u5024\u306e\u5f71\u97ff\u304c\u5927\u304d\u3044\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def RobustScaler ( input_df : pd . DataFrame , target_cols : list ): \"\"\"Robust Scaler Args: input_df (pd.DataFrame): input Dataframe target_cols (list): target columns Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] scaler = preprocessing . RobustScaler () . fit ( features . values ) features = scaler . transform ( features . values ) input_df [ target_cols ] = features output_df = input_df . copy () return output_df #target columns target_cols = [ \"MedInc\" , \"HouseAge\" , \"AveRooms\" ] df_rob = df_1 [ target_cols ] . copy () RobustScaler ( input_df = df_rob , target_cols = target_cols ) \u30ab\u30b9\u30bf\u30e0\u95a2\u6570\u306e\u5b9f\u88c5 \u00b6 \u6307\u5b9a\u30ab\u30e9\u30e0\u306b\u72ec\u81ea\u306e\u95a2\u6570\u3092\u3042\u3066\u304c\u3044\u3001\u65b0\u305f\u306b\u8aac\u660e\u5909\u6570\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def FunctionTransformer ( input_df : pd . DataFrame , target_cols : list , function ): \"\"\"Apply function to dataframe Args: input_df (pd.DataFrame): input dataframe target_cols (list): target columns function : function Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] scaler = preprocessing . FunctionTransformer ( function ) . fit ( features . values ) features = scaler . transform ( features . values ) input_df [ target_cols ] = features output_df = input_df . copy () return output_df #target columns target_cols = [ \"MedInc\" , \"HouseAge\" , \"AveRooms\" ] df_func = df_1 [ target_cols ] . copy () #define function def apply_func ( x ): x = x * 2 return x FunctionTransformer ( input_df = df_func , target_cols = target_cols , function = apply_func ) \u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0 \u00b6 \u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3066\u6307\u5b9a\u8aac\u660e\u5909\u6570\u306e\u6570\u3092\u6570\u3048\u3001\u65b0\u305f\u306a\u8aac\u660e\u5909\u6570\u3092\u8ffd\u52a0\u3059\u308b\u3002 def CountEncoding ( target_cols , input_df : pd . DataFrame ): \"\"\"CountEncoding Args: target_cols (category): target columns input_df (pd.DataFrame): input dataframe Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] encoder = ce . CountEncoder () . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"CE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) return output_df target_cols = [ \"sex\" ] output_df = CountEncoding ( target_cols = target_cols , input_df = df_2 ) output_df [[ \"sex\" , \"CE_sex\" , \"target\" ]] \u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0 \u00b6 \u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3066category\u5909\u6570\u3092\u5909\u63db\u3059\u308b\u3002 def LabelEncoding ( target_cols , input_df : pd . DataFrame ): \"\"\"Labelencoding Args: target_cols (category): target columns input_df (pd.DataFrame): input dataframe Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] encoder = ce . OrdinalEncoder () . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"LE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) return output_df target_cols = [ \"sex\" ] output_df = LabelEncoding ( target_cols = target_cols , input_df = df_2 ) output_df [[ \"sex\" , \"LE_sex\" , \"target\" ]] OneHot\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0 \u00b6 OneHot\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3066category\u5909\u6570\u3092\u5909\u63db\u3059\u308b\u3002 def OneHotEncoding ( target_cols , input_df : pd . DataFrame ): \"\"\"OneHotEncoding Args: target_cols (category): target columns input_df (pd.DataFrame): input dataframe Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] encoder = ce . OneHotEncoder ( use_cat_names = True ) . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = output_df . columns . str [ 2 :] output_df = output_df . add_prefix ( \"OHE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) return output_df target_cols = [ \"sex\" ] output_df = OneHotEncoding ( target_cols = target_cols , input_df = df_2 ) output_df [[ \"sex\" , \"OHE_female\" , \"OHE_male\" , \"target\" ]] \u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0 \u00b6 \u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3066\u76ee\u7684\u5909\u6570\u306b\u5bfe\u3057\u3066\u306ecategory\u5909\u6570\u306e\u5e73\u5747\u3092\u53d6\u5f97\u3059\u308b\u3002 def TargetEncoding ( target_cols , input_df : pd . DataFrame , target ): \"\"\"TargetEncoding Args: target_cols (category): target columns input_df (pd.DataFrame): input dataframe target (str): column name Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] encoder = ce . TargetEncoder () . fit ( features . values , target ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"TE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) return output_df target_cols = [ \"sex\" ] output_df = TargetEncoding ( target_cols = target_cols , input_df = df_2 , target = df_2 [ \"age\" ]) output_df [[ \"sex\" , \"TE_sex\" , \"age\" ]] \u56db\u5247\u6f14\u7b97\u51e6\u7406 \u00b6 \u6307\u5b9acolumn\u306e\u56db\u5247\u6f14\u7b97\u306b\u3066\u65b0\u898f\u306b\u8aac\u660e\u5909\u6570\u3092\u8ffd\u52a0\u3059\u308b\u3002 def ArithmeticOperation ( input_df , target_column1 : str , target_column2 : str , operation : str ): \"\"\"Arithmetic operation Args: input_df (pd.DataFrame): input dataframe target_column1 (str): target columns 1 target_column2 (str): target columns 2 operation (str): select [\"+\",\"-\",\"*\",\"/\"] Returns: output_df (pd.DataFrame): output dataframe \"\"\" output_df = input_df . copy () output_df_columns_name = f ' { target_column1 }{ operation }{ target_column2 } ' if operation == \"+\" : output_df [ output_df_columns_name ] = output_df [ target_column1 ] + \\ output_df [ target_column2 ] elif operation == \"-\" : output_df [ output_df_columns_name ] = output_df [ target_column1 ] - \\ output_df [ target_column2 ] elif operation == \"*\" : output_df [ output_df_columns_name ] = output_df [ target_column1 ] * \\ output_df [ target_column2 ] elif operation == \"/\" : output_df [ output_df_columns_name ] = output_df [ target_column1 ] / \\ output_df [ target_column2 ] output_df = pd . concat ( [ input_df , output_df [ output_df_columns_name ]], axis = 1 ) return output_df target_cols = [ \"AveRooms\" , \"AveBedrms\" ] output_df = ArithmeticOperation ( input_df = df_1 , target_column1 = \"AveRooms\" , target_column2 = \"AveBedrms\" , operation = \"+\" ) output_df [[ \"AveRooms\" , \"AveBedrms\" , \"AveRooms+AveBedrms\" ]] \u30ab\u30e9\u30e0\u7d50\u5408 \u00b6 category\u5909\u6570\u306b\u5bfe\u3057\u3066\u6307\u5b9a\u306eNumeric\u5909\u6570\u306e\u5e73\u5747\u3001\u6700\u5927\u5024\u3001\u6700\u5c0f\u5024\u3001\u5408\u8a08\u3092\u7b97\u51fa\u3002\u65b0\u898f\u306b\u8aac\u660e\u5909\u6570\u3068\u3057\u3066\u8ffd\u52a0\u3059\u308b\u3002 def AggregateOperation ( input_df : pd . DataFrame , key : str , agg_column : str , agg_funcs : list , fillna = None ): \"\"\"Aggregate Operation Args: input_df (pd.DataFrame): input DataFrame key (str): category column agg_column (str): _numeric column agg_funcs (list): select [\"sum\",\"mean\",\"max\",\"min\"] fillna (optional): Defaults to None. Returns: output_df (pd.DataFrame): output dataframe \"\"\" if fillna : input_df [ agg_column ] = input_df [ agg_column ] . fillna ( fillna ) group_df = input_df . groupby ( key ) . agg ({ agg_column : agg_funcs }) . reset_index () column_names = [ f 'GP_ { agg_column } @ { key } _ { agg_func } ' for agg_func in agg_funcs ] group_df . columns = [ key ] + column_names output_df = pd . merge ( input_df [ key ], group_df , on = key , how = \"left\" ) . drop ( columns = [ key ]) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) return output_df output_df = AggregateOperation ( input_df = df_2 , key = \"sex\" , agg_column = \"age\" , agg_funcs = [ \"sum\" , \"mean\" , \"max\" , \"min\" ]) output_df [[ \"sex\" , \"age\" , \"GP_age@sex_sum\" , \"GP_age@sex_mean\" , \"GP_age@sex_max\" , \"GP_age@sex_min\" ]] \u30b0\u30e9\u30d5\u30a3\u30ab\u30eb\u30e9\u30c3\u30bd\u306b\u3088\u308b\u504f\u76f8\u95a2\u53ef\u8996\u5316 \u00b6 \u30b0\u30e9\u30d5\u30a3\u30ab\u30eb\u30e9\u30c3\u30bd\u306b\u3066\u5404\u8aac\u660e\u5909\u6570\u9593\u306e\u76f8\u95a2\u95a2\u4fc2\u3092\u53ef\u8996\u5316\u3059\u308b\u3002 from sklearn.covariance import GraphicalLasso from sklearn.covariance import GraphicalLassoCV import pydot from IPython.display import Image , display # \u53ef\u8996\u5316\u30e9\u30a4\u30d6\u30e9\u30ea import matplotlib.pyplot as plt import matplotlib as mpl import seaborn as sns from statsmodels.graphics.factorplots import interaction_plot def execute_graphical_lassoCV ( X , alphas = 4 , verbose = True ): X_std = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) glasso = GraphicalLassoCV ( alphas = alphas , verbose = verbose ) glasso . fit ( X_std ) print ( \"alpha = \" , glasso . alpha_ ) return glasso def execute_graphical_lasso ( X , alpha = 0.2 , verbose = True ): X_std = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) glasso = GraphicalLasso ( alpha = alpha , verbose = verbose ) glasso . fit ( X_std ) return glasso def Disp_Gaussian_Graphical_Model ( model , feature_names ): \"\"\" GraphLasso\u306b\u3088\u308b\u5909\u6570\u9593\u306e\u95a2\u4fc2\u306e\u30b0\u30e9\u30d5\u5316@\u77e5\u8b58\u306e\u30b5\u30e9\u30c0\u30dc\u30a6\u30eb \u30d0\u30b0\u4fee\u6b63\u3057\u3066\u4f7f\u7528 \"\"\" pre_ = model . precision_ #\u30b9\u30d1\u30fc\u30b9\u5316\u3057\u305f\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u9006\u884c\u5217 #\u30b0\u30e9\u30d5\u8868\u793a\u306e\u305f\u3081\u306b\u5bfe\u89d2\u6210\u5206\u306f0\u306b\u3059\u308b pre_zero = pre_ - np . diag ( np . diag ( pre_ )) #\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u9006\u884c\u5217(\u2252\u96a3\u63a5\u884c\u5217)\u304b\u3089\u30b0\u30e9\u30d5\u3092\u751f\u6210\u3059\u308b g = pydot . Dot ( graph_type = 'graph' ) df = pd . DataFrame ( index = feature_names . tolist (), columns = feature_names . tolist (), data = pre_zero . tolist ()) #\u30ce\u30fc\u30c9\u3092\u8ffd\u52a0 for c in df . columns : node = pydot . Node ( c ) g . add_node ( node ) #\u30a8\u30c3\u30b8\u3092\u8ffd\u52a0 for i in df . index : for c in df . columns : if c >= i : #print(i, c) #\u30a8\u30c3\u30b8\u306e\u7d76\u5bfe\u5024\u304c0.1\u3088\u308a\u5927\u304d\u3044\u3068\u304d\u306b\u8868\u793a(\u3053\u3053\u306f\u597d\u307f\u306b\u3088\u308b) if abs ( df . loc [ i , c ]) > 0.1 : if df . loc [ i , c ] > 0.0 : style = \"solid\" else : style = \"dashed\" edge = pydot . Edge ( g . get_node ( i )[ 0 ], g . get_node ( c )[ 0 ], penwidth = 5 * abs ( df . loc [ i , c ]), style = style ) edge . set_label ( ' {:.1f} ' . format ( df . loc [ i , c ])) g . add_edge ( edge ) #\u7d50\u5408\u6570\u306b\u3088\u3063\u3066\u8272\u3092\u5909\u3048\u308b nodeList = g . get_node_list () num_edges = np . sum ( abs ( pre_zero ) > 0.1 , axis = 0 ) max_num_edges = max ( np . amax ( num_edges ), 1 ) for i , tag in enumerate ( df . index ): node = g . get_node ( tag )[ 0 ] # H S V\u306e\u9806 S\u3092\u5909\u66f4 color = str ( 0.3 ) + \" \" + str ( num_edges [ i ] / max_num_edges ) + \" \" + str ( 1 ) node . set_color ( \"black\" ) #node\u306e\u8f2a\u90ed\u306e\u8272 node . set_fillcolor ( color ) #node\u306e\u4e2d\u8eab\u306e\u8272 node . set_style ( 'filled' ) # \u30b0\u30e9\u30d5\u3092\u51fa\u529b #g.write_png('LassoEstimatedGGM.png', prog='neato') g . write_png ( 'LassoEstimatedGGM.png' ) glasso = execute_graphical_lasso ( df_1 . values , alpha = 0.3 ) Disp_Gaussian_Graphical_Model ( glasso , np . array ( df_1 . columns )) display ( Image ( 'LassoEstimatedGGM.png' ))","title":"General Data Analysis"},{"location":"general_data_analysis/#general-data-analysis","text":"","title":"General Data Analysis"},{"location":"general_data_analysis/#_1","text":"","title":"\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u53d6\u5f97"},{"location":"general_data_analysis/#_2","text":"# import module import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import japanize_matplotlib from matplotlib_venn import venn2 import missingno as msno import math from scipy import stats import warnings","title":"\u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"general_data_analysis/#california-housing-price","text":"\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3068\u3057\u3066 scikit learn\u306ecalifornia housing \u306e\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u8aad\u307f\u8fbc\u3080\u3002 #Calfornia hosing data\u306e\u8aad\u307f\u8fbc\u307f # \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u53d6\u5f97 data = sklearn . datasets . fetch_california_housing () df_1 = pd . DataFrame ( data . data , columns = data . feature_names ) df_1 [ \"Price\" ] = data . target #\u30c7\u30fc\u30bf\u5206\u5272 X_train_1 , X_test_1 , y_train_1 , y_test_1 = train_test_split ( df_1 . iloc [:,: - 1 ], df_1 . Price , test_size = 0.3 , random_state = 42 ) #\u30e2\u30c7\u30eb\u4f5c\u6210 tree_reg = DecisionTreeRegressor ( random_state = 0 ) tree_reg . fit ( X_train_1 , y_train_1 ) #\u4e88\u6e2c y_obs_1 = y_test_1 y_pred_1 = tree_reg . predict ( X_test","title":"\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u53d6\u5f97 (california housing price)"},{"location":"general_data_analysis/#taitanic","text":"\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u3068\u3057\u3066taitanic\u306e\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u8aad\u307f\u8fbc\u3080\u3002 #Taitanic\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f #\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u4f5c\u6210 titanic = fetch_openml ( data_id = 40945 , as_frame = True ) df_2 = titanic . data #\u30c7\u30fc\u30bf\u5206\u5272 train_2 , test_2 = train_test_split ( df_2 , test_size = 0.5 )","title":"\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u53d6\u5f97 (taitanic)"},{"location":"general_data_analysis/#_3","text":"","title":"\u53ef\u8996\u5316\u30e2\u30b8\u30e5\u30fc\u30eb"},{"location":"general_data_analysis/#_4","text":"\u6b20\u640d\u5024\u3092msno\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u7528\u3057\u3066\u53ef\u8996\u5316\u3059\u308b\u3002\u3056\u3063\u304f\u308a\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u5168\u4f53\u306e\u6b20\u640d\u5024\u3001\u6b20\u640d\u5024\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u7406\u89e3\u3059\u308b\u969b\u306b\u4f7f\u7528\u3002 def missing_plot ( df : pd . DataFrame , type = \"matrix\" ): \"\"\"show missing plot by misno Args: df (pd.DataFrame): target_df type: matrix/bar/heatmap/dendrogram \"\"\" if type == \"matrix\" : msno . matrix ( df ) if type == \"bar\" : msno . bar ( df ) if type == \"heatmap\" : msno . heatmap ( df ) if type == \"dendrogram\" : msno . dendrogram ( df ) missing_plot ( df_2 , type = \"matrix\" ) missing_plot ( df_2 , type = \"bar\" ) missing_plot ( df_2 , type = \"heatmap\" ) missing_plot ( df_2 , type = \"dendrogram\" ) Matrix Bar Heatmap Dendrogram","title":"\u6b20\u640d\u5024\u306e\u53ef\u8996\u5316"},{"location":"general_data_analysis/#_5","text":"\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3067\u6b20\u640d\u5024\u304c\u3042\u308b\u30ab\u30e9\u30e0\u540d\u3068\u305d\u306e\u6b20\u640d\u5024\u5272\u5408\u3092\u8868\u793a\u3059\u308b\u3002 def get_null_df ( df : pd . DataFrame ): \"\"\"Calculate missing data rate Args: df (pd.DataFrame): target_df Returns: col_null_df: dataframe of null_feature_rate \"\"\" col_null_df = pd . DataFrame ( columns = [ 'Column' , 'Type' , 'Total NaN' , '%' ]) col_null = df . columns [ df . isna () . any ()] . to_list () L = len ( df ) for col in col_null : T = 0 if df [ col ] . dtype == \"float64\" : T = \"Numerical\" elif df [ col ] . dtype == \"int64\" : T = \"Numerical\" else : T = \"Categorical\" nulls = len ( df [ df [ col ] . isna () == True ][ col ]) col_null_df = col_null_df . append ({ 'Column' : col , 'Type' : T , 'Total NaN' : nulls , '%' : ( nulls / L ) * 100 }, ignore_index = True ) return col_null_df get_null_df ( df_2 )","title":"\u6b20\u640d\u5024\u306e\u53ef\u8996\u5316"},{"location":"general_data_analysis/#_6","text":"\u30e6\u30cb\u30fc\u30af\u306a\u5024\u3068\u305d\u306e\u500b\u6570\u3092\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u5f62\u5f0f\u3067\u53d6\u5f97\u3059\u308b\u3002\u30c7\u30fc\u30bf\u5168\u4f53\u306e\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u5168\u4f53\u50cf\u3092\u628a\u63e1\u3057\u305f\u3044\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def get_unique_df ( df : pd . DataFrame ): \"\"\"Show unique column data Args: df (pd.dataFrame):target_df Returns: unique_df: unique_df_columns \"\"\" unique_df = pd . DataFrame ( columns = [ 'Feature' , 'Unique' , 'Count' ]) for col in df . columns : v = df [ col ] . unique () l = len ( v ) unique_df = unique_df . append ({ 'Feature' : col , 'Unique' : v , 'Count' : l }, ignore_index = True ) return unique_df get_unique_df ( df_2 )","title":"\u30e6\u30cb\u30fc\u30af\u5024\u306e\u53ef\u8996\u5316"},{"location":"general_data_analysis/#_7","text":"train, test\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u3042\u308b\u5834\u5408\u305d\u308c\u305e\u308c\u306e\u9805\u76ee\u306b\u5bfe\u3059\u308b\u5404\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5206\u5e03\u3092\u30d9\u30f3\u56f3\u3067\u53ef\u8996\u5316\u3059\u308b\u3002 \u3069\u3061\u3089\u304b\u4e00\u65b9\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3060\u3051\u306b\u3057\u304b\u306a\u3044\u9805\u76ee\u3092\u898b\u3064\u3051\u308b\u306e\u306b\u6700\u9069\u306a\u624b\u6cd5\u3002 def get_venn_plot ( train : pd . DataFrame , test : pd . DataFrame ): \"\"\"show venn plot from train/test_dataset Args: train (pd.DataFrame): target_train_df test (pd.DataFrame): target_test_df \"\"\" columns = test . columns columns_num = len ( columns ) n_cols = 4 n_rows = columns_num // n_cols + 1 fig , axes = plt . subplots ( figsize = ( n_cols * 3 , n_rows * 3 ), ncols = n_cols , nrows = n_rows ) for col , ax in zip ( columns , axes . ravel ()): venn2 ( subsets = ( set ( train [ col ] . unique ()), set ( test [ col ] . unique ())), set_labels = ( 'Train' , 'Test' ), ax = ax ) ax . set_title ( col ) fig . tight_layout () get_venn_plot ( train_2 , test_2 )","title":"\u30d9\u30f3\u56f3\u306e\u4f5c\u6210"},{"location":"general_data_analysis/#_8","text":"seaborn\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u3092\u5229\u7528\u3057\u3066Numeric\u30c7\u30fc\u30bf\u9593\u306e\u76f8\u95a2\u3092\u53ef\u8996\u5316\u3059\u308b\u3002 \u8aac\u660e\u5909\u6570\u9593\u306e\u95a2\u4fc2\u3092\u628a\u63e1\u3057\u305f\u3044\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def get_corr_heatmap ( train : pd . DataFrame , test : pd . DataFrame ): \"\"\"show correlation heat map Args: train (pd.DataFrame): target_train_df test (pd.DataFrame): target_test_df \"\"\" fig , axs = plt . subplots ( nrows = 2 , figsize = ( 15 , 15 )) plt . subplots_adjust ( wspace = 0.4 , hspace = 0.6 ) sns . heatmap ( train . corr ( ), ax = axs [ 0 ], annot = True , square = True , cmap = 'coolwarm' , annot_kws = { 'size' : 14 }, fmt = \"1.2f\" ) sns . heatmap ( test . corr (), ax = axs [ 1 ], annot = True , square = True , cmap = 'coolwarm' , annot_kws = { 'size' : 14 }, fmt = \"1.2f\" ) for i in range ( 2 ): axs [ i ] . tick_params ( axis = 'x' , labelsize = 14 ) axs [ i ] . tick_params ( axis = 'y' , labelsize = 14 ) axs [ 0 ] . set_title ( 'Training Set Correldef get_check_norm(x:pd.Series): \"\"\"show norm feature with Sapiro-Wilk test Args: x (pd.Series): Pandas Series \"\"\" plt . hist ( x , bins = 100 ) #\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u8868\u793a plt . pause ( .01 ) stats . probplot ( x , dist = \"norm\" , plot = plt ) #QQ\u30d7\u30ed\u30c3\u30c8\u8868\u793a plt . pause ( .01 ) p = stats . shapiro ( x ) #\u30b7\u30e3\u30d4\u30ed\u30a6\u30a3\u30eb\u30af\u691c\u5b9a print ( p ) ations ', size=15) axs [ 1 ] . set_title ( 'Test Set Correlations' , size = 15 ) plt . show () get_corr_heatmap ( X_train_1 , X_test_1 )","title":"\u76f8\u95a2\u56f3\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u306e\u4f5c\u6210"},{"location":"general_data_analysis/#_9","text":"\u6b63\u898f\u5206\u5e03\u306b\u3069\u306e\u7a0b\u5ea6\u5f93\u3063\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002 \u30b7\u30e3\u30d4\u30ed\u30a6\u30a3\u30eb\u30af\u691c\u5b9a\u306b\u3066\u6b63\u898f\u5206\u5e03\u306b\u3069\u306e\u7a0b\u5ea6\u5f93\u3046\u306e\u304b\u5b9a\u91cf\u5316\u3059\u308b\u4e8b\u304c\u3067\u304d\u308b\u3002 def get_check_norm ( x : pd . Series ): \"\"\"show norm feature with Sapiro-Wilk test Args: x (pd.Series): Pandas Series \"\"\" plt . hist ( x , bins = 100 ) #\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u8868\u793a plt . pause ( .01 ) stats . probplot ( x , dist = \"norm\" , plot = plt ) #QQ\u30d7\u30ed\u30c3\u30c8\u8868\u793a plt . pause ( .01 ) p = stats . shapiro ( x ) #\u30b7\u30e3\u30d4\u30ed\u30a6\u30a3\u30eb\u30af\u691c\u5b9a print ( p ) get_check_norm ( df_1 . Population )","title":"\u6b63\u898f\u5206\u5e03\u306e\u78ba\u8a8d"},{"location":"general_data_analysis/#yyplot","text":"\u4e88\u5b9f\u5224\u5b9a\u3068\u3057\u3066\u4e88\u6e2c\u7d50\u679c\u3068\u5b9f\u6e2c\u5024\u306e\u6bd4\u8f03\u3092\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def get_yyplot ( y_obs : list , y_pred : list ): \"\"\"show yyplot Args: y_obs (list): actual y y_pred (list]): predicted y \"\"\" yvalues = np . concatenate ([ y_obs , y_pred ]) ymin , ymax , yrange = np . amin ( yvalues ), np . amax ( yvalues ), np . ptp ( yvalues ) fig = plt . figure ( figsize = ( 8 , 8 )) plt . scatter ( y_obs , y_pred ) plt . plot ([ ymin - yrange * 0.01 , ymax + yrange * 0.01 ], [ ymin - yrange * 0.01 , ymax + yrange * 0.01 ]) plt . xlabel ( 'y_observed' , fontsize = 24 ) plt . ylabel ( 'y_predicted' , fontsize = 24 ) plt . title ( 'Observed-Predicted Plot' , fontsize = 24 ) plt . tick_params ( labelsize = 16 ) get_yyplot ( y_obs = y_obs_1 , y_pred = y_pred_1 )","title":"yyplot \u4f5c\u6210\u95a2\u6570"},{"location":"general_data_analysis/#_10","text":"train/test\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067Numeric\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3092\u78ba\u8a8d\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 train/test\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067\u5927\u304d\u304f\u5206\u5e03\u304c\u7570\u306a\u308b\u5834\u5408\u306f\u8a13\u7df4\u6642\u306e\u30c7\u30fc\u30bf\u9078\u629e\u306b\u7559\u610f\u3059\u308b\u3002 def get_numeric_features_plot ( train : pd . DataFrame , test : pd . DataFrame , cont_features : list , height , figsize , hspace = .3 ): \"\"\"Show Numeric Features Distribution Args: train (pd.DataFrame): train_df test (pd.DataFrame): test_df cont_features (list): target_features height ([float]): plot_height figsize ([float]): plot_size hspace (float, optional): space of figs. Defaults to .3. \"\"\" ncols = 2 nrows = int ( math . ceil ( len ( cont_features ) / 2 )) fig , axs = plt . subplots ( ncols = ncols , nrows = nrows , figsize = ( height * 2 , height * nrows )) plt . subplots_adjust ( right = 1.5 , hspace = hspace ) for i , feature in enumerate ( cont_features ): plt . subplot ( nrows , ncols , i + 1 ) # Distribution of target features sns . distplot ( train [ feature ], label = 'Train' , hist = True , color = '#e74c3c' ) sns . distplot ( test [ feature ], label = 'Test' , hist = True , color = '#2ecc71' ) plt . xlabel ( ' {} ' . format ( feature ), size = figsize , labelpad = 15 ) plt . ylabel ( 'Density' , size = figsize , labelpad = 15 ) plt . tick_params ( axis = 'x' , labelsize = figsize ) plt . tick_params ( axis = 'y' , labelsize = figsize ) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . title ( 'Distribution of {} Feature' . format ( feature ), size = figsize , y = 1.05 ) plt . show () get_numeric_features_plot ( train = X_train_1 , test = X_test_1 , cont_features = X_train_1 . columns , height = 5 , figsize = 12 , hspace = .4 )","title":"\u6570\u5024\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u53ef\u8996\u5316"},{"location":"general_data_analysis/#_11","text":"train/test\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3067category\u30c7\u30fc\u30bf\u306e\u5206\u5e03\u3092\u78ba\u8a8d\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 def categorical_count_plot ( train : pd . DataFrame , test : pd . DataFrame , cat_features : list , height , figsize , hspace = .3 ): \"\"\"Show Numeric Features Distribution Args: train (pd.DataFrame): train_df test (pd.DataFrame): test_df cat_features (list): target_features height ([float]): plot_height figsize ([float]): plot_size hspace (float, optional): space of figs. Defaults to .3. \"\"\" ncols = 2 nrows = int ( math . ceil ( len ( cat_features ) / 2 )) train [ \"type\" ] = \"train\" test [ \"type\" ] = \"test\" whole_df = pd . concat ([ train , test ], axis = 0 ) . reset_index ( drop = True ) fig , axs = plt . subplots ( ncols = ncols , nrows = nrows , figsize = ( height * 2 , height * nrows )) plt . subplots_adjust ( right = 1.5 , hspace = hspace ) for i , feature in enumerate ( cat_features ): plt . subplot ( nrows , ncols , i + 1 ) # Distribution of target features ax = sns . countplot ( data = whole_df , x = feature , hue = \"type\" ) ax . set_xticklabels ( ax . get_xticklabels (), rotation = 90 ) plt . xlabel ( ' {} ' . format ( feature ), size = figsize , labelpad = 15 ) plt . ylabel ( 'Density' , size = figsize , labelpad = 15 ) plt . tick_params ( axis = 'x' , labelsize = figsize ) plt . tick_params ( axis = 'y' , labelsize = figsize ) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . title ( 'Count of {} Feature' . format ( feature ), size = figsize , y = 1.05 ) plt . show () #category item\u306e\u6307\u5b9a cat_items = [ 'sex' , 'boat' ] categorical_count_plot ( train = train_2 , test = test_2 , cat_features = cat_items , height = 5 , figsize = 12 , hspace = .4 )","title":"\u30ab\u30c6\u30b4\u30ea\u30fc\u30c7\u30fc\u30bf\u306e\u30ab\u30a6\u30f3\u30c8\u53ef\u8996\u5316"},{"location":"general_data_analysis/#_12","text":"category\u30c7\u30fc\u30bf\u306e\u69cb\u6210\u5272\u5408\u3092\u5186\u30b0\u30e9\u30d5\u3067\u53ef\u8996\u5316\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def get_pie_plot ( df : pd . DataFrame , target_feature : str , column : str , title : str ): \"\"\"show target_feature rate Args: df (pd.DataFrame): target df target_feature ([str]): target feature to show rate column ([str]): column to count target_feature title ([str]): fig titles \"\"\" plt . style . use ( 'default' ) sns . set_style ( 'whitegrid' ) sns . set ( font = \"IPAexGothic\" ) sns . set_palette ( 'Set2' ) target_df = df . groupby ( target_feature )[ column ] . count () . reset_index () labels = [] target = target_df . copy () for i in range ( len ( target )): label = target [ target_feature ][ i ] + \" [\" + str ( round ( target [ column ][ i ] * 100 / target [ column ] . sum (), 1 )) + \" %\" + \"]\" labels . append ( label ) x = labels x = target [ target_feature ] y = target [ column ] fig = plt . figure ( figsize = ( 50 , 5 )) ax = fig . add_subplot ( 1 , 2 , 1 ) ax . pie ( y , counterclock = False , startangle = 90 , rotatelabels = False , labels = x ); plt . title ( title ) plt . legend ( labels , loc = \"lower left\" ); get_pie_plot ( df = df_2 , target_feature = \"target\" , column = \"sex\" , title = \"Rate of sex\" )","title":"\u5186\u30b0\u30e9\u30d5\u306e\u4f5c\u6210"},{"location":"general_data_analysis/#boxplot","text":"\u30ab\u30c6\u30b4\u30ea\u30fc\u5909\u6570\u306b\u5bfe\u3059\u308btarget(Numeric)\u5024\u306e\u7bc4\u56f2\u3092boxplot\u3067\u53ef\u8996\u5316\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def get_cat_boxplot ( df , x , y , title , order : list , height , hue = None ): \"\"\"Show box plot Args: df ([type]): target_df x ([type]): target_x y ([type]): target_y title ([type]): fig title order (list): BoxPlot_order hue ([type], optional): show hue. Defaults to None. \"\"\" fig = plt . figure ( figsize = ( height * 1.5 , height )) plt . style . use ( 'default' ) sns . set_style ( 'whitegrid' ) sns . set ( font = \"IPAexGothic\" ) sns . set_palette ( 'Set2' ) sns . set_style ( { \"xtick.major.size\" : 4 , \"ytick.major.size\" : 4 }) ax = plt . gca () if hue is None : g = sns . boxplot ( data = df , x = x , y = y , sym = \"\" , order = order ) else : g = sns . boxplot ( data = df , x = x , y = y , sym = \"\" , order = order , hue = hue ) g . set_title ( title ) labels = ax . get_xticklabels () plt . setp ( labels , rotation = 90 ); get_cat_boxplot ( df = df_2 , x = \"sex\" , y = \"age\" , title = \"age boxplot of each sex\" , order = [ \"male\" , \"female\" ], height = 8 )","title":"\u30ab\u30c6\u30b4\u30ea\u30fc\u306e\u30c7\u30fc\u30bf\u306eBoxPlot\u4f5c\u6210"},{"location":"general_data_analysis/#_13","text":"","title":"\u524d\u51e6\u7406\u30e2\u30b8\u30e5\u30fc\u30eb"},{"location":"general_data_analysis/#_14","text":"Numeric\u30c7\u30fc\u30bf\u3092\u6a19\u6e96\u5316\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 def StandardScaler ( input_df : pd . DataFrame , target_cols : list ): \"\"\"Standard Scaling Args: input_df (pd.DataFrame): input dataframe target_cols (list): target columns Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] scaler = preprocessing . StandardScaler () . fit ( features . values ) features = scaler . transform ( features . values ) input_df [ target_cols ] = features output_df = input_df . copy () return output_df #target columns target_cols = df_1 . columns StandardScaler ( input_df = df_1 , target_cols = target_cols )","title":"\u6a19\u6e96\u5316"},{"location":"general_data_analysis/#minmax","text":"Numeric\u30c7\u30fc\u30bf\u3092\u6b63\u898f\u5316\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 def MinMaxScaler ( input_df : pd . DataFrame , target_cols : list ): \"\"\"MinMax Scaling Args: input_df (pd.DataFrame): input dataframe target_cols (list): target columns Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] scaler = preprocessing . MinMaxScaler () . fit ( features . values ) features = scaler . transform ( features . values ) input_df [ target_cols ] = features output_df = input_df . copy () return output_df #target columns target_cols = [ \"MedInc\" , \"HouseAge\" , \"AveRooms\" ] df_minmax = df_1 [ target_cols ] . copy () MinMaxScaler ( input_df = df_minmax , target_cols = target_cols )","title":"\u6b63\u898f\u5316 (Min/Max)"},{"location":"general_data_analysis/#l1l2-norm","text":"Numeric\u30c7\u30fc\u30bf\u3092(L1/L2 Norm)\u3067\u6b63\u898f\u5316\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 def Normalizer ( input_df : pd . DataFrame , target_cols : list , norm = \"l2\" ): \"\"\"_summary_ Args: input_df (pd.DataFrame): input dataframe target_cols (list): target columns norm (list, optional): select [\"l2\", \"l2\", \"max\"]. Default is \"l2\" Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] scaler = preprocessing . Normalizer ( norm = norm ) . fit ( features . values ) features = scaler . transform ( features . values ) input_df [ target_cols ] = features output_df = input_df . copy () return output_df #target columns target_cols = [ \"MedInc\" , \"HouseAge\" , \"AveRooms\" ] df_nor = df_1 [ target_cols ] . copy () Normalizer ( input_df = df_nor , target_cols = target_cols , norm = \"l1\" )","title":"\u6b63\u898f\u5316 (L1/L2 Norm)"},{"location":"general_data_analysis/#robust-scaler","text":"Numeric\u30c7\u30fc\u30bf\u3092Robust Scaler\u3067\u6a19\u6e96\u5316\u3059\u308b\u969b\u306b\u4f7f\u7528\u3059\u308b\u3002 \u5916\u308c\u5024\u306e\u5f71\u97ff\u304c\u5927\u304d\u3044\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def RobustScaler ( input_df : pd . DataFrame , target_cols : list ): \"\"\"Robust Scaler Args: input_df (pd.DataFrame): input Dataframe target_cols (list): target columns Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] scaler = preprocessing . RobustScaler () . fit ( features . values ) features = scaler . transform ( features . values ) input_df [ target_cols ] = features output_df = input_df . copy () return output_df #target columns target_cols = [ \"MedInc\" , \"HouseAge\" , \"AveRooms\" ] df_rob = df_1 [ target_cols ] . copy () RobustScaler ( input_df = df_rob , target_cols = target_cols )","title":"\u6a19\u6e96\u5316 (Robust Scaler)"},{"location":"general_data_analysis/#_15","text":"\u6307\u5b9a\u30ab\u30e9\u30e0\u306b\u72ec\u81ea\u306e\u95a2\u6570\u3092\u3042\u3066\u304c\u3044\u3001\u65b0\u305f\u306b\u8aac\u660e\u5909\u6570\u3092\u4f5c\u6210\u3059\u308b\u969b\u306b\u5229\u7528\u3059\u308b\u3002 def FunctionTransformer ( input_df : pd . DataFrame , target_cols : list , function ): \"\"\"Apply function to dataframe Args: input_df (pd.DataFrame): input dataframe target_cols (list): target columns function : function Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] scaler = preprocessing . FunctionTransformer ( function ) . fit ( features . values ) features = scaler . transform ( features . values ) input_df [ target_cols ] = features output_df = input_df . copy () return output_df #target columns target_cols = [ \"MedInc\" , \"HouseAge\" , \"AveRooms\" ] df_func = df_1 [ target_cols ] . copy () #define function def apply_func ( x ): x = x * 2 return x FunctionTransformer ( input_df = df_func , target_cols = target_cols , function = apply_func )","title":"\u30ab\u30b9\u30bf\u30e0\u95a2\u6570\u306e\u5b9f\u88c5"},{"location":"general_data_analysis/#_16","text":"\u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3066\u6307\u5b9a\u8aac\u660e\u5909\u6570\u306e\u6570\u3092\u6570\u3048\u3001\u65b0\u305f\u306a\u8aac\u660e\u5909\u6570\u3092\u8ffd\u52a0\u3059\u308b\u3002 def CountEncoding ( target_cols , input_df : pd . DataFrame ): \"\"\"CountEncoding Args: target_cols (category): target columns input_df (pd.DataFrame): input dataframe Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] encoder = ce . CountEncoder () . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"CE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) return output_df target_cols = [ \"sex\" ] output_df = CountEncoding ( target_cols = target_cols , input_df = df_2 ) output_df [[ \"sex\" , \"CE_sex\" , \"target\" ]]","title":"\u30ab\u30a6\u30f3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0"},{"location":"general_data_analysis/#_17","text":"\u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3066category\u5909\u6570\u3092\u5909\u63db\u3059\u308b\u3002 def LabelEncoding ( target_cols , input_df : pd . DataFrame ): \"\"\"Labelencoding Args: target_cols (category): target columns input_df (pd.DataFrame): input dataframe Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] encoder = ce . OrdinalEncoder () . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"LE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) return output_df target_cols = [ \"sex\" ] output_df = LabelEncoding ( target_cols = target_cols , input_df = df_2 ) output_df [[ \"sex\" , \"LE_sex\" , \"target\" ]]","title":"\u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0"},{"location":"general_data_analysis/#onehot","text":"OneHot\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3066category\u5909\u6570\u3092\u5909\u63db\u3059\u308b\u3002 def OneHotEncoding ( target_cols , input_df : pd . DataFrame ): \"\"\"OneHotEncoding Args: target_cols (category): target columns input_df (pd.DataFrame): input dataframe Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] encoder = ce . OneHotEncoder ( use_cat_names = True ) . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = output_df . columns . str [ 2 :] output_df = output_df . add_prefix ( \"OHE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) return output_df target_cols = [ \"sex\" ] output_df = OneHotEncoding ( target_cols = target_cols , input_df = df_2 ) output_df [[ \"sex\" , \"OHE_female\" , \"OHE_male\" , \"target\" ]]","title":"OneHot\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0"},{"location":"general_data_analysis/#_18","text":"\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3066\u76ee\u7684\u5909\u6570\u306b\u5bfe\u3057\u3066\u306ecategory\u5909\u6570\u306e\u5e73\u5747\u3092\u53d6\u5f97\u3059\u308b\u3002 def TargetEncoding ( target_cols , input_df : pd . DataFrame , target ): \"\"\"TargetEncoding Args: target_cols (category): target columns input_df (pd.DataFrame): input dataframe target (str): column name Returns: output_df (pd.DataFrame): output dataframe \"\"\" features = input_df [ target_cols ] encoder = ce . TargetEncoder () . fit ( features . values , target ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"TE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) return output_df target_cols = [ \"sex\" ] output_df = TargetEncoding ( target_cols = target_cols , input_df = df_2 , target = df_2 [ \"age\" ]) output_df [[ \"sex\" , \"TE_sex\" , \"age\" ]]","title":"\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0"},{"location":"general_data_analysis/#_19","text":"\u6307\u5b9acolumn\u306e\u56db\u5247\u6f14\u7b97\u306b\u3066\u65b0\u898f\u306b\u8aac\u660e\u5909\u6570\u3092\u8ffd\u52a0\u3059\u308b\u3002 def ArithmeticOperation ( input_df , target_column1 : str , target_column2 : str , operation : str ): \"\"\"Arithmetic operation Args: input_df (pd.DataFrame): input dataframe target_column1 (str): target columns 1 target_column2 (str): target columns 2 operation (str): select [\"+\",\"-\",\"*\",\"/\"] Returns: output_df (pd.DataFrame): output dataframe \"\"\" output_df = input_df . copy () output_df_columns_name = f ' { target_column1 }{ operation }{ target_column2 } ' if operation == \"+\" : output_df [ output_df_columns_name ] = output_df [ target_column1 ] + \\ output_df [ target_column2 ] elif operation == \"-\" : output_df [ output_df_columns_name ] = output_df [ target_column1 ] - \\ output_df [ target_column2 ] elif operation == \"*\" : output_df [ output_df_columns_name ] = output_df [ target_column1 ] * \\ output_df [ target_column2 ] elif operation == \"/\" : output_df [ output_df_columns_name ] = output_df [ target_column1 ] / \\ output_df [ target_column2 ] output_df = pd . concat ( [ input_df , output_df [ output_df_columns_name ]], axis = 1 ) return output_df target_cols = [ \"AveRooms\" , \"AveBedrms\" ] output_df = ArithmeticOperation ( input_df = df_1 , target_column1 = \"AveRooms\" , target_column2 = \"AveBedrms\" , operation = \"+\" ) output_df [[ \"AveRooms\" , \"AveBedrms\" , \"AveRooms+AveBedrms\" ]]","title":"\u56db\u5247\u6f14\u7b97\u51e6\u7406"},{"location":"general_data_analysis/#_20","text":"category\u5909\u6570\u306b\u5bfe\u3057\u3066\u6307\u5b9a\u306eNumeric\u5909\u6570\u306e\u5e73\u5747\u3001\u6700\u5927\u5024\u3001\u6700\u5c0f\u5024\u3001\u5408\u8a08\u3092\u7b97\u51fa\u3002\u65b0\u898f\u306b\u8aac\u660e\u5909\u6570\u3068\u3057\u3066\u8ffd\u52a0\u3059\u308b\u3002 def AggregateOperation ( input_df : pd . DataFrame , key : str , agg_column : str , agg_funcs : list , fillna = None ): \"\"\"Aggregate Operation Args: input_df (pd.DataFrame): input DataFrame key (str): category column agg_column (str): _numeric column agg_funcs (list): select [\"sum\",\"mean\",\"max\",\"min\"] fillna (optional): Defaults to None. Returns: output_df (pd.DataFrame): output dataframe \"\"\" if fillna : input_df [ agg_column ] = input_df [ agg_column ] . fillna ( fillna ) group_df = input_df . groupby ( key ) . agg ({ agg_column : agg_funcs }) . reset_index () column_names = [ f 'GP_ { agg_column } @ { key } _ { agg_func } ' for agg_func in agg_funcs ] group_df . columns = [ key ] + column_names output_df = pd . merge ( input_df [ key ], group_df , on = key , how = \"left\" ) . drop ( columns = [ key ]) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) return output_df output_df = AggregateOperation ( input_df = df_2 , key = \"sex\" , agg_column = \"age\" , agg_funcs = [ \"sum\" , \"mean\" , \"max\" , \"min\" ]) output_df [[ \"sex\" , \"age\" , \"GP_age@sex_sum\" , \"GP_age@sex_mean\" , \"GP_age@sex_max\" , \"GP_age@sex_min\" ]]","title":"\u30ab\u30e9\u30e0\u7d50\u5408"},{"location":"general_data_analysis/#_21","text":"\u30b0\u30e9\u30d5\u30a3\u30ab\u30eb\u30e9\u30c3\u30bd\u306b\u3066\u5404\u8aac\u660e\u5909\u6570\u9593\u306e\u76f8\u95a2\u95a2\u4fc2\u3092\u53ef\u8996\u5316\u3059\u308b\u3002 from sklearn.covariance import GraphicalLasso from sklearn.covariance import GraphicalLassoCV import pydot from IPython.display import Image , display # \u53ef\u8996\u5316\u30e9\u30a4\u30d6\u30e9\u30ea import matplotlib.pyplot as plt import matplotlib as mpl import seaborn as sns from statsmodels.graphics.factorplots import interaction_plot def execute_graphical_lassoCV ( X , alphas = 4 , verbose = True ): X_std = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) glasso = GraphicalLassoCV ( alphas = alphas , verbose = verbose ) glasso . fit ( X_std ) print ( \"alpha = \" , glasso . alpha_ ) return glasso def execute_graphical_lasso ( X , alpha = 0.2 , verbose = True ): X_std = ( X - X . mean ( axis = 0 )) / X . std ( axis = 0 ) glasso = GraphicalLasso ( alpha = alpha , verbose = verbose ) glasso . fit ( X_std ) return glasso def Disp_Gaussian_Graphical_Model ( model , feature_names ): \"\"\" GraphLasso\u306b\u3088\u308b\u5909\u6570\u9593\u306e\u95a2\u4fc2\u306e\u30b0\u30e9\u30d5\u5316@\u77e5\u8b58\u306e\u30b5\u30e9\u30c0\u30dc\u30a6\u30eb \u30d0\u30b0\u4fee\u6b63\u3057\u3066\u4f7f\u7528 \"\"\" pre_ = model . precision_ #\u30b9\u30d1\u30fc\u30b9\u5316\u3057\u305f\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u9006\u884c\u5217 #\u30b0\u30e9\u30d5\u8868\u793a\u306e\u305f\u3081\u306b\u5bfe\u89d2\u6210\u5206\u306f0\u306b\u3059\u308b pre_zero = pre_ - np . diag ( np . diag ( pre_ )) #\u5206\u6563\u5171\u5206\u6563\u884c\u5217\u306e\u9006\u884c\u5217(\u2252\u96a3\u63a5\u884c\u5217)\u304b\u3089\u30b0\u30e9\u30d5\u3092\u751f\u6210\u3059\u308b g = pydot . Dot ( graph_type = 'graph' ) df = pd . DataFrame ( index = feature_names . tolist (), columns = feature_names . tolist (), data = pre_zero . tolist ()) #\u30ce\u30fc\u30c9\u3092\u8ffd\u52a0 for c in df . columns : node = pydot . Node ( c ) g . add_node ( node ) #\u30a8\u30c3\u30b8\u3092\u8ffd\u52a0 for i in df . index : for c in df . columns : if c >= i : #print(i, c) #\u30a8\u30c3\u30b8\u306e\u7d76\u5bfe\u5024\u304c0.1\u3088\u308a\u5927\u304d\u3044\u3068\u304d\u306b\u8868\u793a(\u3053\u3053\u306f\u597d\u307f\u306b\u3088\u308b) if abs ( df . loc [ i , c ]) > 0.1 : if df . loc [ i , c ] > 0.0 : style = \"solid\" else : style = \"dashed\" edge = pydot . Edge ( g . get_node ( i )[ 0 ], g . get_node ( c )[ 0 ], penwidth = 5 * abs ( df . loc [ i , c ]), style = style ) edge . set_label ( ' {:.1f} ' . format ( df . loc [ i , c ])) g . add_edge ( edge ) #\u7d50\u5408\u6570\u306b\u3088\u3063\u3066\u8272\u3092\u5909\u3048\u308b nodeList = g . get_node_list () num_edges = np . sum ( abs ( pre_zero ) > 0.1 , axis = 0 ) max_num_edges = max ( np . amax ( num_edges ), 1 ) for i , tag in enumerate ( df . index ): node = g . get_node ( tag )[ 0 ] # H S V\u306e\u9806 S\u3092\u5909\u66f4 color = str ( 0.3 ) + \" \" + str ( num_edges [ i ] / max_num_edges ) + \" \" + str ( 1 ) node . set_color ( \"black\" ) #node\u306e\u8f2a\u90ed\u306e\u8272 node . set_fillcolor ( color ) #node\u306e\u4e2d\u8eab\u306e\u8272 node . set_style ( 'filled' ) # \u30b0\u30e9\u30d5\u3092\u51fa\u529b #g.write_png('LassoEstimatedGGM.png', prog='neato') g . write_png ( 'LassoEstimatedGGM.png' ) glasso = execute_graphical_lasso ( df_1 . values , alpha = 0.3 ) Disp_Gaussian_Graphical_Model ( glasso , np . array ( df_1 . columns )) display ( Image ( 'LassoEstimatedGGM.png' ))","title":"\u30b0\u30e9\u30d5\u30a3\u30ab\u30eb\u30e9\u30c3\u30bd\u306b\u3088\u308b\u504f\u76f8\u95a2\u53ef\u8996\u5316"},{"location":"git_command/","text":"Git Command \u00b6 \u521d\u671f\u8a2d\u5b9a\u3092\u884c\u3046 \u00b6 $ git config --global user.name \"XXXX\" $ git config --global user.email \"XXXX@hogehoge.com\" \u30ed\u30fc\u30ab\u30eb\u306b\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3001\u30ea\u30e2\u30fc\u30c8\u306b\u30d7\u30c3\u30b7\u30e5\u3059\u308b \u00b6 $ git init $ git add . $ git commit -m \"Initial commit\" $ git remote add origin https://github.com/XXXX/XXXXXX.git $ git push -u origin master \u30ea\u30e2\u30fc\u30c8\u304b\u3089\u30af\u30ed\u30fc\u30f3\u3059\u308b \u00b6 $ git clone https://github.com/XXXX/XXXXXX.git \u30ea\u30e2\u30fc\u30c8\u304b\u3089\u5909\u66f4\u3092\u53d6\u5f97\u3059\u308b \u00b6 $ git pull or $ git fetch $ git merge origin/master \u30d5\u30a1\u30a4\u30eb\u306e\u767b\u9332\uff08\u30b3\u30df\u30c3\u30c8\u3059\u308b\u305f\u3081\uff09 \u00b6 $ git add <\u30d5\u30a1\u30a4\u30eb\u540d> \u30d5\u30a1\u30a4\u30eb\u306e\u5909\u66f4\u3084\u8ffd\u52a0\u3092\u30b3\u30df\u30c3\u30c8 \u00b6 $ git commit -m \"\u30b3\u30df\u30c3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\" \u30ed\u30fc\u30ab\u30eb\u306e\u5909\u66f4\u3092\u78ba\u8a8d\u3059\u308b \u00b6 $ git status \u30ea\u30e2\u30fc\u30c8\u3068\u30ed\u30fc\u30ab\u30eb\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u5dee\u5206\u3092\u62bd\u51fa\u3059\u308b \u00b6 $ git diff <\u30d5\u30a1\u30a4\u30eb\u540d> commit\u306e\u5909\u66f4\u5c65\u6b74\u3092\u307f\u308b \u00b6 $ git log \u6307\u5b9a\u3057\u305fcommit\u306e\u5909\u66f4\u70b9\u3092\u898b\u308b \u00b6 $ git show <\u30b3\u30df\u30c3\u30c8\u306e\u30cf\u30c3\u30b7\u30e5\u5024> \u30ea\u30e2\u30fc\u30c8\u306b\u30d7\u30c3\u30b7\u30e5 \u00b6 $ git push origin <\u30d6\u30e9\u30f3\u30c1\u540d> add\u306e\u53d6\u308a\u6d88\u3057 \u00b6 $ git reset HEAD <\u30d5\u30a1\u30a4\u30eb\u540d> commit\u306e\u53d6\u308a\u6d88\u3057 \u00b6 $ git reset --hard HEAD^ -hard\uff1a\u30b3\u30df\u30c3\u30c8\u53d6\u308a\u6d88\u3057\u305f\u4e0a\u3067\u30ef\u30fc\u30af\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5185\u5bb9\u3082\u66f8\u304d\u63db\u3048\u305f\u3044\u5834\u5408-soft\uff1a\u30ef\u30fc\u30af\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5185\u5bb9\u306f\u305d\u306e\u307e\u307e\u3067\u30b3\u30df\u30c3\u30c8\u3060\u3051\u3092\u53d6\u308a\u6d88\u3057\u305f\u3044\u5834\u5408HEAD^\uff1a\u76f4\u524d\u306e\u30b3\u30df\u30c3\u30c8HEAD~{n} \uff1an\u500b\u524d\u306e\u30b3\u30df\u30c3\u30c8 commit\u306e\u6253\u3061\u6d88\u3057 \u00b6 $ git revert <\u30b3\u30df\u30c3\u30c8\u306e\u30cf\u30c3\u30b7\u30e5\u5024> \u30b3\u30df\u30c3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u4fee\u6b63 \u00b6 $ git commit --amend \"\u65b0\u3057\u3044\u30b3\u30df\u30c3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\" push\u306e\u53d6\u308a\u6d88\u3057 \u00b6 $ git reset --hard <\u623b\u3057\u305f\u3044\u30b3\u30df\u30c3\u30c8\u306e\u30cf\u30c3\u30b7\u30e5\u5024> $ git push -f \u30ed\u30fc\u30ab\u30eb\u3067\u30d6\u30e9\u30f3\u30c1\u3092\u4f5c\u6210 \u00b6 $ git branch <\u30d6\u30e9\u30f3\u30c1\u540d> \u30ed\u30fc\u30ab\u30eb\u3067\u30d6\u30e9\u30f3\u30c1\u3092\u5207\u308a\u66ff\u3048 \u00b6 $ git checkout <\u30d6\u30e9\u30f3\u30c1\u540d> \u30d6\u30e9\u30f3\u30c1\u4f5c\u6210 & \u5207\u308a\u66ff\u3048 \u00b6 $ git checkout -b <\u30d6\u30e9\u30f3\u30c1\u540d> \u30d6\u30e9\u30f3\u30c1\u540d\u306e\u5909\u66f4 \u00b6 $ git branch -m <\u53e4\u3044\u30d6\u30e9\u30f3\u30c1\u540d> <\u65b0\u3057\u3044\u30d6\u30e9\u30f3\u30c1\u540d> \u30d6\u30e9\u30f3\u30c1\u306e\u524a\u9664 \u00b6 $ git branch -d <\u30d6\u30e9\u30f3\u30c1\u540d> \u30ed\u30fc\u30ab\u30eb\u306e\u30d6\u30e9\u30f3\u30c1\u3092\u30ea\u30e2\u30fc\u30c8\u306b\u53cd\u6620 \u00b6 $ git push -u origin <\u30ed\u30fc\u30ab\u30eb\u306e\u30d6\u30e9\u30f3\u30c1\u540d> \u30ea\u30e2\u30fc\u30c8\u306e\u30d6\u30e9\u30f3\u30c1\u3092\u30ed\u30fc\u30ab\u30eb\u6301\u3063\u3066\u304f\u308b \u00b6 $ git branch <\u30d6\u30e9\u30f3\u30c1\u540d> origin/<\u30d6\u30e9\u30f3\u30c1\u540d> \u30ea\u30e2\u30fc\u30c8\u306e\u30d6\u30e9\u30f3\u30c1\u3092\u30ed\u30fc\u30ab\u30eb\u6301\u3063\u3066\u304f\u308b & \u5207\u308a\u66ff\u3048 \u00b6 $ git checkout -b <\u30d6\u30e9\u30f3\u30c1\u540d> origin/<\u30d6\u30e9\u30f3\u30c1\u540d> \u5168\u3066\u306e\u30d6\u30e9\u30f3\u30c1\u3092\u78ba\u8a8d\u3059\u308b \u00b6 $ git branch -a \u30d6\u30e9\u30f3\u30c1\u3092\u6bd4\u8f03\u3059\u308b \u00b6 $ git diff <\u30d6\u30e9\u30f3\u30c1\u540d> <\u30d6\u30e9\u30f3\u30c1\u540d> \u30d6\u30e9\u30f3\u30c1\u3092\u30de\u30fc\u30b8\u3059\u308b \u00b6 $ git merge <\u30d6\u30e9\u30f3\u30c1\u540d> fast-forward\u306e\u95a2\u4fc2\u3067\u3042\u3063\u3066\u3082\u5fc5\u305a\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u4f5c\u308b \u00b6 $ git merge --no-ff <\u30d6\u30e9\u30f3\u30c1\u540d> \u30d6\u30e9\u30f3\u30c1\u3092\u30ea\u30d9\u30fc\u30b9\u3059\u308b \u00b6 $ git rebase <\u30d6\u30e9\u30f3\u30c1\u540d> \u203b merge\u306e\u5834\u5408\u306f\u5206\u5c90\u5143\u3001rebase\u306e\u5834\u5408\u306f\u5206\u5c90\u5148\u306e\u30d6\u30e9\u30f3\u30c1\u3067\u5b9f\u884c\u3059\u308b\u3068\u3044\u3046\u70b9\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u5909\u66f4\u70b9\u3092\u4e00\u65e6\u9000\u907f\u3055\u305b\u308b \u00b6 $ git stash save \u9000\u907f\u3057\u305f\u4f5c\u696d\u306e\u4e00\u89a7\u3092\u898b\u308b \u00b6 $ git stash list \u9000\u907f\u3057\u305f\u4f5c\u696d\u3092\u623b\u3059 \u00b6 $ git stash apply <stash\u540d> \u9000\u907f\u3057\u305f\u4f5c\u696d\u3092\u6d88\u3059 \u00b6 $ git stash drop <stash\u540d> \u9000\u907f\u3057\u305f\u4f5c\u696d\u3092\u3059\u3079\u3066\u6d88\u3059 \u00b6 $ git stash clear \u30d5\u30a1\u30a4\u30eb\u524a\u9664 \u00b6 $ git rm -f <\u30d5\u30a1\u30a4\u30eb\u540d> \u30d5\u30a1\u30a4\u30eb\u30ea\u30cd\u30fc\u30e0 \u00b6 $ git mv <\u5143\u306e\u30d5\u30a1\u30a4\u30eb\u540d> <\u5909\u3048\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u540d> \u30d5\u30a1\u30a4\u30eb\u3092\u6700\u65b0\u306e\u30b3\u30df\u30c3\u30c8\u306e\u72b6\u614b\u306b\u623b\u3059 \u00b6 $ git checkout HEAD <\u30d5\u30a1\u30a4\u30eb\u540d> \u30d5\u30a1\u30a4\u30eb\u3092\u6307\u5b9a\u30b3\u30df\u30c3\u30c8\u307e\u3067\u623b\u3059 \u00b6 $ git checkout <\u30b3\u30df\u30c3\u30c8\u306e\u30cf\u30c3\u30b7\u30e5\u5024> <\u30d5\u30a1\u30a4\u30eb\u540d> .gitignore \u3092\u7121\u8996\u3057\u3066\u8ffd\u52a0\u3059\u308b \u00b6 $ git add -f <\u30d5\u30a1\u30a4\u30eb\u540d> \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3060\u3051\u767b\u9332(.gitkeep\u3092\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4f5c\u6210\u3059\u308b) \u00b6 $ touch <\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d>/.gitkeep","title":"Git Command"},{"location":"git_command/#git-command","text":"","title":"Git Command"},{"location":"git_command/#_1","text":"$ git config --global user.name \"XXXX\" $ git config --global user.email \"XXXX@hogehoge.com\"","title":"\u521d\u671f\u8a2d\u5b9a\u3092\u884c\u3046"},{"location":"git_command/#_2","text":"$ git init $ git add . $ git commit -m \"Initial commit\" $ git remote add origin https://github.com/XXXX/XXXXXX.git $ git push -u origin master","title":"\u30ed\u30fc\u30ab\u30eb\u306b\u30ea\u30dd\u30b8\u30c8\u30ea\u3092\u4f5c\u6210\u3057\u3001\u30ea\u30e2\u30fc\u30c8\u306b\u30d7\u30c3\u30b7\u30e5\u3059\u308b"},{"location":"git_command/#_3","text":"$ git clone https://github.com/XXXX/XXXXXX.git","title":"\u30ea\u30e2\u30fc\u30c8\u304b\u3089\u30af\u30ed\u30fc\u30f3\u3059\u308b"},{"location":"git_command/#_4","text":"$ git pull or $ git fetch $ git merge origin/master","title":"\u30ea\u30e2\u30fc\u30c8\u304b\u3089\u5909\u66f4\u3092\u53d6\u5f97\u3059\u308b"},{"location":"git_command/#_5","text":"$ git add <\u30d5\u30a1\u30a4\u30eb\u540d>","title":"\u30d5\u30a1\u30a4\u30eb\u306e\u767b\u9332\uff08\u30b3\u30df\u30c3\u30c8\u3059\u308b\u305f\u3081\uff09"},{"location":"git_command/#_6","text":"$ git commit -m \"\u30b3\u30df\u30c3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\"","title":"\u30d5\u30a1\u30a4\u30eb\u306e\u5909\u66f4\u3084\u8ffd\u52a0\u3092\u30b3\u30df\u30c3\u30c8"},{"location":"git_command/#_7","text":"$ git status","title":"\u30ed\u30fc\u30ab\u30eb\u306e\u5909\u66f4\u3092\u78ba\u8a8d\u3059\u308b"},{"location":"git_command/#_8","text":"$ git diff <\u30d5\u30a1\u30a4\u30eb\u540d>","title":"\u30ea\u30e2\u30fc\u30c8\u3068\u30ed\u30fc\u30ab\u30eb\u306e\u30d5\u30a1\u30a4\u30eb\u306e\u5dee\u5206\u3092\u62bd\u51fa\u3059\u308b"},{"location":"git_command/#commit","text":"$ git log","title":"commit\u306e\u5909\u66f4\u5c65\u6b74\u3092\u307f\u308b"},{"location":"git_command/#commit_1","text":"$ git show <\u30b3\u30df\u30c3\u30c8\u306e\u30cf\u30c3\u30b7\u30e5\u5024>","title":"\u6307\u5b9a\u3057\u305fcommit\u306e\u5909\u66f4\u70b9\u3092\u898b\u308b"},{"location":"git_command/#_9","text":"$ git push origin <\u30d6\u30e9\u30f3\u30c1\u540d>","title":"\u30ea\u30e2\u30fc\u30c8\u306b\u30d7\u30c3\u30b7\u30e5"},{"location":"git_command/#add","text":"$ git reset HEAD <\u30d5\u30a1\u30a4\u30eb\u540d>","title":"add\u306e\u53d6\u308a\u6d88\u3057"},{"location":"git_command/#commit_2","text":"$ git reset --hard HEAD^ -hard\uff1a\u30b3\u30df\u30c3\u30c8\u53d6\u308a\u6d88\u3057\u305f\u4e0a\u3067\u30ef\u30fc\u30af\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5185\u5bb9\u3082\u66f8\u304d\u63db\u3048\u305f\u3044\u5834\u5408-soft\uff1a\u30ef\u30fc\u30af\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u5185\u5bb9\u306f\u305d\u306e\u307e\u307e\u3067\u30b3\u30df\u30c3\u30c8\u3060\u3051\u3092\u53d6\u308a\u6d88\u3057\u305f\u3044\u5834\u5408HEAD^\uff1a\u76f4\u524d\u306e\u30b3\u30df\u30c3\u30c8HEAD~{n} \uff1an\u500b\u524d\u306e\u30b3\u30df\u30c3\u30c8","title":"commit\u306e\u53d6\u308a\u6d88\u3057"},{"location":"git_command/#commit_3","text":"$ git revert <\u30b3\u30df\u30c3\u30c8\u306e\u30cf\u30c3\u30b7\u30e5\u5024>","title":"commit\u306e\u6253\u3061\u6d88\u3057"},{"location":"git_command/#_10","text":"$ git commit --amend \"\u65b0\u3057\u3044\u30b3\u30df\u30c3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\"","title":"\u30b3\u30df\u30c3\u30c8\u30e1\u30c3\u30bb\u30fc\u30b8\u306e\u4fee\u6b63"},{"location":"git_command/#push","text":"$ git reset --hard <\u623b\u3057\u305f\u3044\u30b3\u30df\u30c3\u30c8\u306e\u30cf\u30c3\u30b7\u30e5\u5024> $ git push -f","title":"push\u306e\u53d6\u308a\u6d88\u3057"},{"location":"git_command/#_11","text":"$ git branch <\u30d6\u30e9\u30f3\u30c1\u540d>","title":"\u30ed\u30fc\u30ab\u30eb\u3067\u30d6\u30e9\u30f3\u30c1\u3092\u4f5c\u6210"},{"location":"git_command/#_12","text":"$ git checkout <\u30d6\u30e9\u30f3\u30c1\u540d>","title":"\u30ed\u30fc\u30ab\u30eb\u3067\u30d6\u30e9\u30f3\u30c1\u3092\u5207\u308a\u66ff\u3048"},{"location":"git_command/#_13","text":"$ git checkout -b <\u30d6\u30e9\u30f3\u30c1\u540d>","title":"\u30d6\u30e9\u30f3\u30c1\u4f5c\u6210 &amp; \u5207\u308a\u66ff\u3048"},{"location":"git_command/#_14","text":"$ git branch -m <\u53e4\u3044\u30d6\u30e9\u30f3\u30c1\u540d> <\u65b0\u3057\u3044\u30d6\u30e9\u30f3\u30c1\u540d>","title":"\u30d6\u30e9\u30f3\u30c1\u540d\u306e\u5909\u66f4"},{"location":"git_command/#_15","text":"$ git branch -d <\u30d6\u30e9\u30f3\u30c1\u540d>","title":"\u30d6\u30e9\u30f3\u30c1\u306e\u524a\u9664"},{"location":"git_command/#_16","text":"$ git push -u origin <\u30ed\u30fc\u30ab\u30eb\u306e\u30d6\u30e9\u30f3\u30c1\u540d>","title":"\u30ed\u30fc\u30ab\u30eb\u306e\u30d6\u30e9\u30f3\u30c1\u3092\u30ea\u30e2\u30fc\u30c8\u306b\u53cd\u6620"},{"location":"git_command/#_17","text":"$ git branch <\u30d6\u30e9\u30f3\u30c1\u540d> origin/<\u30d6\u30e9\u30f3\u30c1\u540d>","title":"\u30ea\u30e2\u30fc\u30c8\u306e\u30d6\u30e9\u30f3\u30c1\u3092\u30ed\u30fc\u30ab\u30eb\u6301\u3063\u3066\u304f\u308b"},{"location":"git_command/#_18","text":"$ git checkout -b <\u30d6\u30e9\u30f3\u30c1\u540d> origin/<\u30d6\u30e9\u30f3\u30c1\u540d>","title":"\u30ea\u30e2\u30fc\u30c8\u306e\u30d6\u30e9\u30f3\u30c1\u3092\u30ed\u30fc\u30ab\u30eb\u6301\u3063\u3066\u304f\u308b &amp; \u5207\u308a\u66ff\u3048"},{"location":"git_command/#_19","text":"$ git branch -a","title":"\u5168\u3066\u306e\u30d6\u30e9\u30f3\u30c1\u3092\u78ba\u8a8d\u3059\u308b"},{"location":"git_command/#_20","text":"$ git diff <\u30d6\u30e9\u30f3\u30c1\u540d> <\u30d6\u30e9\u30f3\u30c1\u540d>","title":"\u30d6\u30e9\u30f3\u30c1\u3092\u6bd4\u8f03\u3059\u308b"},{"location":"git_command/#_21","text":"$ git merge <\u30d6\u30e9\u30f3\u30c1\u540d>","title":"\u30d6\u30e9\u30f3\u30c1\u3092\u30de\u30fc\u30b8\u3059\u308b"},{"location":"git_command/#fast-forward","text":"$ git merge --no-ff <\u30d6\u30e9\u30f3\u30c1\u540d>","title":"fast-forward\u306e\u95a2\u4fc2\u3067\u3042\u3063\u3066\u3082\u5fc5\u305a\u30de\u30fc\u30b8\u30b3\u30df\u30c3\u30c8\u3092\u4f5c\u308b"},{"location":"git_command/#_22","text":"$ git rebase <\u30d6\u30e9\u30f3\u30c1\u540d> \u203b merge\u306e\u5834\u5408\u306f\u5206\u5c90\u5143\u3001rebase\u306e\u5834\u5408\u306f\u5206\u5c90\u5148\u306e\u30d6\u30e9\u30f3\u30c1\u3067\u5b9f\u884c\u3059\u308b\u3068\u3044\u3046\u70b9\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002","title":"\u30d6\u30e9\u30f3\u30c1\u3092\u30ea\u30d9\u30fc\u30b9\u3059\u308b"},{"location":"git_command/#_23","text":"$ git stash save","title":"\u5909\u66f4\u70b9\u3092\u4e00\u65e6\u9000\u907f\u3055\u305b\u308b"},{"location":"git_command/#_24","text":"$ git stash list","title":"\u9000\u907f\u3057\u305f\u4f5c\u696d\u306e\u4e00\u89a7\u3092\u898b\u308b"},{"location":"git_command/#_25","text":"$ git stash apply <stash\u540d>","title":"\u9000\u907f\u3057\u305f\u4f5c\u696d\u3092\u623b\u3059"},{"location":"git_command/#_26","text":"$ git stash drop <stash\u540d>","title":"\u9000\u907f\u3057\u305f\u4f5c\u696d\u3092\u6d88\u3059"},{"location":"git_command/#_27","text":"$ git stash clear","title":"\u9000\u907f\u3057\u305f\u4f5c\u696d\u3092\u3059\u3079\u3066\u6d88\u3059"},{"location":"git_command/#_28","text":"$ git rm -f <\u30d5\u30a1\u30a4\u30eb\u540d>","title":"\u30d5\u30a1\u30a4\u30eb\u524a\u9664"},{"location":"git_command/#_29","text":"$ git mv <\u5143\u306e\u30d5\u30a1\u30a4\u30eb\u540d> <\u5909\u3048\u305f\u3044\u30d5\u30a1\u30a4\u30eb\u540d>","title":"\u30d5\u30a1\u30a4\u30eb\u30ea\u30cd\u30fc\u30e0"},{"location":"git_command/#_30","text":"$ git checkout HEAD <\u30d5\u30a1\u30a4\u30eb\u540d>","title":"\u30d5\u30a1\u30a4\u30eb\u3092\u6700\u65b0\u306e\u30b3\u30df\u30c3\u30c8\u306e\u72b6\u614b\u306b\u623b\u3059"},{"location":"git_command/#_31","text":"$ git checkout <\u30b3\u30df\u30c3\u30c8\u306e\u30cf\u30c3\u30b7\u30e5\u5024> <\u30d5\u30a1\u30a4\u30eb\u540d>","title":"\u30d5\u30a1\u30a4\u30eb\u3092\u6307\u5b9a\u30b3\u30df\u30c3\u30c8\u307e\u3067\u623b\u3059"},{"location":"git_command/#gitignore","text":"$ git add -f <\u30d5\u30a1\u30a4\u30eb\u540d>","title":".gitignore \u3092\u7121\u8996\u3057\u3066\u8ffd\u52a0\u3059\u308b"},{"location":"git_command/#gitkeep","text":"$ touch <\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u540d>/.gitkeep","title":"\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3060\u3051\u767b\u9332(.gitkeep\u3092\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4f5c\u6210\u3059\u308b)"},{"location":"lgb_template_cls/","text":"LGBM Classification Template \u00b6 \u5206\u985e(2\u5024\u5206\u985e) \u00b6 \u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f \u00b6 import numpy as np import pandas as pd import glob import time from tqdm import tqdm from sklearn import model_selection , metrics import lightgbm as lgb \u30c7\u30fc\u30bf\u306e\u78ba\u8a8d \u00b6 # \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f train_df = pd . read_csv ( \"../input/uec2021-exercise-1/train.csv\" ) test_df = pd . read_csv ( \"../input/uec2021-exercise-1/test.csv\" ) sample_submission_df = pd . read_csv ( \"../input/uec2021-exercise-1/sample_submission.csv\" ) train_df . shape , test_df . shape , sample_submission_df . shape train_df . head ( 2 ), test_df . head ( 2 ) train_df . columns sample_submission_df . head ( 2 ) \u30e2\u30c7\u30eb\u306e\u4f5c\u6210 \u00b6 class SingleLgb : def __init__ ( self , seed = 99 , dry_run = False ): self . train_param = self . get_param ( seed ) if dry_run : self . num_rounds = 100 else : self . num_rounds = 4000 def do_train_direct ( self , x_train , x_test , y_train , y_test ): lgb_train = lgb . Dataset ( x_train , y_train ) lgb_eval = lgb . Dataset ( x_test , y_test ) # print('Start training...') model = lgb . train ( self . train_param , lgb_train , valid_sets = [ lgb_eval ], verbose_eval = 400 , num_boost_round = self . num_rounds , early_stopping_rounds = 400 , #categorical_feature=[] ) # print('End training...') return model @staticmethod def show_feature_importance ( model , filename = None ): fi = pd . DataFrame ({ \"name\" : model . feature_name (), \"importance_split\" : model . feature_importance ( importance_type = \"split\" ) . astype ( int ), \"importance_gain\" : model . feature_importance ( importance_type = \"gain\" ) . astype ( int ), }) fi = fi . sort_values ( by = \"importance_gain\" , ascending = False ) print ( fi ) @staticmethod def get_param ( seed = 99 ): return { 'num_leaves' : 1023 , 'min_data_in_leaf' : 50 , 'objective' : 'binary' , 'metric' : 'AUC' , 'max_depth' : - 1 , 'learning_rate' : 0.05 , \"boosting\" : \"gbdt\" , \"feature_fraction\" : 0.9 , \"verbosity\" : - 1 , \"random_state\" : seed , } class SingleTrainer : def __init__ ( self , pred_col , dry_run = False ): self . pred_col = pred_col self . target_col = \"is_canceled\" self . dry_run = dry_run self . val_size = 1000 * 1000 def train_model ( self , df ): X = df [ self . pred_col ] y = df [ self . target_col ] kf = model_selection . KFold ( n_splits = 4 ) models , scores = list (), list () for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): print ( \"---------\" ) print ( \"fold=\" , fold ) X_train , X_val = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train , y_val = y . iloc [ train_idx ], y . iloc [ test_idx ] print ( X_train . shape , X_val . shape ) lgbm = SingleLgb ( seed = 99 , dry_run = self . dry_run ) model = lgbm . do_train_direct ( X_train , X_val , y_train , y_val ) score = model . best_score [ \"valid_0\" ][ \"auc\" ] if fold == 0 : lgbm . show_feature_importance ( model ) models . append ( model ) scores . append ( score ) #break return models , np . mean ( scores ) \u5b66\u7fd2\u306e\u5b9f\u884c \u00b6 pred_col = [ 'hotel' , 'lead_time' , 'stays_in_weekend_nights' , 'stays_in_week_nights' , 'adults' , 'children' , 'babies' , 'meal' , 'country' , 'market_segment' , 'distribution_channel' , 'is_repeated_guest' , 'previous_cancellations' , 'previous_bookings_not_canceled' , 'reserved_room_type' , 'assigned_room_type' , 'booking_changes' , 'deposit_type' , 'agent' , 'company' , 'days_in_waiting_list' , 'customer_type' , 'adr' , 'required_car_parking_spaces' , 'total_of_special_requests' ] temp_df = train_df . copy () temp_test = test_df . copy () cat_col = [ \"hotel\" , \"meal\" , \"country\" , \"market_segment\" , \"distribution_channel\" , \"reserved_room_type\" , \"assigned_room_type\" , \"deposit_type\" , \"customer_type\" ] for c in cat_col : temp_df [ c ] = temp_df [ c ] . astype ( \"category\" ) temp_test [ c ] = temp_test [ c ] . astype ( \"category\" ) trainer = SingleTrainer ( pred_col , dry_run = False ) models , score = trainer . train_model ( temp_df ) \u63a8\u8ad6 \u00b6 preds = [] for m in models : preds . append ( m . predict ( temp_test [ pred_col ])) # \u30c7\u30fc\u30bf\u4f5c\u6210 sub_df = pd . DataFrame ({ \"id\" : test_df [ \"id\" ], \"pred\" : np . mean ( preds , axis = 0 ) }) sub_df . head ( 2 ) sub_df [ \"pred\" ] . hist () train_df [ \"is_canceled\" ] . value_counts () # \u30d5\u30a1\u30a4\u30eb\u3078\u306e\u66f8\u304d\u51fa\u3057 sub_df . to_csv ( \"sub_cv_models.csv\" , index = False ) \u5206\u985e(\u591a\u5024\u5206\u985e) \u00b6 \u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f \u00b6 import numpy as np import pandas as pd import glob import time from tqdm import tqdm from sklearn import model_selection , metrics import lightgbm as lgb \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f/\u78ba\u8a8d \u00b6 train = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/train_student_info.csv\" ) test = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/test_student_info.csv\" ) train . shape , test . shape courses = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/courses.csv\" ) print ( courses . shape ) # \u30b3\u30fc\u30b9\u306e\u8a73\u7d30\u3067\u3001ID\u3067\u8868\u73fe\u3067\u304d\u306a\u304f\u306f\u306a\u3044\u306e\u3067\u4e00\u65e6\u7121\u8996\u3057\u3068\u304f registration = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/student_registration.csv\" ) print ( registration . shape ) student_assess = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/student_assessment.csv\" ) print ( student_assess . shape ) assess = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/assessments.csv\" ) print ( assess . shape ) assess . head ( 2 ) student_assess . head ( 2 ) train . head ( 2 ) \u524d\u51e6\u7406\u95a2\u6570 \u00b6 def make_sa_df (): sa = pd . merge ( student_assess , assess , on = \"id_assessment\" , how = \"left\" ) aggs = { \"date\" : [ \"max\" ], } key = \"id_student\" ret_df = sa . groupby ( key ) . agg ( aggs ) . reset_index () ret_df . columns = [ key ] + [ k + \"_\" + agg for k , v in aggs . items () for agg in v ] print ( ret_df . shape ) return ret_df def make_reg_df (): key = \"id_student\" aggs = { \"date_registration\" : [ \"mean\" ], } key = \"id_student\" ret_df = registration . groupby ( key ) . agg ( aggs ) . reset_index () ret_df . columns = [ key ] + [ k + \"_\" + agg for k , v in aggs . items () for agg in v ] print ( ret_df . shape ) return ret_df def merge_df ( main_df , sa_df , reg_df ): key = \"id_student\" ret_df = pd . merge ( main_df , reg_df , on = key , how = \"left\" ) ret_df = pd . merge ( ret_df , sa_df , on = key , how = \"left\" ) print ( ret_df . shape ) return ret_df def make_cat_cols ( df ): cat_cols = [ 'code_module' , 'code_presentation' , 'gender' , 'region' , 'highest_education' , 'imd_band' , 'age_band' , 'studied_credits' , 'disability' , ] for c in cat_cols : df [ c ] = df [ c ] . astype ( \"category\" ) return df sa_df = make_sa_df () reg_df = make_reg_df () train_df = merge_df ( train , sa_df , reg_df ) train_df = make_cat_cols ( train_df ) \u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u4f5c\u6210 \u00b6 class SingleLgb : def __init__ ( self , seed = 99 , dry_run = False ): self . train_param = self . get_param ( seed ) if dry_run : self . num_rounds = 100 else : self . num_rounds = 1000 def do_train_direct ( self , x_train , x_test , y_train , y_test ): lgb_train = lgb . Dataset ( x_train , y_train ) lgb_eval = lgb . Dataset ( x_test , y_test ) # print('Start training...') model = lgb . train ( self . train_param , lgb_train , valid_sets = [ lgb_eval ], verbose_eval = 100 , num_boost_round = self . num_rounds , early_stopping_rounds = 100 , #categorical_feature=[] ) # print('End training...') return model @staticmethod def show_feature_importance ( model , filename = None ): fi = pd . DataFrame ({ \"name\" : model . feature_name (), \"importance_split\" : model . feature_importance ( importance_type = \"split\" ) . astype ( int ), \"importance_gain\" : model . feature_importance ( importance_type = \"gain\" ) . astype ( int ), }) fi = fi . sort_values ( by = \"importance_gain\" , ascending = False ) print ( fi ) @staticmethod def get_param ( seed = 99 ): return { 'num_leaves' : 31 , 'min_data_in_leaf' : 50 , 'objective' : 'multiclass' , \"num_class\" : 4 , 'metric' : 'multi_logloss' , 'max_depth' : - 1 , 'learning_rate' : 0.05 , \"boosting\" : \"gbdt\" , \"feature_fraction\" : 0.9 , \"verbosity\" : - 1 , \"random_state\" : seed , } class SingleTrainer : def __init__ ( self , pred_col , dry_run = False ): self . pred_col = pred_col self . target_col = \"final_result_int\" self . dry_run = dry_run self . val_size = 1000 * 1000 def train_model ( self , df ): X = df [ self . pred_col ] y = df [ self . target_col ] kf = model_selection . KFold ( n_splits = 4 ) models , scores = list (), list () for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): print ( \"---------\" ) print ( \"fold=\" , fold ) X_train , X_val = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train , y_val = y . iloc [ train_idx ], y . iloc [ test_idx ] print ( X_train . shape , X_val . shape ) lgbm = SingleLgb ( seed = 99 , dry_run = self . dry_run ) model = lgbm . do_train_direct ( X_train , X_val , y_train , y_val ) score = model . best_score [ \"valid_0\" ][ \"multi_logloss\" ] if fold == 0 : lgbm . show_feature_importance ( model ) self . eval_model ( model , X_val , y_val ) models . append ( model ) scores . append ( score ) #break return models , np . mean ( scores ) def eval_model ( self , model , X_val , y_val ): temp_pred = model . predict ( X_val ) temp_pred = np . argmax ( temp_pred , axis = 1 ) acc_score = ( temp_pred == y_val ) . mean () print ( pd . DataFrame ( temp_pred ) . value_counts ()) print ( \"acc_score=\" , acc_score ) \u5b66\u7fd2\u306e\u5b9f\u884c \u00b6 pred_col = [ 'code_module' , 'code_presentation' , 'gender' , 'region' , 'highest_education' , 'imd_band' , 'age_band' , 'num_of_prev_attempts' , 'studied_credits' , 'disability' , 'date_registration_mean' , 'date_max' , ] temp_df = train_df . copy () target_map = { \"Distinction\" : 3 , \"Pass\" : 2 , \"Withdrawn\" : 0 , \"Fail\" : 1 , } temp_df [ \"final_result_int\" ] = temp_df [ \"final_result\" ] . map ( target_map ) . astype ( int ) trainer = SingleTrainer ( pred_col , dry_run = False ) models , score = trainer . train_model ( temp_df ) print ( score ) \u63a8\u8ad6 \u00b6 test_df = merge_df ( test , sa_df , reg_df ) test_df = make_cat_cols ( test_df ) test_df . shape preds = [] for m in models : preds . append ( m . predict ( test_df [ pred_col ])) pred = np . mean ( preds , axis = 0 ) # \u30e2\u30c7\u30eb\u305f\u3061\u306e\u5e73\u5747\u3092\u3068\u308b print ( pd . DataFrame ( pred ) . describe ()) pred = np . argmax ( pred , axis = 1 ) # \u4e88\u6e2c\u304c\u6700\u5927\u306e\u30ab\u30e9\u30e0\u3092\u4e88\u6e2c\u30af\u30e9\u30b9\u3068\u3059\u308b pred . shape , test_df . shape sub_df = pd . DataFrame ({ \"id\" : test_df [ \"id\" ], \"pred\" : pred }) sub_df [ \"pred\" ] . value_counts () temp_df [ \"final_result_int\" ] . value_counts () target_map = { 3 : \"Distinction\" , 2 : \"Pass\" , 0 : \"Withdrawn\" , 1 : \"Fail\" , } sub_df [ \"pred\" ] = sub_df [ \"pred\" ] . map ( target_map ) sub_df . head () # \u30d5\u30a1\u30a4\u30eb\u3078\u306e\u66f8\u304d\u51fa\u3057 sub_df . to_csv ( \"base_model.csv\" , index = False )","title":"LGBM Cls Template"},{"location":"lgb_template_cls/#lgbm-classification-template","text":"","title":"LGBM Classification Template"},{"location":"lgb_template_cls/#2","text":"","title":"\u5206\u985e(2\u5024\u5206\u985e)"},{"location":"lgb_template_cls/#_1","text":"import numpy as np import pandas as pd import glob import time from tqdm import tqdm from sklearn import model_selection , metrics import lightgbm as lgb","title":"\u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"lgb_template_cls/#_2","text":"# \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f train_df = pd . read_csv ( \"../input/uec2021-exercise-1/train.csv\" ) test_df = pd . read_csv ( \"../input/uec2021-exercise-1/test.csv\" ) sample_submission_df = pd . read_csv ( \"../input/uec2021-exercise-1/sample_submission.csv\" ) train_df . shape , test_df . shape , sample_submission_df . shape train_df . head ( 2 ), test_df . head ( 2 ) train_df . columns sample_submission_df . head ( 2 )","title":"\u30c7\u30fc\u30bf\u306e\u78ba\u8a8d"},{"location":"lgb_template_cls/#_3","text":"class SingleLgb : def __init__ ( self , seed = 99 , dry_run = False ): self . train_param = self . get_param ( seed ) if dry_run : self . num_rounds = 100 else : self . num_rounds = 4000 def do_train_direct ( self , x_train , x_test , y_train , y_test ): lgb_train = lgb . Dataset ( x_train , y_train ) lgb_eval = lgb . Dataset ( x_test , y_test ) # print('Start training...') model = lgb . train ( self . train_param , lgb_train , valid_sets = [ lgb_eval ], verbose_eval = 400 , num_boost_round = self . num_rounds , early_stopping_rounds = 400 , #categorical_feature=[] ) # print('End training...') return model @staticmethod def show_feature_importance ( model , filename = None ): fi = pd . DataFrame ({ \"name\" : model . feature_name (), \"importance_split\" : model . feature_importance ( importance_type = \"split\" ) . astype ( int ), \"importance_gain\" : model . feature_importance ( importance_type = \"gain\" ) . astype ( int ), }) fi = fi . sort_values ( by = \"importance_gain\" , ascending = False ) print ( fi ) @staticmethod def get_param ( seed = 99 ): return { 'num_leaves' : 1023 , 'min_data_in_leaf' : 50 , 'objective' : 'binary' , 'metric' : 'AUC' , 'max_depth' : - 1 , 'learning_rate' : 0.05 , \"boosting\" : \"gbdt\" , \"feature_fraction\" : 0.9 , \"verbosity\" : - 1 , \"random_state\" : seed , } class SingleTrainer : def __init__ ( self , pred_col , dry_run = False ): self . pred_col = pred_col self . target_col = \"is_canceled\" self . dry_run = dry_run self . val_size = 1000 * 1000 def train_model ( self , df ): X = df [ self . pred_col ] y = df [ self . target_col ] kf = model_selection . KFold ( n_splits = 4 ) models , scores = list (), list () for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): print ( \"---------\" ) print ( \"fold=\" , fold ) X_train , X_val = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train , y_val = y . iloc [ train_idx ], y . iloc [ test_idx ] print ( X_train . shape , X_val . shape ) lgbm = SingleLgb ( seed = 99 , dry_run = self . dry_run ) model = lgbm . do_train_direct ( X_train , X_val , y_train , y_val ) score = model . best_score [ \"valid_0\" ][ \"auc\" ] if fold == 0 : lgbm . show_feature_importance ( model ) models . append ( model ) scores . append ( score ) #break return models , np . mean ( scores )","title":"\u30e2\u30c7\u30eb\u306e\u4f5c\u6210"},{"location":"lgb_template_cls/#_4","text":"pred_col = [ 'hotel' , 'lead_time' , 'stays_in_weekend_nights' , 'stays_in_week_nights' , 'adults' , 'children' , 'babies' , 'meal' , 'country' , 'market_segment' , 'distribution_channel' , 'is_repeated_guest' , 'previous_cancellations' , 'previous_bookings_not_canceled' , 'reserved_room_type' , 'assigned_room_type' , 'booking_changes' , 'deposit_type' , 'agent' , 'company' , 'days_in_waiting_list' , 'customer_type' , 'adr' , 'required_car_parking_spaces' , 'total_of_special_requests' ] temp_df = train_df . copy () temp_test = test_df . copy () cat_col = [ \"hotel\" , \"meal\" , \"country\" , \"market_segment\" , \"distribution_channel\" , \"reserved_room_type\" , \"assigned_room_type\" , \"deposit_type\" , \"customer_type\" ] for c in cat_col : temp_df [ c ] = temp_df [ c ] . astype ( \"category\" ) temp_test [ c ] = temp_test [ c ] . astype ( \"category\" ) trainer = SingleTrainer ( pred_col , dry_run = False ) models , score = trainer . train_model ( temp_df )","title":"\u5b66\u7fd2\u306e\u5b9f\u884c"},{"location":"lgb_template_cls/#_5","text":"preds = [] for m in models : preds . append ( m . predict ( temp_test [ pred_col ])) # \u30c7\u30fc\u30bf\u4f5c\u6210 sub_df = pd . DataFrame ({ \"id\" : test_df [ \"id\" ], \"pred\" : np . mean ( preds , axis = 0 ) }) sub_df . head ( 2 ) sub_df [ \"pred\" ] . hist () train_df [ \"is_canceled\" ] . value_counts () # \u30d5\u30a1\u30a4\u30eb\u3078\u306e\u66f8\u304d\u51fa\u3057 sub_df . to_csv ( \"sub_cv_models.csv\" , index = False )","title":"\u63a8\u8ad6"},{"location":"lgb_template_cls/#_6","text":"","title":"\u5206\u985e(\u591a\u5024\u5206\u985e)"},{"location":"lgb_template_cls/#_7","text":"import numpy as np import pandas as pd import glob import time from tqdm import tqdm from sklearn import model_selection , metrics import lightgbm as lgb","title":"\u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"lgb_template_cls/#_8","text":"train = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/train_student_info.csv\" ) test = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/test_student_info.csv\" ) train . shape , test . shape courses = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/courses.csv\" ) print ( courses . shape ) # \u30b3\u30fc\u30b9\u306e\u8a73\u7d30\u3067\u3001ID\u3067\u8868\u73fe\u3067\u304d\u306a\u304f\u306f\u306a\u3044\u306e\u3067\u4e00\u65e6\u7121\u8996\u3057\u3068\u304f registration = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/student_registration.csv\" ) print ( registration . shape ) student_assess = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/student_assessment.csv\" ) print ( student_assess . shape ) assess = pd . read_csv ( \"/kaggle/input/uec2021-exercise-2/assessments.csv\" ) print ( assess . shape ) assess . head ( 2 ) student_assess . head ( 2 ) train . head ( 2 )","title":"\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f/\u78ba\u8a8d"},{"location":"lgb_template_cls/#_9","text":"def make_sa_df (): sa = pd . merge ( student_assess , assess , on = \"id_assessment\" , how = \"left\" ) aggs = { \"date\" : [ \"max\" ], } key = \"id_student\" ret_df = sa . groupby ( key ) . agg ( aggs ) . reset_index () ret_df . columns = [ key ] + [ k + \"_\" + agg for k , v in aggs . items () for agg in v ] print ( ret_df . shape ) return ret_df def make_reg_df (): key = \"id_student\" aggs = { \"date_registration\" : [ \"mean\" ], } key = \"id_student\" ret_df = registration . groupby ( key ) . agg ( aggs ) . reset_index () ret_df . columns = [ key ] + [ k + \"_\" + agg for k , v in aggs . items () for agg in v ] print ( ret_df . shape ) return ret_df def merge_df ( main_df , sa_df , reg_df ): key = \"id_student\" ret_df = pd . merge ( main_df , reg_df , on = key , how = \"left\" ) ret_df = pd . merge ( ret_df , sa_df , on = key , how = \"left\" ) print ( ret_df . shape ) return ret_df def make_cat_cols ( df ): cat_cols = [ 'code_module' , 'code_presentation' , 'gender' , 'region' , 'highest_education' , 'imd_band' , 'age_band' , 'studied_credits' , 'disability' , ] for c in cat_cols : df [ c ] = df [ c ] . astype ( \"category\" ) return df sa_df = make_sa_df () reg_df = make_reg_df () train_df = merge_df ( train , sa_df , reg_df ) train_df = make_cat_cols ( train_df )","title":"\u524d\u51e6\u7406\u95a2\u6570"},{"location":"lgb_template_cls/#_10","text":"class SingleLgb : def __init__ ( self , seed = 99 , dry_run = False ): self . train_param = self . get_param ( seed ) if dry_run : self . num_rounds = 100 else : self . num_rounds = 1000 def do_train_direct ( self , x_train , x_test , y_train , y_test ): lgb_train = lgb . Dataset ( x_train , y_train ) lgb_eval = lgb . Dataset ( x_test , y_test ) # print('Start training...') model = lgb . train ( self . train_param , lgb_train , valid_sets = [ lgb_eval ], verbose_eval = 100 , num_boost_round = self . num_rounds , early_stopping_rounds = 100 , #categorical_feature=[] ) # print('End training...') return model @staticmethod def show_feature_importance ( model , filename = None ): fi = pd . DataFrame ({ \"name\" : model . feature_name (), \"importance_split\" : model . feature_importance ( importance_type = \"split\" ) . astype ( int ), \"importance_gain\" : model . feature_importance ( importance_type = \"gain\" ) . astype ( int ), }) fi = fi . sort_values ( by = \"importance_gain\" , ascending = False ) print ( fi ) @staticmethod def get_param ( seed = 99 ): return { 'num_leaves' : 31 , 'min_data_in_leaf' : 50 , 'objective' : 'multiclass' , \"num_class\" : 4 , 'metric' : 'multi_logloss' , 'max_depth' : - 1 , 'learning_rate' : 0.05 , \"boosting\" : \"gbdt\" , \"feature_fraction\" : 0.9 , \"verbosity\" : - 1 , \"random_state\" : seed , } class SingleTrainer : def __init__ ( self , pred_col , dry_run = False ): self . pred_col = pred_col self . target_col = \"final_result_int\" self . dry_run = dry_run self . val_size = 1000 * 1000 def train_model ( self , df ): X = df [ self . pred_col ] y = df [ self . target_col ] kf = model_selection . KFold ( n_splits = 4 ) models , scores = list (), list () for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): print ( \"---------\" ) print ( \"fold=\" , fold ) X_train , X_val = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train , y_val = y . iloc [ train_idx ], y . iloc [ test_idx ] print ( X_train . shape , X_val . shape ) lgbm = SingleLgb ( seed = 99 , dry_run = self . dry_run ) model = lgbm . do_train_direct ( X_train , X_val , y_train , y_val ) score = model . best_score [ \"valid_0\" ][ \"multi_logloss\" ] if fold == 0 : lgbm . show_feature_importance ( model ) self . eval_model ( model , X_val , y_val ) models . append ( model ) scores . append ( score ) #break return models , np . mean ( scores ) def eval_model ( self , model , X_val , y_val ): temp_pred = model . predict ( X_val ) temp_pred = np . argmax ( temp_pred , axis = 1 ) acc_score = ( temp_pred == y_val ) . mean () print ( pd . DataFrame ( temp_pred ) . value_counts ()) print ( \"acc_score=\" , acc_score )","title":"\u5b66\u7fd2\u30e2\u30c7\u30eb\u306e\u4f5c\u6210"},{"location":"lgb_template_cls/#_11","text":"pred_col = [ 'code_module' , 'code_presentation' , 'gender' , 'region' , 'highest_education' , 'imd_band' , 'age_band' , 'num_of_prev_attempts' , 'studied_credits' , 'disability' , 'date_registration_mean' , 'date_max' , ] temp_df = train_df . copy () target_map = { \"Distinction\" : 3 , \"Pass\" : 2 , \"Withdrawn\" : 0 , \"Fail\" : 1 , } temp_df [ \"final_result_int\" ] = temp_df [ \"final_result\" ] . map ( target_map ) . astype ( int ) trainer = SingleTrainer ( pred_col , dry_run = False ) models , score = trainer . train_model ( temp_df ) print ( score )","title":"\u5b66\u7fd2\u306e\u5b9f\u884c"},{"location":"lgb_template_cls/#_12","text":"test_df = merge_df ( test , sa_df , reg_df ) test_df = make_cat_cols ( test_df ) test_df . shape preds = [] for m in models : preds . append ( m . predict ( test_df [ pred_col ])) pred = np . mean ( preds , axis = 0 ) # \u30e2\u30c7\u30eb\u305f\u3061\u306e\u5e73\u5747\u3092\u3068\u308b print ( pd . DataFrame ( pred ) . describe ()) pred = np . argmax ( pred , axis = 1 ) # \u4e88\u6e2c\u304c\u6700\u5927\u306e\u30ab\u30e9\u30e0\u3092\u4e88\u6e2c\u30af\u30e9\u30b9\u3068\u3059\u308b pred . shape , test_df . shape sub_df = pd . DataFrame ({ \"id\" : test_df [ \"id\" ], \"pred\" : pred }) sub_df [ \"pred\" ] . value_counts () temp_df [ \"final_result_int\" ] . value_counts () target_map = { 3 : \"Distinction\" , 2 : \"Pass\" , 0 : \"Withdrawn\" , 1 : \"Fail\" , } sub_df [ \"pred\" ] = sub_df [ \"pred\" ] . map ( target_map ) sub_df . head () # \u30d5\u30a1\u30a4\u30eb\u3078\u306e\u66f8\u304d\u51fa\u3057 sub_df . to_csv ( \"base_model.csv\" , index = False )","title":"\u63a8\u8ad6"},{"location":"lgb_template_reg/","text":"LGBM Regression Template \u00b6 \u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u4e88\u6e2c (Kaggle Electric Price Prediction) \u00b6 #data analysis libraries import numpy as np import pandas as pd import math #\u6700\u5927\u8868\u793a\u5217\u6570\u306e\u6307\u5b9a\uff08\u3053\u3053\u3067\u306f50\u5217\u3092\u6307\u5b9a\uff09 pd . set_option ( 'display.max_columns' , 50 ) #visualization libraries import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline #machinelearning libraries from sklearn.model_selection import TimeSeriesSplit from sklearn.metrics import mean_squared_error from statsmodels.graphics.tsaplots import plot_acf , plot_pacf import lightgbm as lgb import category_encoders as ce from sklearn.decomposition import PCA import tqdm from sklearn import preprocessing import warnings warnings . resetwarnings () warnings . simplefilter ( 'ignore' , FutureWarning ) warnings . simplefilter ( 'ignore' , UserWarning ) warnings . simplefilter ( 'ignore' , DeprecationWarning ) 0. Load_Data \u00b6 # azureml-core \u306e\u30d0\u30fc\u30b8\u30e7\u30f3 1.0.72 \u4ee5\u4e0a\u304c\u5fc5\u8981\u3067\u3059 # \u30d0\u30fc\u30b8\u30e7\u30f3 1.1.34 \u4ee5\u964d\u306e azureml-dataprep[pandas] \u304c\u5fc5\u8981\u3067\u3059 from azureml.core import Workspace , Dataset subscription_id = '7c8a3124-bfd6-4ad6-9036-0ac6bc860830' resource_group = 'Dept-PEN-ALG-D-RG' workspace_name = 'algworkspace' workspace = Workspace ( subscription_id , resource_group , workspace_name ) train = Dataset . get_by_name ( workspace , name = 'sp-act-01' ) . to_pandas_dataframe () test = Dataset . get_by_name ( workspace , name = 'sp-act-02' ) . to_pandas_dataframe () . reset_index ( drop = True ) weather = Dataset . get_by_name ( workspace , name = 'sp-act-03' ) . to_pandas_dataframe () . reset_index ( drop = True ) sample_sub = Dataset . get_by_name ( workspace , name = 'sp-act-04' ) . to_pandas_dataframe () . set_index ( \"id\" ) 1. Data Confirmation & Pre-Treatment \u00b6 #Train_Dataset display ( train . head ()) display ( train . shape ) #Test_Dataset display ( test . head ()) display ( test . shape ) #Weather_Dataset display ( weather . head ()) display ( weather . shape ) #Sample_submmision display ( sample_sub . head ()) display ( sample_sub . shape ) 1.1 Energy Dataset \u00b6 #Confirm data summary train . describe ( include = \"all\" ) # Drop unusable columns like NaN,0 # forecast data also dropped to pridict with actual data only drop_columns = [ 'id' , 'generation fossil coal-derived gas' , 'generation fossil oil shale' , 'generation fossil peat' , 'generation geothermal' , 'generation hydro pumped storage aggregated' , 'generation marine' , 'generation wind offshore' , 'forecast wind offshore eday ahead' , 'total load forecast' , 'forecast solar day ahead' , 'forecast wind onshore day ahead' ] train = train . drop ( drop_columns , axis = 1 ) test = test . drop ( drop_columns , axis = 1 ) #Confirm null data rate def get_null_df ( df : pd . DataFrame ): \"\"\"Calculate missing data rate Args: df (pd.DataFrame): target_df Returns: col_null_df: dataframe of null_feature_rate \"\"\" col_null_df = pd . DataFrame ( columns = [ 'Column' , 'Type' , 'Total NaN' , '%' ]) col_null = df . columns [ df . isna () . any ()] . to_list () L = len ( df ) for col in col_null : T = 0 if df [ col ] . dtype == \"float64\" : T = \"Numerical\" elif df [ col ] . dtype == \"int64\" : T = \"Numerical\" else : T = \"Categorical\" nulls = len ( df [ df [ col ] . isna () == True ][ col ]) col_null_df = col_null_df . append ({ 'Column' : col , 'Type' : T , 'Total NaN' : nulls , '%' : ( nulls / L ) * 100 }, ignore_index = True ) return col_null_df print ( \"Train_data\" ) display ( get_null_df ( train )) print ( \"Test_data\" ) display ( get_null_df ( test )) #Fill NaN values with ffill time_train = train [ \"time\" ] time_test = test [ \"time\" ] train = train . iloc [:, 2 :] . interpolate ( method = 'linear' , limit_direction = 'forward' , axis = 0 ) test = test . iloc [:, 2 :] . interpolate ( method = 'linear' , limit_direction = 'forward' , axis = 0 ) train . insert ( 0 , \"time\" , time_train ) test . insert ( 0 , \"time\" , time_test ) 1.2 Weather Data \u00b6 #Confirm data summary weather . describe ( include = \"all\" ) def get_unique_df ( df : pd . DataFrame ): \"\"\"Show unique column data Args: df (pd.dataFrame):target_df Returns: unique_df: unique_df_columns \"\"\" unique_df = pd . DataFrame ( columns = [ 'Feature' , 'Unique' , 'Count' ]) for col in df . columns : v = df [ col ] . unique () l = len ( v ) unique_df = unique_df . append ({ 'Feature' : col , 'Unique' : v , 'Count' : l }, ignore_index = True ) return unique_df get_unique_df ( weather ) weather . columns # Drop unusable columns drop_columns = [ 'year' ] weather = weather . drop ( drop_columns , axis = 1 ) #Confirm null data rate print ( \"Weather_data\" ) display ( get_null_df ( weather )) #delete indent weather = weather . replace ( \" Barcelona\" , \"Barcelona\" ) city_name = [ \"Valencia\" , \"Madrid\" , \"Bilbao\" , \"Barcelona\" , \"Seville\" ] for city in city_name : print ( city + \":\" + str ( len ( weather [ weather . city_name == city ] . reset_index ( drop = True )))) #make all city weather data frame #adjust time frame to Valencia(Min data number) def make_weather_dataset ( df , city_name = [ \"Madrid\" , \"Bilbao\" , \"Barcelona\" , \"Seville\" , \"Valencia\" ]): df_concat = pd . DataFrame () for city in city_name : city_df = weather [ weather . city_name == city ] . reset_index ( drop = True ) dt_iso = city_df [ \"dt_iso\" ] city_df = city_df . drop ( columns = \"city_name\" ) city_df . columns = [ str ( col ) + '_' + city for col in city_df . columns ] df_concat = pd . concat ([ df_concat , city_df . iloc [:, 1 :]], axis = 1 ) df_concat . insert ( 0 , \"time\" , dt_iso ) df_concat = df_concat [ ~ df_concat [ \"time\" ] . isnull ()] df_concat = df_concat . drop_duplicates ( subset = [ \"time\" ]) . reset_index ( drop = True ) return df_concat weather_concat = make_weather_dataset ( weather ) weather_concat 1.3 Merge Two Datasets \u00b6 #change date datatype train [ \"time\" ] = pd . to_datetime ( train [ \"time\" ]) weather_concat [ \"time\" ] = pd . to_datetime ( weather_concat [ \"time\" ]) #concat with weather_df train_all = pd . merge ( train , weather_concat , on = \"time\" , how = \"left\" ) test_all = pd . merge ( test , weather_concat , on = \"time\" , how = \"left\" ) # train_all=train_all.loc[:,~train_all.columns.duplicated()] # test_all=test_all.loc[:,~test_all.columns.duplicated()] display ( train . shape ) display ( train_all . shape ) display ( test . shape ) display ( test_all . shape ) 2. Visualization and Time Series Analysis \u00b6 #Confirm Distribution of train & test dataset def get_numeric_features_plot ( train : pd . DataFrame , test : pd . DataFrame , cont_features : list , height , figsize , hspace = .3 ): \"\"\"Show Numeric Features Distribution Args: train (pd.DataFrame): train_df test (pd.DataFrame): test_df cont_features (list): target_features height ([type]): plot_height figsize ([type]): plot_size hspace (float, optional): space of figs. Defaults to .3. \"\"\" ncols = 2 nrows = int ( math . ceil ( len ( cont_features ) / 2 )) fig , axs = plt . subplots ( ncols = ncols , nrows = nrows , figsize = ( height * 2 , height * nrows )) plt . subplots_adjust ( right = 1.5 , hspace = hspace ) for i , feature in enumerate ( cont_features ): plt . subplot ( nrows , ncols , i + 1 ) # Distribution of target features sns . distplot ( train [ feature ], label = 'Train' , hist = True , color = '#e74c3c' ) sns . distplot ( test [ feature ], label = 'Test' , hist = True , color = '#2ecc71' ) plt . xlabel ( ' {} ' . format ( feature ), size = figsize , labelpad = 15 ) plt . ylabel ( 'Density' , size = figsize , labelpad = 15 ) plt . tick_params ( axis = 'x' , labelsize = figsize ) plt . tick_params ( axis = 'y' , labelsize = figsize ) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . title ( 'Distribution of {} Feature' . format ( feature ), size = figsize , y = 1.05 ) plt . show () #Get only float and int columns cont_features = list ( train_all . select_dtypes ( include = 'float' or 'int' ) . columns ) cont_features . remove ( \"price actual\" ) get_numeric_features_plot ( train_all , test_all , cont_features = cont_features , height = 4 , figsize = 10 , hspace = .7 ) # Categorical Features def get_categorical_features_plot ( train : pd . DataFrame , test : pd . DataFrame , cat_features : list , height , figsize , hspace = .3 ): \"\"\"Show Numeric Features Distribution Args: train (pd.DataFrame): train_df test (pd.DataFrame): test_df cat_features (list): target_features height ([type]): plot_height figsize ([type]): plot_size hspace (float, optional): space of figs. Defaults to .3. \"\"\" ncols = 2 nrows = int ( math . ceil ( len ( cat_features ) / 2 )) train [ \"type\" ] = \"train\" test [ \"type\" ] = \"test\" whole_df = pd . concat ([ train , test ], axis = 0 ) . reset_index ( drop = True ) fig , axs = plt . subplots ( ncols = ncols , nrows = nrows , figsize = ( height * 2 , height * nrows )) plt . subplots_adjust ( right = 1.5 , hspace = hspace ) for i , feature in enumerate ( cat_features ): plt . subplot ( nrows , ncols , i + 1 ) # Distribution of target features ax = sns . countplot ( data = whole_df , x = feature , hue = \"type\" ) ax . set_xticklabels ( ax . get_xticklabels (), rotation = 90 ) plt . xlabel ( ' {} ' . format ( feature ), size = figsize , labelpad = 15 ) plt . ylabel ( 'Density' , size = figsize , labelpad = 15 ) plt . tick_params ( axis = 'x' , labelsize = figsize ) plt . tick_params ( axis = 'y' , labelsize = figsize ) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . title ( 'Count of {} Feature' . format ( feature ), size = figsize , y = 1.05 ) plt . show () #Get only object columns cat_features = list ( train_all . select_dtypes ( include = 'object' ) . columns ) get_categorical_features_plot ( train_all , test_all , cat_features = cat_features , height = 8 , figsize = 10 , hspace = 1.3 ) #Autocorrelation (3 yeasrs 2015-2017) rolling = train_all [ 'price actual' ] . rolling ( 24 * 7 , center = True ) . mean () fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( nrows = 3 , figsize = ( 15 , 8 )) ax1 . plot ( rolling , linestyle = '-' , linewidth = 2 , label = 'Weekly rolling mean' ) ax1 . set_title ( \"price actual\" ) plot_acf ( train_all [ 'price actual' ], lags = 50 , ax = ax2 ) plot_pacf ( train_all [ 'price actual' ], lags = 50 , ax = ax3 ) plt . tight_layout () plt . show () #Autocorrelation (1 year 2015) train_2015 = train_all . iloc [: 8760 ,:] rolling = train_2015 [ 'price actual' ] . rolling ( 24 * 7 , center = True ) . mean () fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( nrows = 3 , figsize = ( 15 , 8 )) ax1 . plot ( rolling , linestyle = '-' , linewidth = 2 , label = 'Weekly rolling mean' ) ax1 . set_title ( \"price actual\" ) plot_acf ( train_2015 [ 'price actual' ], lags = 50 , ax = ax2 ) plot_pacf ( train_2015 [ 'price actual' ], lags = 50 , ax = ax3 ) plt . tight_layout () plt . show () # Find the correlations between the electricity price and the rest of the features correlations = train_all . corr ( method = 'pearson' ) highly_correlated = abs ( correlations [ correlations > 0.7 ]) print ( highly_correlated [ highly_correlated < 1.0 ] . stack ()) 3. Feature Engineering \u00b6 Parameters \u00b6 class CONFIG : exp = \"exp11\" lag = 168 fold = 3 depth = 3 3.1 Time Features \u00b6 ## Make Time features by sin/cos def make_time ( input_df ): output_df = pd . DataFrame () output_df [ \"time\" ] = input_df [ \"time\" ] #output_df[\"weekday\"]=\"\" # for i in range(len(output_df)): # position = output_df[\"time\"][i] # weekday = position.weekday() # if (weekday == 6): # output_df.loc[position, 'weekday'] = 2 # elif (weekday == 5): # output_df.loc[position, 'weekday'] = 1 # else: # output_df.loc[position, 'weekday'] = 0 output_df [ \"month\" ] = output_df [ \"time\" ] . apply ( lambda x : x . month ) output_df [ \"day\" ] = output_df [ \"time\" ] . apply ( lambda x : x . day ) output_df [ \"year\" ] = output_df [ \"time\" ] . apply ( lambda x : x . year ) #Monthyly trend by Day output_df [ \"month_sin\" ] = np . sin ( output_df [ \"day\" ] / 31 * np . pi * 2 ) output_df [ \"month_cos\" ] = np . cos ( output_df [ \"day\" ] / 31 * np . pi * 2 ) #Yearly trend by Month output_df [ \"year_sin\" ] = np . sin ( output_df [ \"month\" ] / 12 * np . pi * 2 ) output_df [ \"year_cos\" ] = np . cos ( output_df [ \"month\" ] / 12 * np . pi * 2 ) #Quater trend by month output_df [ \"quater_sin\" ] = np . sin ( output_df [ \"month\" ] / 4 * np . pi * 2 ) output_df [ \"quater_cos\" ] = np . cos ( output_df [ \"month\" ] / 4 * np . pi * 2 ) #Half Yearly trend by Month output_df [ \"half_sin\" ] = np . sin ( output_df [ \"month\" ] / 6 * np . pi * 2 ) output_df [ \"half_cos\" ] = np . cos ( output_df [ \"month\" ] / 6 * np . pi * 2 ) #2 Month Trend by Month output_df [ \"2month_sin\" ] = np . sin ( output_df [ \"month\" ] / 2 * np . pi * 2 ) output_df [ \"2month_cos\" ] = np . cos ( output_df [ \"month\" ] / 2 * np . pi * 2 ) output_df = output_df . drop ( columns = [ \"time\" ]) # print(input_df.shape) # print(output_df.shape) # print(len(input_df.columns.unique())) # print(len(output_df.columns.unique())) return output_df 3.2 Make Lag Feature \u00b6 def make_lag ( input_df ): #Select only highly correlated columns for lag features targets = list ( highly_correlated [ highly_correlated < 1.0 ] . dropna ( how = \"all\" ) . index ) targets . remove ( \"price actual\" ) #1 day #shift_time=args.lag shift_time = CONFIG . lag output_df = input_df [ targets ] . copy () for target in targets : for i in range ( shift_time ): output_df [ target + f \"_lag { i + 1 } \" ] = output_df [ target ] . shift ( i + 1 ) output_df = output_df . fillna ( 0 ) output_df = output_df . iloc [:, len ( targets ):] return output_df 3.3 Encording \u00b6 def label_encoding ( input_df ): target_cols = list ( input_df . select_dtypes ( include = 'object' ) . columns ) features = input_df [ target_cols ] encoder = ce . OrdinalEncoder () . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"LE_\" ) output_df = output_df . astype ( str ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) output_df = output_df . iloc [:, - len ( target_cols ):] return output_df def count_encoding ( input_df ): target_cols = list ( input_df . select_dtypes ( include = 'object' ) . columns ) features = input_df [ target_cols ] encoder = ce . CountEncoder () . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"CE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) output_df = output_df . iloc [:, - len ( target_cols ):] return output_df def target_encoding ( input_df ): target_cols = list ( input_df . select_dtypes ( include = 'object' ) . columns ) features = input_df [ target_cols ] target = input_df [ \"price actual\" ] encoder = ce . TargetEncoder () . fit ( features . values , target ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"TE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) output_df = output_df . iloc [:, - len ( target_cols ):] return output_df 3.4 Apply Feature Enginering to Dataset \u00b6 def preprocess ( train , test ): input_df = pd . concat ([ train , test ]) . reset_index ( drop = True ) funcs = [ make_time , label_encoding , count_encoding , target_encoding , make_lag ] output = [] for func in tqdm . tqdm ( funcs ): _df = func ( input_df ) output . append ( _df ) output = pd . concat ( output , axis = 1 ) output = pd . concat ([ input_df , output ], axis = 1 ) # output=standard_scaler(output) # output=select_features(output) drop_cols = list ( input_df . select_dtypes ( include = 'object' ) . columns ) output = output . drop ( columns = drop_cols ) train_x = output . iloc [: len ( train )] test_x = output . iloc [ len ( train ):] . reset_index ( drop = True ) test_x = test_x . drop ( columns = [ \"price actual\" ]) return train_x , test_x train_x , test_x = preprocess ( train_all , test_all ) train_x . head () test_x . head () 4. Model Development \u00b6 train_x . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 26305 entries, 0 to 26304 Columns: 3162 entries, time to temp_max_Valencia_lag168 dtypes: datetime64[ns](1), float64(3120), int64(25), object(16) memory usage: 634.6+ MB #Get Categorical features cat_col = list ( train_x . select_dtypes ( include = 'object' ) . columns ) train_x [ cat_col ] = train_x [ cat_col ] . astype ( int ) test_x [ cat_col ] = test_x [ cat_col ] . astype ( int ) #Get Columns for Predict pred_col = list ( train_x . columns ) pred_col . remove ( \"price actual\" ) pred_col . remove ( \"time\" ) # ==================================================== # Model Preparation # ==================================================== class SingleLgb : def __init__ ( self , cat_col , seed = 71 , dry_run = False ): self . train_param = self . get_param ( seed ) if dry_run : self . num_rounds = 2000 else : self . num_rounds = 10000 self . cat_col = cat_col def do_train_direct ( self , x_train , x_test , y_train , y_test ): lgb_train = lgb . Dataset ( x_train , y_train ) lgb_eval = lgb . Dataset ( x_test , y_test ) print ( 'Start training...' ) #LOGGER.info('Start training...') model = lgb . train ( self . train_param , lgb_train , valid_sets = [ lgb_eval ], verbose_eval = 100 , num_boost_round = self . num_rounds , early_stopping_rounds = 100 , categorical_feature = self . cat_col ) print ( 'End training...' ) #LOGGER.info('End training...') return model @staticmethod def show_feature_importance ( model , filename = None ): fi = pd . DataFrame ({ \"name\" : model . feature_name (), \"importance_split\" : model . feature_importance ( importance_type = \"split\" ) . astype ( int ), \"importance_gain\" : model . feature_importance ( importance_type = \"gain\" ) . astype ( int ), }) fi = fi . sort_values ( by = \"importance_gain\" , ascending = False ) print ( fi ) #LOGGER.info(fi) @staticmethod def get_param ( seed = 71 ): return { 'num_leaves' : 1023 , 'min_data_in_leaf' : 50 , 'objective' : 'regression' , 'metric' : 'rmse' , 'max_depth' : CONFIG . depth , 'learning_rate' : 0.05 , \"boosting\" : \"gbdt\" , \"feature_fraction\" : 0.9 , \"verbosity\" : - 1 , \"random_state\" : seed , } class SingleTrainer : def __init__ ( self , pred_col , cat_col , dry_run = False ): self . pred_col = pred_col self . target_col = 'price actual' self . dry_run = dry_run self . cat_col = cat_col def adjust_price ( self , x ): price_list = [] for item in x : remain = item % 25 if remain >= 12.5 : item = ( item - remain ) + 25 price_list . append ( item ) else : item = item - remain price_list . append ( item ) return price_list def train_model ( self , df ): X = df [ self . pred_col ] y = df [ self . target_col ] kf = TimeSeriesSplit ( n_splits = CONFIG . fold ) models , scores = list (), list () for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): print ( \"---------\" ) print ( \"fold=\" , fold ) #LOGGER.info(\"---------\") #LOGGER.info(f\"fold={fold}\") X_train , X_val = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train , y_val = y . iloc [ train_idx ], y . iloc [ test_idx ] print ( X_train . shape , X_val . shape ) lgbm = SingleLgb ( seed = 71 , dry_run = self . dry_run , cat_col = self . cat_col ) model = lgbm . do_train_direct ( X_train , X_val , y_train , y_val ) y_pred = model . predict ( X_val ) y_pred = self . adjust_price ( y_pred ) score = np . sqrt ( mean_squared_error ( y_val , y_pred )) if fold == 0 : lgbm . show_feature_importance ( model ) models . append ( model ) scores . append ( score ) print ( f 'fold= { fold } RMSE Score=' + str ( score )) #LOGGER.info(f'fold= {fold} | MAE Score='+str(score)) #LOGGER.info(np.mean(scores)) return models , np . mean ( scores ) 5. Training \u00b6 trainer = SingleTrainer ( pred_col , cat_col = cat_col , dry_run = False ) models , score = trainer . train_model ( train_x ) print ( score ) preds_train = [] for m in models : preds_train . append ( m . predict ( train_x [ pred_col ])) pred_train = np . mean ( preds_train , axis = 0 ) # \u30e2\u30c7\u30eb\u305f\u3061\u306e\u5e73\u5747\u3092\u3068\u308b fig , ax = plt . subplots ( figsize = ( 8 , 8 )) sns . histplot ( train_x [ \"price actual\" ], label = 'Actual' , ax = ax , color = 'black' ) sns . histplot ( pred_train , label = 'Predict' , ax = ax , color = 'C1' ) ax . legend () ax . grid () #yy-plot def yy_plot ( y_obs , y_pred ): yvalues = np . concatenate ([ y_obs , y_pred ]) ymin , ymax , yrange = np . amin ( yvalues ), np . amax ( yvalues ), np . ptp ( yvalues ) plt . figure ( figsize = ( 8 , 8 )) plt . scatter ( y_obs , y_pred , alpha = .6 ) plt . plot ([ ymin - yrange * 0.01 , ymax + yrange * 0.01 ], [ ymin - yrange * 0.01 , ymax + yrange * 0.01 ]) #plt.xlim(10, 17) #plt.ylim(10,17) plt . xlabel ( 'y_observed' , fontsize = 24 ) plt . ylabel ( 'y_predicted' , fontsize = 24 ) plt . title ( 'Observed-Predicted Plot' , fontsize = 24 ) plt . tick_params ( labelsize = 16 ) yy_plot ( y_obs = train_x [ \"price actual\" ] . values , y_pred = pred_train ) 6. Inference \u00b6 preds = [] for m in models : preds . append ( m . predict ( test_x [ pred_col ])) pred = np . mean ( preds , axis = 0 ) # \u30e2\u30c7\u30eb\u305f\u3061\u306e\u5e73\u5747\u3092\u3068\u308b sub_df = sample_sub . copy () sub_df [ \"predicted\" ] = pred 7. Make Submit File \u00b6 #\u63d0\u51fa\u30b5\u30f3\u30d7\u30eb\u30d5\u30a1\u30a4\u30eb\u306e\u78ba\u8a8d sub_df #csv\u3068\u3057\u3066\u5410\u304d\u51fa\u3059 sub_df . to_csv ( f \"submit_\" + CONFIG . exp + \".csv\" )","title":"LGBM Reg Templete"},{"location":"lgb_template_reg/#lgbm-regression-template","text":"","title":"LGBM Regression Template"},{"location":"lgb_template_reg/#kaggle-electric-price-prediction","text":"#data analysis libraries import numpy as np import pandas as pd import math #\u6700\u5927\u8868\u793a\u5217\u6570\u306e\u6307\u5b9a\uff08\u3053\u3053\u3067\u306f50\u5217\u3092\u6307\u5b9a\uff09 pd . set_option ( 'display.max_columns' , 50 ) #visualization libraries import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline #machinelearning libraries from sklearn.model_selection import TimeSeriesSplit from sklearn.metrics import mean_squared_error from statsmodels.graphics.tsaplots import plot_acf , plot_pacf import lightgbm as lgb import category_encoders as ce from sklearn.decomposition import PCA import tqdm from sklearn import preprocessing import warnings warnings . resetwarnings () warnings . simplefilter ( 'ignore' , FutureWarning ) warnings . simplefilter ( 'ignore' , UserWarning ) warnings . simplefilter ( 'ignore' , DeprecationWarning )","title":"\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u4e88\u6e2c (Kaggle Electric Price Prediction)"},{"location":"lgb_template_reg/#0-load_data","text":"# azureml-core \u306e\u30d0\u30fc\u30b8\u30e7\u30f3 1.0.72 \u4ee5\u4e0a\u304c\u5fc5\u8981\u3067\u3059 # \u30d0\u30fc\u30b8\u30e7\u30f3 1.1.34 \u4ee5\u964d\u306e azureml-dataprep[pandas] \u304c\u5fc5\u8981\u3067\u3059 from azureml.core import Workspace , Dataset subscription_id = '7c8a3124-bfd6-4ad6-9036-0ac6bc860830' resource_group = 'Dept-PEN-ALG-D-RG' workspace_name = 'algworkspace' workspace = Workspace ( subscription_id , resource_group , workspace_name ) train = Dataset . get_by_name ( workspace , name = 'sp-act-01' ) . to_pandas_dataframe () test = Dataset . get_by_name ( workspace , name = 'sp-act-02' ) . to_pandas_dataframe () . reset_index ( drop = True ) weather = Dataset . get_by_name ( workspace , name = 'sp-act-03' ) . to_pandas_dataframe () . reset_index ( drop = True ) sample_sub = Dataset . get_by_name ( workspace , name = 'sp-act-04' ) . to_pandas_dataframe () . set_index ( \"id\" )","title":"0. Load_Data"},{"location":"lgb_template_reg/#1-data-confirmation-pre-treatment","text":"#Train_Dataset display ( train . head ()) display ( train . shape ) #Test_Dataset display ( test . head ()) display ( test . shape ) #Weather_Dataset display ( weather . head ()) display ( weather . shape ) #Sample_submmision display ( sample_sub . head ()) display ( sample_sub . shape )","title":"1. Data Confirmation &amp; Pre-Treatment"},{"location":"lgb_template_reg/#11-energy-dataset","text":"#Confirm data summary train . describe ( include = \"all\" ) # Drop unusable columns like NaN,0 # forecast data also dropped to pridict with actual data only drop_columns = [ 'id' , 'generation fossil coal-derived gas' , 'generation fossil oil shale' , 'generation fossil peat' , 'generation geothermal' , 'generation hydro pumped storage aggregated' , 'generation marine' , 'generation wind offshore' , 'forecast wind offshore eday ahead' , 'total load forecast' , 'forecast solar day ahead' , 'forecast wind onshore day ahead' ] train = train . drop ( drop_columns , axis = 1 ) test = test . drop ( drop_columns , axis = 1 ) #Confirm null data rate def get_null_df ( df : pd . DataFrame ): \"\"\"Calculate missing data rate Args: df (pd.DataFrame): target_df Returns: col_null_df: dataframe of null_feature_rate \"\"\" col_null_df = pd . DataFrame ( columns = [ 'Column' , 'Type' , 'Total NaN' , '%' ]) col_null = df . columns [ df . isna () . any ()] . to_list () L = len ( df ) for col in col_null : T = 0 if df [ col ] . dtype == \"float64\" : T = \"Numerical\" elif df [ col ] . dtype == \"int64\" : T = \"Numerical\" else : T = \"Categorical\" nulls = len ( df [ df [ col ] . isna () == True ][ col ]) col_null_df = col_null_df . append ({ 'Column' : col , 'Type' : T , 'Total NaN' : nulls , '%' : ( nulls / L ) * 100 }, ignore_index = True ) return col_null_df print ( \"Train_data\" ) display ( get_null_df ( train )) print ( \"Test_data\" ) display ( get_null_df ( test )) #Fill NaN values with ffill time_train = train [ \"time\" ] time_test = test [ \"time\" ] train = train . iloc [:, 2 :] . interpolate ( method = 'linear' , limit_direction = 'forward' , axis = 0 ) test = test . iloc [:, 2 :] . interpolate ( method = 'linear' , limit_direction = 'forward' , axis = 0 ) train . insert ( 0 , \"time\" , time_train ) test . insert ( 0 , \"time\" , time_test )","title":"1.1 Energy Dataset"},{"location":"lgb_template_reg/#12-weather-data","text":"#Confirm data summary weather . describe ( include = \"all\" ) def get_unique_df ( df : pd . DataFrame ): \"\"\"Show unique column data Args: df (pd.dataFrame):target_df Returns: unique_df: unique_df_columns \"\"\" unique_df = pd . DataFrame ( columns = [ 'Feature' , 'Unique' , 'Count' ]) for col in df . columns : v = df [ col ] . unique () l = len ( v ) unique_df = unique_df . append ({ 'Feature' : col , 'Unique' : v , 'Count' : l }, ignore_index = True ) return unique_df get_unique_df ( weather ) weather . columns # Drop unusable columns drop_columns = [ 'year' ] weather = weather . drop ( drop_columns , axis = 1 ) #Confirm null data rate print ( \"Weather_data\" ) display ( get_null_df ( weather )) #delete indent weather = weather . replace ( \" Barcelona\" , \"Barcelona\" ) city_name = [ \"Valencia\" , \"Madrid\" , \"Bilbao\" , \"Barcelona\" , \"Seville\" ] for city in city_name : print ( city + \":\" + str ( len ( weather [ weather . city_name == city ] . reset_index ( drop = True )))) #make all city weather data frame #adjust time frame to Valencia(Min data number) def make_weather_dataset ( df , city_name = [ \"Madrid\" , \"Bilbao\" , \"Barcelona\" , \"Seville\" , \"Valencia\" ]): df_concat = pd . DataFrame () for city in city_name : city_df = weather [ weather . city_name == city ] . reset_index ( drop = True ) dt_iso = city_df [ \"dt_iso\" ] city_df = city_df . drop ( columns = \"city_name\" ) city_df . columns = [ str ( col ) + '_' + city for col in city_df . columns ] df_concat = pd . concat ([ df_concat , city_df . iloc [:, 1 :]], axis = 1 ) df_concat . insert ( 0 , \"time\" , dt_iso ) df_concat = df_concat [ ~ df_concat [ \"time\" ] . isnull ()] df_concat = df_concat . drop_duplicates ( subset = [ \"time\" ]) . reset_index ( drop = True ) return df_concat weather_concat = make_weather_dataset ( weather ) weather_concat","title":"1.2 Weather Data"},{"location":"lgb_template_reg/#13-merge-two-datasets","text":"#change date datatype train [ \"time\" ] = pd . to_datetime ( train [ \"time\" ]) weather_concat [ \"time\" ] = pd . to_datetime ( weather_concat [ \"time\" ]) #concat with weather_df train_all = pd . merge ( train , weather_concat , on = \"time\" , how = \"left\" ) test_all = pd . merge ( test , weather_concat , on = \"time\" , how = \"left\" ) # train_all=train_all.loc[:,~train_all.columns.duplicated()] # test_all=test_all.loc[:,~test_all.columns.duplicated()] display ( train . shape ) display ( train_all . shape ) display ( test . shape ) display ( test_all . shape )","title":"1.3 Merge Two Datasets"},{"location":"lgb_template_reg/#2-visualization-and-time-series-analysis","text":"#Confirm Distribution of train & test dataset def get_numeric_features_plot ( train : pd . DataFrame , test : pd . DataFrame , cont_features : list , height , figsize , hspace = .3 ): \"\"\"Show Numeric Features Distribution Args: train (pd.DataFrame): train_df test (pd.DataFrame): test_df cont_features (list): target_features height ([type]): plot_height figsize ([type]): plot_size hspace (float, optional): space of figs. Defaults to .3. \"\"\" ncols = 2 nrows = int ( math . ceil ( len ( cont_features ) / 2 )) fig , axs = plt . subplots ( ncols = ncols , nrows = nrows , figsize = ( height * 2 , height * nrows )) plt . subplots_adjust ( right = 1.5 , hspace = hspace ) for i , feature in enumerate ( cont_features ): plt . subplot ( nrows , ncols , i + 1 ) # Distribution of target features sns . distplot ( train [ feature ], label = 'Train' , hist = True , color = '#e74c3c' ) sns . distplot ( test [ feature ], label = 'Test' , hist = True , color = '#2ecc71' ) plt . xlabel ( ' {} ' . format ( feature ), size = figsize , labelpad = 15 ) plt . ylabel ( 'Density' , size = figsize , labelpad = 15 ) plt . tick_params ( axis = 'x' , labelsize = figsize ) plt . tick_params ( axis = 'y' , labelsize = figsize ) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . title ( 'Distribution of {} Feature' . format ( feature ), size = figsize , y = 1.05 ) plt . show () #Get only float and int columns cont_features = list ( train_all . select_dtypes ( include = 'float' or 'int' ) . columns ) cont_features . remove ( \"price actual\" ) get_numeric_features_plot ( train_all , test_all , cont_features = cont_features , height = 4 , figsize = 10 , hspace = .7 ) # Categorical Features def get_categorical_features_plot ( train : pd . DataFrame , test : pd . DataFrame , cat_features : list , height , figsize , hspace = .3 ): \"\"\"Show Numeric Features Distribution Args: train (pd.DataFrame): train_df test (pd.DataFrame): test_df cat_features (list): target_features height ([type]): plot_height figsize ([type]): plot_size hspace (float, optional): space of figs. Defaults to .3. \"\"\" ncols = 2 nrows = int ( math . ceil ( len ( cat_features ) / 2 )) train [ \"type\" ] = \"train\" test [ \"type\" ] = \"test\" whole_df = pd . concat ([ train , test ], axis = 0 ) . reset_index ( drop = True ) fig , axs = plt . subplots ( ncols = ncols , nrows = nrows , figsize = ( height * 2 , height * nrows )) plt . subplots_adjust ( right = 1.5 , hspace = hspace ) for i , feature in enumerate ( cat_features ): plt . subplot ( nrows , ncols , i + 1 ) # Distribution of target features ax = sns . countplot ( data = whole_df , x = feature , hue = \"type\" ) ax . set_xticklabels ( ax . get_xticklabels (), rotation = 90 ) plt . xlabel ( ' {} ' . format ( feature ), size = figsize , labelpad = 15 ) plt . ylabel ( 'Density' , size = figsize , labelpad = 15 ) plt . tick_params ( axis = 'x' , labelsize = figsize ) plt . tick_params ( axis = 'y' , labelsize = figsize ) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . legend ( loc = 'upper right' , prop = { 'size' : figsize }) plt . title ( 'Count of {} Feature' . format ( feature ), size = figsize , y = 1.05 ) plt . show () #Get only object columns cat_features = list ( train_all . select_dtypes ( include = 'object' ) . columns ) get_categorical_features_plot ( train_all , test_all , cat_features = cat_features , height = 8 , figsize = 10 , hspace = 1.3 ) #Autocorrelation (3 yeasrs 2015-2017) rolling = train_all [ 'price actual' ] . rolling ( 24 * 7 , center = True ) . mean () fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( nrows = 3 , figsize = ( 15 , 8 )) ax1 . plot ( rolling , linestyle = '-' , linewidth = 2 , label = 'Weekly rolling mean' ) ax1 . set_title ( \"price actual\" ) plot_acf ( train_all [ 'price actual' ], lags = 50 , ax = ax2 ) plot_pacf ( train_all [ 'price actual' ], lags = 50 , ax = ax3 ) plt . tight_layout () plt . show () #Autocorrelation (1 year 2015) train_2015 = train_all . iloc [: 8760 ,:] rolling = train_2015 [ 'price actual' ] . rolling ( 24 * 7 , center = True ) . mean () fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( nrows = 3 , figsize = ( 15 , 8 )) ax1 . plot ( rolling , linestyle = '-' , linewidth = 2 , label = 'Weekly rolling mean' ) ax1 . set_title ( \"price actual\" ) plot_acf ( train_2015 [ 'price actual' ], lags = 50 , ax = ax2 ) plot_pacf ( train_2015 [ 'price actual' ], lags = 50 , ax = ax3 ) plt . tight_layout () plt . show () # Find the correlations between the electricity price and the rest of the features correlations = train_all . corr ( method = 'pearson' ) highly_correlated = abs ( correlations [ correlations > 0.7 ]) print ( highly_correlated [ highly_correlated < 1.0 ] . stack ())","title":"2. Visualization and Time Series Analysis"},{"location":"lgb_template_reg/#3-feature-engineering","text":"","title":"3. Feature Engineering"},{"location":"lgb_template_reg/#parameters","text":"class CONFIG : exp = \"exp11\" lag = 168 fold = 3 depth = 3","title":"Parameters"},{"location":"lgb_template_reg/#31-time-features","text":"## Make Time features by sin/cos def make_time ( input_df ): output_df = pd . DataFrame () output_df [ \"time\" ] = input_df [ \"time\" ] #output_df[\"weekday\"]=\"\" # for i in range(len(output_df)): # position = output_df[\"time\"][i] # weekday = position.weekday() # if (weekday == 6): # output_df.loc[position, 'weekday'] = 2 # elif (weekday == 5): # output_df.loc[position, 'weekday'] = 1 # else: # output_df.loc[position, 'weekday'] = 0 output_df [ \"month\" ] = output_df [ \"time\" ] . apply ( lambda x : x . month ) output_df [ \"day\" ] = output_df [ \"time\" ] . apply ( lambda x : x . day ) output_df [ \"year\" ] = output_df [ \"time\" ] . apply ( lambda x : x . year ) #Monthyly trend by Day output_df [ \"month_sin\" ] = np . sin ( output_df [ \"day\" ] / 31 * np . pi * 2 ) output_df [ \"month_cos\" ] = np . cos ( output_df [ \"day\" ] / 31 * np . pi * 2 ) #Yearly trend by Month output_df [ \"year_sin\" ] = np . sin ( output_df [ \"month\" ] / 12 * np . pi * 2 ) output_df [ \"year_cos\" ] = np . cos ( output_df [ \"month\" ] / 12 * np . pi * 2 ) #Quater trend by month output_df [ \"quater_sin\" ] = np . sin ( output_df [ \"month\" ] / 4 * np . pi * 2 ) output_df [ \"quater_cos\" ] = np . cos ( output_df [ \"month\" ] / 4 * np . pi * 2 ) #Half Yearly trend by Month output_df [ \"half_sin\" ] = np . sin ( output_df [ \"month\" ] / 6 * np . pi * 2 ) output_df [ \"half_cos\" ] = np . cos ( output_df [ \"month\" ] / 6 * np . pi * 2 ) #2 Month Trend by Month output_df [ \"2month_sin\" ] = np . sin ( output_df [ \"month\" ] / 2 * np . pi * 2 ) output_df [ \"2month_cos\" ] = np . cos ( output_df [ \"month\" ] / 2 * np . pi * 2 ) output_df = output_df . drop ( columns = [ \"time\" ]) # print(input_df.shape) # print(output_df.shape) # print(len(input_df.columns.unique())) # print(len(output_df.columns.unique())) return output_df","title":"3.1 Time Features"},{"location":"lgb_template_reg/#32-make-lag-feature","text":"def make_lag ( input_df ): #Select only highly correlated columns for lag features targets = list ( highly_correlated [ highly_correlated < 1.0 ] . dropna ( how = \"all\" ) . index ) targets . remove ( \"price actual\" ) #1 day #shift_time=args.lag shift_time = CONFIG . lag output_df = input_df [ targets ] . copy () for target in targets : for i in range ( shift_time ): output_df [ target + f \"_lag { i + 1 } \" ] = output_df [ target ] . shift ( i + 1 ) output_df = output_df . fillna ( 0 ) output_df = output_df . iloc [:, len ( targets ):] return output_df","title":"3.2 Make Lag Feature"},{"location":"lgb_template_reg/#33-encording","text":"def label_encoding ( input_df ): target_cols = list ( input_df . select_dtypes ( include = 'object' ) . columns ) features = input_df [ target_cols ] encoder = ce . OrdinalEncoder () . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"LE_\" ) output_df = output_df . astype ( str ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) output_df = output_df . iloc [:, - len ( target_cols ):] return output_df def count_encoding ( input_df ): target_cols = list ( input_df . select_dtypes ( include = 'object' ) . columns ) features = input_df [ target_cols ] encoder = ce . CountEncoder () . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"CE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) output_df = output_df . iloc [:, - len ( target_cols ):] return output_df def target_encoding ( input_df ): target_cols = list ( input_df . select_dtypes ( include = 'object' ) . columns ) features = input_df [ target_cols ] target = input_df [ \"price actual\" ] encoder = ce . TargetEncoder () . fit ( features . values , target ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"TE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) output_df = output_df . iloc [:, - len ( target_cols ):] return output_df","title":"3.3 Encording"},{"location":"lgb_template_reg/#34-apply-feature-enginering-to-dataset","text":"def preprocess ( train , test ): input_df = pd . concat ([ train , test ]) . reset_index ( drop = True ) funcs = [ make_time , label_encoding , count_encoding , target_encoding , make_lag ] output = [] for func in tqdm . tqdm ( funcs ): _df = func ( input_df ) output . append ( _df ) output = pd . concat ( output , axis = 1 ) output = pd . concat ([ input_df , output ], axis = 1 ) # output=standard_scaler(output) # output=select_features(output) drop_cols = list ( input_df . select_dtypes ( include = 'object' ) . columns ) output = output . drop ( columns = drop_cols ) train_x = output . iloc [: len ( train )] test_x = output . iloc [ len ( train ):] . reset_index ( drop = True ) test_x = test_x . drop ( columns = [ \"price actual\" ]) return train_x , test_x train_x , test_x = preprocess ( train_all , test_all ) train_x . head () test_x . head ()","title":"3.4 Apply Feature Enginering to Dataset"},{"location":"lgb_template_reg/#4-model-development","text":"train_x . info () <class 'pandas.core.frame.DataFrame'> RangeIndex: 26305 entries, 0 to 26304 Columns: 3162 entries, time to temp_max_Valencia_lag168 dtypes: datetime64[ns](1), float64(3120), int64(25), object(16) memory usage: 634.6+ MB #Get Categorical features cat_col = list ( train_x . select_dtypes ( include = 'object' ) . columns ) train_x [ cat_col ] = train_x [ cat_col ] . astype ( int ) test_x [ cat_col ] = test_x [ cat_col ] . astype ( int ) #Get Columns for Predict pred_col = list ( train_x . columns ) pred_col . remove ( \"price actual\" ) pred_col . remove ( \"time\" ) # ==================================================== # Model Preparation # ==================================================== class SingleLgb : def __init__ ( self , cat_col , seed = 71 , dry_run = False ): self . train_param = self . get_param ( seed ) if dry_run : self . num_rounds = 2000 else : self . num_rounds = 10000 self . cat_col = cat_col def do_train_direct ( self , x_train , x_test , y_train , y_test ): lgb_train = lgb . Dataset ( x_train , y_train ) lgb_eval = lgb . Dataset ( x_test , y_test ) print ( 'Start training...' ) #LOGGER.info('Start training...') model = lgb . train ( self . train_param , lgb_train , valid_sets = [ lgb_eval ], verbose_eval = 100 , num_boost_round = self . num_rounds , early_stopping_rounds = 100 , categorical_feature = self . cat_col ) print ( 'End training...' ) #LOGGER.info('End training...') return model @staticmethod def show_feature_importance ( model , filename = None ): fi = pd . DataFrame ({ \"name\" : model . feature_name (), \"importance_split\" : model . feature_importance ( importance_type = \"split\" ) . astype ( int ), \"importance_gain\" : model . feature_importance ( importance_type = \"gain\" ) . astype ( int ), }) fi = fi . sort_values ( by = \"importance_gain\" , ascending = False ) print ( fi ) #LOGGER.info(fi) @staticmethod def get_param ( seed = 71 ): return { 'num_leaves' : 1023 , 'min_data_in_leaf' : 50 , 'objective' : 'regression' , 'metric' : 'rmse' , 'max_depth' : CONFIG . depth , 'learning_rate' : 0.05 , \"boosting\" : \"gbdt\" , \"feature_fraction\" : 0.9 , \"verbosity\" : - 1 , \"random_state\" : seed , } class SingleTrainer : def __init__ ( self , pred_col , cat_col , dry_run = False ): self . pred_col = pred_col self . target_col = 'price actual' self . dry_run = dry_run self . cat_col = cat_col def adjust_price ( self , x ): price_list = [] for item in x : remain = item % 25 if remain >= 12.5 : item = ( item - remain ) + 25 price_list . append ( item ) else : item = item - remain price_list . append ( item ) return price_list def train_model ( self , df ): X = df [ self . pred_col ] y = df [ self . target_col ] kf = TimeSeriesSplit ( n_splits = CONFIG . fold ) models , scores = list (), list () for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): print ( \"---------\" ) print ( \"fold=\" , fold ) #LOGGER.info(\"---------\") #LOGGER.info(f\"fold={fold}\") X_train , X_val = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train , y_val = y . iloc [ train_idx ], y . iloc [ test_idx ] print ( X_train . shape , X_val . shape ) lgbm = SingleLgb ( seed = 71 , dry_run = self . dry_run , cat_col = self . cat_col ) model = lgbm . do_train_direct ( X_train , X_val , y_train , y_val ) y_pred = model . predict ( X_val ) y_pred = self . adjust_price ( y_pred ) score = np . sqrt ( mean_squared_error ( y_val , y_pred )) if fold == 0 : lgbm . show_feature_importance ( model ) models . append ( model ) scores . append ( score ) print ( f 'fold= { fold } RMSE Score=' + str ( score )) #LOGGER.info(f'fold= {fold} | MAE Score='+str(score)) #LOGGER.info(np.mean(scores)) return models , np . mean ( scores )","title":"4. Model Development"},{"location":"lgb_template_reg/#5-training","text":"trainer = SingleTrainer ( pred_col , cat_col = cat_col , dry_run = False ) models , score = trainer . train_model ( train_x ) print ( score ) preds_train = [] for m in models : preds_train . append ( m . predict ( train_x [ pred_col ])) pred_train = np . mean ( preds_train , axis = 0 ) # \u30e2\u30c7\u30eb\u305f\u3061\u306e\u5e73\u5747\u3092\u3068\u308b fig , ax = plt . subplots ( figsize = ( 8 , 8 )) sns . histplot ( train_x [ \"price actual\" ], label = 'Actual' , ax = ax , color = 'black' ) sns . histplot ( pred_train , label = 'Predict' , ax = ax , color = 'C1' ) ax . legend () ax . grid () #yy-plot def yy_plot ( y_obs , y_pred ): yvalues = np . concatenate ([ y_obs , y_pred ]) ymin , ymax , yrange = np . amin ( yvalues ), np . amax ( yvalues ), np . ptp ( yvalues ) plt . figure ( figsize = ( 8 , 8 )) plt . scatter ( y_obs , y_pred , alpha = .6 ) plt . plot ([ ymin - yrange * 0.01 , ymax + yrange * 0.01 ], [ ymin - yrange * 0.01 , ymax + yrange * 0.01 ]) #plt.xlim(10, 17) #plt.ylim(10,17) plt . xlabel ( 'y_observed' , fontsize = 24 ) plt . ylabel ( 'y_predicted' , fontsize = 24 ) plt . title ( 'Observed-Predicted Plot' , fontsize = 24 ) plt . tick_params ( labelsize = 16 ) yy_plot ( y_obs = train_x [ \"price actual\" ] . values , y_pred = pred_train )","title":"5. Training"},{"location":"lgb_template_reg/#6-inference","text":"preds = [] for m in models : preds . append ( m . predict ( test_x [ pred_col ])) pred = np . mean ( preds , axis = 0 ) # \u30e2\u30c7\u30eb\u305f\u3061\u306e\u5e73\u5747\u3092\u3068\u308b sub_df = sample_sub . copy () sub_df [ \"predicted\" ] = pred","title":"6. Inference"},{"location":"lgb_template_reg/#7-make-submit-file","text":"#\u63d0\u51fa\u30b5\u30f3\u30d7\u30eb\u30d5\u30a1\u30a4\u30eb\u306e\u78ba\u8a8d sub_df #csv\u3068\u3057\u3066\u5410\u304d\u51fa\u3059 sub_df . to_csv ( f \"submit_\" + CONFIG . exp + \".csv\" )","title":"7. Make Submit File"},{"location":"nlp/","text":"BERT (Signate) \u00b6 \u30d5\u30a9\u30eb\u30c0\u6307\u5b9a \u00b6 input_dir = \"/content/drive/MyDrive/signate/SystematicReviewWorkShop-PeerSupportGroup/data/\" output_dir = \"/content/drive/MyDrive/signate/SystematicReviewWorkShop-PeerSupportGroup/log/\" submission_dir = \"/content/drive/MyDrive/signate/SystematicReviewWorkShop-PeerSupportGroup/submission/\" model_dir = \"/content/drive/MyDrive/signate/SystematicReviewWorkShop-PeerSupportGroup/model_bin/\" pred_dir = \"/content/drive/MyDrive/signate/SystematicReviewWorkShop-PeerSupportGroup/pred/\" \u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f \u00b6 import os import math import random import pandas as pd import numpy as np from glob import glob import gc gc . enable () import torch import torch.nn as nn import torch.optim as optim import torch.optim as optim from torch.optim.optimizer import Optimizer import torch.optim.lr_scheduler as lr_scheduler from torch.utils.data import Dataset , DataLoader , SequentialSampler , RandomSampler from sklearn.model_selection import StratifiedKFold from sklearn.metrics import fbeta_score from transformers import BertConfig , RobertaConfig from transformers import ( get_cosine_schedule_with_warmup , get_cosine_with_hard_restarts_schedule_with_warmup ) from transformers import BertTokenizer , RobertaTokenizer from transformers import BertModel , RobertaModel from transformers import AutoConfig from transformers import BertForSequenceClassification , RobertaForSequenceClassification from torch import cuda import time from transformers import AdamW from transformers import AutoTokenizer from transformers import AutoModel , AutoModelForSequenceClassification from transformers import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING from IPython.display import clear_output from tqdm import tqdm , trange Config \u8a2d\u5b9a \u00b6 class CFG : exp = \"exp002\" seed = 71 fold = 5 max_len = 71 epochs = 3 train_batch_size = 16 valid_batch_size = 32 model_name = \"bert-base-uncased\" CONFIG = CFG () \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u4f5c\u6210 \u00b6 os . makedirs ( model_dir + CONFIG . exp + \"/\" , exist_ok = True ) os . makedirs ( pred_dir + CONFIG . exp + \"/\" , exist_ok = True ) os . makedirs ( output_dir + CONFIG . exp + \"/\" , exist_ok = True ) \u30b7\u30fc\u30c9\u306e\u56fa\u5b9a \u00b6 def set_random_seed ( random_seed ): random . seed ( random_seed ) np . random . seed ( random_seed ) os . environ [ \"PYTHONHASHSEED\" ] = str ( random_seed ) torch . manual_seed ( random_seed ) torch . cuda . manual_seed ( random_seed ) torch . cuda . manual_seed_all ( random_seed ) torch . backends . cudnn . deterministic = True set_random_seed ( CONFIG . seed ) \u30ed\u30b0\u306e\u53d6\u5f97 \u00b6 def init_logger ( log_file = output_dir + CONFIG . exp + f \"/ { CONFIG . exp } _train.log\" ): from logging import INFO , FileHandler , Formatter , StreamHandler , getLogger logger = getLogger ( __name__ ) logger . setLevel ( INFO ) handler1 = StreamHandler () handler1 . setFormatter ( Formatter ( \" %(message)s \" )) handler2 = FileHandler ( filename = log_file ) handler2 . setFormatter ( Formatter ( \" %(message)s \" )) logger . addHandler ( handler1 ) logger . addHandler ( handler2 ) return logger LOGGER = init_logger () CPU/GPU\u306e\u6307\u5b9a \u00b6 device = 'cuda' if cuda . is_available () else 'cpu' print ( f ' { device } is used' ) \u30c7\u30fc\u30bf\u5206\u5272 \u00b6 def get_train_data ( train ): # \u4ea4\u5dee\u691c\u8a3c \u7528\u306e\u756a\u53f7\u3092\u632f\u308a\u307e\u3059\u3002 Fold = StratifiedKFold ( n_splits = CONFIG . fold , shuffle = True , random_state = CONFIG . seed ) for n , ( train_index , val_index ) in enumerate ( Fold . split ( train , train [ \"judgement\" ])): train . loc [ val_index , \"fold\" ] = int ( n ) train [ \"fold\" ] = train [ \"fold\" ] . astype ( np . uint8 ) return train def get_test_data ( test ): return test \u524d\u51e6\u7406 \u00b6 import re import unicodedata def clean_text ( text ): replaced_text = text . lower () replaced_text = re . sub ( r '[\u3010\u3011]' , ' ' , replaced_text ) # \u3010\u3011\u306e\u9664\u53bb replaced_text = re . sub ( r '[\uff08\uff09()]' , ' ' , replaced_text ) # \uff08\uff09\u306e\u9664\u53bb replaced_text = re . sub ( r '[\uff3b\uff3d\\[\\]]' , ' ' , replaced_text ) # \uff3b\uff3d\u306e\u9664\u53bb replaced_text = re . sub ( r '[@\uff20]\\w+' , '' , replaced_text ) # \u30e1\u30f3\u30b7\u30e7\u30f3\u306e\u9664\u53bb replaced_text = re . sub ( r 'https?:\\/\\/.*?[\\r\\n ]' , '' , replaced_text ) # URL\u306e\u9664\u53bb replaced_text = re . sub ( r '\u3000' , ' ' , replaced_text ) # \u5168\u89d2\u7a7a\u767d\u306e\u9664\u53bb return replaced_text def normalize ( text ): normalized_text = normalize_unicode ( text ) normalized_text = normalize_number ( normalized_text ) normalized_text = lower_text ( normalized_text ) return normalized_text def lower_text ( text ): return text . lower () def normalize_unicode ( text , form = 'NFKC' ): normalized_text = unicodedata . normalize ( form , text ) return normalized_text def normalize_number ( text ): replaced_text = re . sub ( r '\\d+' , '0' , text ) return replaced_text def text_cleaning ( text ): text = clean_text ( text ) text = normalize ( text ) text = lower_text ( text ) text = normalize_unicode ( text ) return text def data_cleaning ( data ): return [ text_cleaning ( text ) for text in data ] \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f \u00b6 pd . set_option ( \"display.max_colwidth\" , 50 ) train = pd . read_csv ( input_dir + \"train.csv\" ) test = pd . read_csv ( input_dir + \"test.csv\" ) sub = pd . read_csv ( input_dir + \"sample_submit.csv\" , header = None ) sub . columns = [ \"id\" , \"judgement\" ] # \u6587\u5b57\u5316\u3051\u30c7\u30fc\u30bf\u3092\u9664\u5916\u3059\u308b\u306a\u3069\u306e\u51e6\u7406\u306f\u5165\u308c\u305f\u65b9\u304c\u3044\u3044?\u03b1\u3068\u304b\u304c\u6587\u5b57\u5316\u3051\u306e\u539f\u56e0 train [ \"title\" ] = data_cleaning ( train [ \"title\" ]) train . head () train = get_train_data ( train ) train . head () \u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc \u00b6 class SRWSDataset ( Dataset ): def __init__ ( self , df , model_name , include_labels = True ): tokenizer = BertTokenizer . from_pretrained ( model_name ) self . df = df self . include_labels = include_labels self . title = df [ \"title\" ] . tolist () self . encoded = tokenizer . batch_encode_plus ( self . title , padding = \"max_length\" , max_length = CONFIG . max_len , truncation = True , return_attention_mask = True ) if self . include_labels : self . labels = df [ \"judgement\" ] . values def __len__ ( self ): return len ( self . df ) def __getitem__ ( self , idx ): input_ids = torch . tensor ( self . encoded [ \"input_ids\" ][ idx ]) attention_mask = torch . tensor ( self . encoded [ \"attention_mask\" ][ idx ]) if self . include_labels : label = torch . tensor ( self . labels [ idx ]) . float () return input_ids , attention_mask , label return input_ids , attention_mask BERT\u30e2\u30c7\u30eb \u00b6 class SRWSModel ( nn . Module ): def __init__ ( self , model_name ): super () . __init__ () self . model = BertForSequenceClassification . from_pretrained ( model_name , num_labels = 1 ) self . sigmoid = nn . Sigmoid () def forward ( self , input_ids , attention_mask ): out = self . model ( input_ids = input_ids , attention_mask = attention_mask ) out = self . sigmoid ( out . logits ) . squeeze () return out \u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u30fc \u00b6 class AverageMeter ( object ): \"\"\"Computes and stores the average and current value\"\"\" def __init__ ( self ): self . reset () def reset ( self ): self . val = 0 self . avg = 0 self . sum = 0 self . count = 0 def update ( self , val , n = 1 ): self . val = val self . sum += val * n self . count += n self . avg = self . sum / self . count def asMinutes ( s ): m = math . floor ( s / 60 ) s -= m * 60 return \" %d m %d s\" % ( m , s ) def timeSince ( since , percent ): now = time . time () s = now - since es = s / ( percent ) rs = es - s return \" %s (remain %s )\" % ( asMinutes ( s ), asMinutes ( rs )) \u8a13\u7df4/\u63a8\u8ad6 \u00b6 def train_fn ( train_loader , model , criterion , optimizer , epoch , device ): start = end = time . time () losses = AverageMeter () # switch to train mode model . train () for step , ( input_ids , attention_mask , labels ) in enumerate ( train_loader ): optimizer . zero_grad () input_ids = input_ids . to ( device ) attention_mask = attention_mask . to ( device ) labels = labels . to ( device ) batch_size = labels . size ( 0 ) y_preds = model ( input_ids , attention_mask ) loss = criterion ( y_preds , labels ) # record loss losses . update ( loss . item (), batch_size ) loss . backward () optimizer . step () if step % 100 == 0 or step == ( len ( train_loader ) - 1 ): print ( f \"Epoch: [ { epoch + 1 } ][ { step } / { len ( train_loader ) } ] \" f \"Elapsed { timeSince ( start , float ( step + 1 ) / len ( train_loader )) : s } \" f \"Loss: { losses . avg : .4f } \" ) return losses . avg def valid_fn ( valid_loader , model , criterion , device ): start = end = time . time () losses = AverageMeter () # switch to evaluation mode model . eval () preds = [] for step , ( input_ids , attention_mask , labels ) in enumerate ( valid_loader ): input_ids = input_ids . to ( device ) attention_mask = attention_mask . to ( device ) labels = labels . to ( device ) batch_size = labels . size ( 0 ) # compute loss with torch . no_grad (): y_preds = model ( input_ids , attention_mask ) loss = criterion ( y_preds , labels ) losses . update ( loss . item (), batch_size ) # record score preds . append ( y_preds . to ( \"cpu\" ) . numpy ()) if step % 100 == 0 or step == ( len ( valid_loader ) - 1 ): print ( f \"EVAL: [ { step } / { len ( valid_loader ) } ] \" f \"Elapsed { timeSince ( start , float ( step + 1 ) / len ( valid_loader )) : s } \" f \"Loss: { losses . avg : .4f } \" ) predictions = np . concatenate ( preds ) return losses . avg , predictions def inference (): predictions = [] test_dataset = SRWSDataset ( test , CONFIG . model_name , include_labels = False ) test_loader = DataLoader ( test_dataset , batch_size = 16 , shuffle = False , num_workers = 4 , pin_memory = True ) for fold in range ( CONFIG . fold ): LOGGER . info ( f \"========== model: { CONFIG . model_name } fold: { fold } inference ==========\" ) model = SRWSModel ( CONFIG . model_name ) model . to ( device ) model . load_state_dict ( torch . load ( model_dir + CONFIG . exp + \"/\" + f \" { CONFIG . model_name } _fold { fold } _best.pth\" )[ \"model\" ]) model . eval () preds = [] for i , ( input_ids , attention_mask ) in tqdm ( enumerate ( test_loader ), total = len ( test_loader )): input_ids = input_ids . to ( device ) attention_mask = attention_mask . to ( device ) with torch . no_grad (): y_preds = model ( input_ids , attention_mask ) preds . append ( y_preds . to ( \"cpu\" ) . numpy ()) preds = np . concatenate ( preds ) predictions . append ( preds ) predictions = np . mean ( predictions , axis = 0 ) return predictions \u8a13\u7df4\u5b9f\u884c \u00b6 def train_loop ( train , fold ): LOGGER . info ( f \"========== fold: { fold } training ==========\" ) # ==================================================== # Data Loader # ==================================================== trn_idx = train [ train [ \"fold\" ] != fold ] . index val_idx = train [ train [ \"fold\" ] == fold ] . index train_folds = train . loc [ trn_idx ] . reset_index ( drop = True ) valid_folds = train . loc [ val_idx ] . reset_index ( drop = True ) train_dataset = SRWSDataset ( train_folds , CONFIG . model_name ) valid_dataset = SRWSDataset ( valid_folds , CONFIG . model_name ) train_loader = DataLoader ( train_dataset , batch_size = CONFIG . train_batch_size , shuffle = True , num_workers = 4 , pin_memory = True , drop_last = True , ) valid_loader = DataLoader ( valid_dataset , batch_size = CONFIG . valid_batch_size , shuffle = False , num_workers = 4 , pin_memory = True , drop_last = False , ) # ==================================================== # Model # ==================================================== model = SRWSModel ( CONFIG . model_name ) model . to ( device ) optimizer = AdamW ( model . parameters (), lr = 2e-5 ) criterion = nn . BCELoss () # ==================================================== # Loop # ==================================================== best_score = - 1 best_loss = np . inf for epoch in range ( CONFIG . epochs ): start_time = time . time () # train avg_loss = train_fn ( train_loader , model , criterion , optimizer , epoch , device ) # eval avg_val_loss , preds = valid_fn ( valid_loader , model , criterion , device ) valid_labels = valid_folds [ \"judgement\" ] . values # scoring score = fbeta_score ( valid_labels , np . where ( preds < border , 0 , 1 ), beta = 7.0 ) elapsed = time . time () - start_time LOGGER . info ( f \"Epoch { epoch + 1 } - avg_train_loss: { avg_loss : .4f } avg_val_loss: { avg_val_loss : .4f } time: { elapsed : .0f } s\" ) LOGGER . info ( f \"Epoch { epoch + 1 } - Score: { score } \" ) if score > best_score : best_score = score LOGGER . info ( f \"Epoch { epoch + 1 } - Save Best Score: { best_score : .4f } Model\" ) torch . save ( { \"model\" : model . state_dict (), \"preds\" : preds }, model_dir + CONFIG . exp + \"/\" + f \" { CONFIG . model_name } _fold { fold } _best.pth\" ) check_point = torch . load ( model_dir + CONFIG . exp + \"/\" + f \" { CONFIG . model_name } _fold { fold } _best.pth\" ) valid_folds [ \"preds\" ] = check_point [ \"preds\" ] return valid_folds def get_result ( result_df ): preds = result_df [ \"preds\" ] . values labels = result_df [ \"judgement\" ] . values score = fbeta_score ( labels , np . where ( preds < border , 0 , 1 ), beta = 7.0 ) LOGGER . info ( f \"Score: { score : <.5f } \" ) \u5b66\u7fd2\u306e\u5b9f\u884c \u00b6 # Training border = len ( train [ train [ \"judgement\" ] == 1 ]) / len ( train [ \"judgement\" ]) oof_df = pd . DataFrame () for fold in range ( CONFIG . fold ): _oof_df = train_loop ( train , fold ) oof_df = pd . concat ([ oof_df , _oof_df ]) LOGGER . info ( f \"========== fold: { fold } result ==========\" ) get_result ( _oof_df ) # CV result LOGGER . info ( f \"========== CV ==========\" ) get_result ( oof_df ) # Save OOF result oof_df . to_csv ( pred_dir + CONFIG . exp + \"/oof_df.csv\" , index = False ) # Inference predictions = inference () predictions = np . where ( predictions < border , 0 , 1 ) # submission sub [ \"judgement\" ] = predictions sub . to_csv ( submission_dir + CONFIG . exp + \"_submission.csv\" , index = False , header = False ) Light GBM (Kiva) \u00b6 \u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f \u00b6 import logging import datetime import warnings import joblib import pandas as pd pd . set_option ( \"display.max_colwidth\" , 50 ) import numpy as np import matplotlib.pyplot as plt from currency_converter import CurrencyConverter import seaborn as sns import os import re from tqdm import tqdm from sklearn.metrics import fbeta_score from sklearn.utils import class_weight from sklearn.model_selection import StratifiedKFold from sklearn.metrics import confusion_matrix from sklearn.decomposition import TruncatedSVD , PCA from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer from gensim.models.doc2vec import Doc2Vec , TaggedDocument from scipy.optimize import minimize , minimize_scalar from contextlib import contextmanager from sklearn import model_selection , metrics import category_encoders as ce import torch import lightgbm as lgb from time import time import texthero as hero import transformers import pycld2 as cld2 #\u30e2\u30b8\u30e5\u30fc\u30eb\u8aad\u307f\u8fbc\u307f import sys , os sys . path . append ( '/home/azureuser/cloudfiles/code/Users/hirahara.kazuki/module' ) import visualize_module import preprocess_module import importlib importlib . reload ( visualize_module ) importlib . reload ( preprocess_module ) vs = visualize_module . Feature_Confirmation () sc = preprocess_module . ScalerModule () en = preprocess_module . Encoding_Module () CONFIG \u00b6 # ==================================================== # CFG # ==================================================== class CONFIG : exp = 'baseline' bert_model_name = 'bert-base-uncased' fold = 5 seed = 71 \u30d5\u30a9\u30eb\u30c0\u30fc\u306e\u6307\u5b9a\u3068\u8aad\u307f\u8fbc\u307f \u00b6 # ==================================================== # Folders # ==================================================== input_dir = \"../data/\" interm_dir = \"../interm/\" submission_dir = \"../submission/\" model_dir = \"../model/\" pred_dir = \"../pred/\" os . makedirs ( model_dir + CONFIG . exp + \"/\" , exist_ok = True ) os . makedirs ( pred_dir + CONFIG . exp + \"/\" , exist_ok = True ) os . makedirs ( interm_dir + CONFIG . exp + \"/\" , exist_ok = True ) os . makedirs ( submission_dir + CONFIG . exp + \"/\" , exist_ok = True ) \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f \u00b6 # ==================================================== # Read Data # ==================================================== train = pd . read_csv ( input_dir + \"train.csv\" ) test = pd . read_csv ( input_dir + \"test.csv\" ) sub = pd . read_csv ( input_dir + \"sample_submission.csv\" ) #Train_Dataset display ( train . head ()) display ( train . shape ) #Test_Dataset display ( test . head ()) display ( test . shape ) \u4e0d\u8981\u306a\u30ab\u30e9\u30e0\u306e\u524a\u9664 \u00b6 # ==================================================== # Delete Unrequired Columns # ==================================================== # \u4e0d\u8981\u306a\u30ab\u30e9\u30e0\u306e\u524a\u9664 drop_columns = [ #'LOAN_ID', 'ORIGINAL_LANGUAGE' , 'DESCRIPTION' , #\u3068\u308a\u3042\u3048\u305a\u7ffb\u8a33\u3092\u4fe1\u983c\u3059\u308b 'IMAGE_ID' , 'COUNTRY_CODE' ] train = train . drop ( drop_columns , axis = 1 ) test = test . drop ( drop_columns , axis = 1 ) \u524d\u51e6\u7406 \u00b6 # ==================================================== # Basic Preprocess of Words # ==================================================== #Text hero\u306b\u3088\u308b\u524d\u51e6\u7406 def cleansing_hero_only_text ( input_df , text_col ): ## get text only custom_pipeline = [ hero . preprocessing . fillna , hero . preprocessing . lowercase , #\u5c0f\u6587\u5b57\u5909\u63db #hero.preprocessing.remove_digits, #hero.preprocessing.remove_punctuation,# \u53e5\u8aad\u70b9\u306e\u524a\u9664 hero . preprocessing . remove_diacritics , # \u30c0\u30a4\u30a2\u30af\u30ea\u30c6\u30a3\u30ab\u30eb\u30de\u30fc\u30af\uff08\u767a\u97f3\u533a\u5225\u7b26\u53f7\u3002\u00e0\u3084\u00e9\u306a\u3069\uff09\u306e\u524a\u9664 hero . preprocessing . remove_stopwords , # \u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u306e\u524a\u9664 hero . preprocessing . remove_whitespace , # \u30b9\u30da\u30fc\u30b9\u306e\u524a\u9664 #hero.preprocessing.stem ] texts = hero . clean ( input_df [ text_col ], custom_pipeline ) return texts #Punct\u524a\u9664 def clean_puncts ( x ): puncts = [ ',' , '.' , '\"' , ':' , ')' , '(' , '-' , '!' , '?' , '|' , ';' , \"'\" , '$' , '&' , '/' , '[' , ']' , '>' , '%' , '=' , '#' , '*' , '+' , ' \\\\ ' , '\u2022' , '~' , '@' , '\u00a3' , '\u00b7' , '_' , '{' , '}' , '\u00a9' , '^' , '\u00ae' , '`' , '<' , '\u2192' , '\u00b0' , '\u20ac' , '\u2122' , '\u203a' , '\u2665' , '\u2190' , '\u00d7' , '\u00a7' , '\u2033' , '\u2032' , '\u00c2' , '\u2588' , '\u00bd' , '\u00e0' , '\u2026' , '\u201c' , '\u2605' , '\u201d' , '\u2013' , '\u25cf' , '\u00e2' , '\u25ba' , '\u2212' , '\u00a2' , '\u00b2' , '\u00ac' , '\u2591' , '\u00b6' , '\u2191' , '\u00b1' , '\u00bf' , '\u25be' , '\u2550' , '\u00a6' , '\u2551' , '\u2015' , '\u00a5' , '\u2593' , '\u2014' , '\u2039' , '\u2500' , '\u2592' , '\uff1a' , '\u00bc' , '\u2295' , '\u25bc' , '\u25aa' , '\u2020' , '\u25a0' , '\u2019' , '\u2580' , '\u00a8' , '\u2584' , '\u266b' , '\u2606' , '\u00e9' , '\u00af' , '\u2666' , '\u00a4' , '\u25b2' , '\u00e8' , '\u00b8' , '\u00be' , '\u00c3' , '\u22c5' , '\u2018' , '\u221e' , '\u00ab' , '\u2219' , '\uff09' , '\u2193' , '\u3001' , '\u2502' , '\uff08' , '\u00bb' , '\uff0c' , '\u266a' , '\u2569' , '\u255a' , '\u00b3' , '\u30fb' , '\u2566' , '\u2563' , '\u2554' , '\u2557' , '\u25ac' , '\u2764' , '\u00ef' , '\u00d8' , '\u00b9' , '\u2264' , '\u2021' , '\u221a' , '\uff08' , '\uff09' , '\uff5e' , '\u27a1' , '\uff05' , '\u21d2' , '\u25b6' , '\u300c' , '\u2784' , '\u2786' , '\u278a' , '\u278b' , '\u278c' , '\u278d' , '\u24ea' , '\u2460' , '\u2461' , '\u2462' , '\u2463' , '\u2464' , '\u2470' , '\u2776' , '\u2777' , '\u2778' , '\u2779' , '\u277a' , '\u277b' , '\u277c' , '\u277d' , '\uff1d' , '\u203b' , '\u3231' , '\uff64' , '\u25b3' , '\u212e' , '\u217c' , '\u2010' , '\uff63' , '\u251d' , '\u21b3' , '\u25c9' , '\uff0f' , '\uff0b' , '\u25cb' , '\u3010' , '\u3011' , '\u2705' , '\u2611' , '\u27a4' , '\uff9e' , '\u21b3' , '\u3036' , '\u261b' , '\uff62' , '\u207a' , '\u300e' , '\u226b' , '\u00c2\u00a9' , '<sub>' , 'A\uff8e\uff72' , '\uff8e\uff72' , \"\uff83\uff69\" ] for punct in puncts : x = x . replace ( punct , '' ) return x #\u6539\u884c\u30b3\u30fc\u30c9\u306e\u524a\u9664 def remove_br ( text ): text = text . replace ( \"br\" , \"\" ) #\u534a\u89d2\u30b9\u30da\u30fc\u30b9\u306e\u524a\u9664 text = text . replace ( \" \" , \" \" ) text = text . replace ( \" \" , \" \" ) return text #\u91d1\u984d\u306e\u53d6\u5f97 def get_price ( x ): # #usd\u306b\u5909\u63db\u3067\u304d\u308b\u8ca8\u5e63\u306f\u5909\u63db\u3059\u308b # def change_usd(x): # rate_unit=x[-3:].upper() # price=re.sub(\"[a-z\\s]\",\"\",x) # c = CurrencyConverter() # try: # x=c.convert(price, rate_unit, 'USD') # except: # x=price # return x re_text = ( \"\\d+\\sphp|\\d+\\scop|\\d+\\shnl|\\d+\\skes|\\d+\\susd|\\d+\\srwf|\" \"\\d+\\spyg|\\d+\\spen|\\d+\\svnd|\\d+\\smga|\\d+\\sinr|\\d+\\smzn|\" \"\\d+\\spkr|\\d+\\sjod|\\d+\\sxof|\\d+\\slrd|\\d+\\sugx|\\d+\\sgtq|\" \"\\d+\\smxn|\\d+\\skhr|\\d+\\stjs|\\d+\\shtg|\\d+\\segp|\\d+\\sidr|\" \"\\d+\\snio|\\d+\\sghs|\\d+\\sfjd|\\d+\\sbob|\\d+\\ssbd|\\d+\\swst|\" \"\\d+\\sbrl|\\d+\\sngn|\\d+\\sxaf|\\d+\\seur|\\d+\\skgs|\\d+\\smwk|\" \"\\d+\\sgel|\\d+\\sall|\\d+\\szmw|\\d+\\scrc|\\d+\\stop|\\d+\\smdl|\" \"\\d+\\slsl|\\d+\\sdop|\\d+\\ssll|\\d+\\stry|\\d+\\snpr|\\d+\\sthb|\" \"\\d+\\spgk|\\d+\\sils|\\d+\\samd\" ) results = re . findall ( re_text , x ) if len ( results ) == 0 : return 0 else : price_list = [] for result in results : price_v = re . sub ( \"[a-z\\s]\" , \"\" , result ) price_list . append ( price_v ) price_list = [ s . strip ( ' ' ) for s in price_list ] #\u4e00\u6642\u7684\u306b\u6700\u5927\u5024\u3092\u53d6\u5f97 price_max = max ( price_list ) #\u4e00\u756a\u306f\u3058\u3081\u306e\u5024\u3092\u7533\u8acb\u8005\u306e\u5e74\u9f62\u3068\u4eee\u5b9a return price_max #\u6b63\u898f\u5316\u8868\u73fe\u306b\u3088\u308b\u306b\u5e74\u9f62\u53d6\u5f97 def get_age ( x ): result_1 = re . findall ( '\\d {2} \\syears' , x ) result_2 = re . findall ( '\\d {2} year' , x ) result = result_1 + result_2 if len ( result ) == 0 : return 0 else : result = [ s . strip ( 'years' ) for s in result ] result = [ s . strip ( 'year' ) for s in result ] result = [ s . strip ( ' ' ) for s in result ] result = [ s . strip ( ' ' ) for s in result ] #\u4e00\u756a\u306f\u3058\u3081\u306e\u5024\u3092\u7533\u8acb\u8005\u306e\u5e74\u9f62\u3068\u4eee\u5b9a result = result [ 0 ] return result def basic_preprocess ( input_df ): output_df = input_df . copy () output_df [ \"DESCRIPTION_TRANSLATED\" ] = cleansing_hero_only_text ( output_df , \"DESCRIPTION_TRANSLATED\" ) output_df [ \"DESCRIPTION_TRANSLATED\" ] = output_df [ \"DESCRIPTION_TRANSLATED\" ] . apply ( lambda x : clean_puncts ( x )) output_df [ \"DESCRIPTION_TRANSLATED\" ] = output_df [ \"DESCRIPTION_TRANSLATED\" ] . apply ( lambda x : remove_br ( x )) output_df [ \"age\" ] = output_df [ \"DESCRIPTION_TRANSLATED\" ] . apply ( lambda x : get_age ( x )) output_df [ \"price\" ] = output_df [ \"DESCRIPTION_TRANSLATED\" ] . apply ( lambda x : get_price ( x )) output_df [ \"age\" ] = output_df [ \"age\" ] . astype ( \"int\" ) output_df [ \"price\" ] = output_df [ \"price\" ] . astype ( \"int\" ) return output_df \u7279\u5fb4\u91cf\u306e\u62bd\u51fa \u00b6 # ==================================================== # Feature Engineering of WORDS # ==================================================== #\u5404\u7279\u5fb4\u91cf\u306e\u53d6\u5f97 def basic_text_features_transforme ( input_df , text_columns , name = \"\" ): \"\"\"basic \u306a text \u7279\u5fb4\u91cf\"\"\" def _get_features ( dataframe , column ): _df = pd . DataFrame () _df [ column + name + '_num_chars' ] = dataframe [ column ] . apply ( len ) _df [ column + name + '_num_exclamation_marks' ] = dataframe [ column ] . apply ( lambda x : x . count ( '!' )) _df [ column + name + '_num_question_marks' ] = dataframe [ column ] . apply ( lambda x : x . count ( '?' )) _df [ column + name + '_num_punctuation' ] = dataframe [ column ] . apply ( lambda x : sum ( x . count ( w ) for w in '.,;:' )) _df [ column + name + '_num_symbols' ] = dataframe [ column ] . apply ( lambda x : sum ( x . count ( w ) for w in '*&$%' )) _df [ column + name + '_num_words' ] = dataframe [ column ] . apply ( lambda x : len ( x . split ())) _df [ column + name + '_num_unique_words' ] = dataframe [ column ] . apply ( lambda x : len ( set ( w for w in x . split ()))) _df [ column + name + '_words_vs_unique' ] = _df [ column + name + '_num_unique_words' ] / _df [ column + name + '_num_words' ] _df [ column + name + '_words_vs_chars' ] = _df [ column + name + '_num_words' ] / _df [ column + name + '_num_chars' ] return _df def vectorize_text ( input_df , text_columns , vectorizer = CountVectorizer (), transformer = TruncatedSVD ( n_components = 128 ), name = 'html_count_svd' ): output_df = pd . DataFrame () output_df [ text_columns ] = input_df [ text_columns ] . fillna ( 'missing' ) . astype ( str ) features = [] for c in text_columns : sentence = vectorizer . fit_transform ( output_df [ c ]) feature = transformer . fit_transform ( sentence ) num_p = feature . shape [ 1 ] feature = pd . DataFrame ( feature , columns = [ name + str ( c ) + str ( num_p ) + f '= { i : 03 } ' for i in range ( num_p )]) features . append ( feature ) output_df = pd . concat ( features , axis = 1 ) return output_df class BERT_Vectorizer : \"\"\" \u4e8b\u524d\u5b66\u7fd2\u6e08\u307f BERT \u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u30c6\u30ad\u30b9\u30c8\u7279\u5fb4\u62bd\u51fa https://www.guruguru.science/competitions/16/discussions/fb792c87-6bad-445d-aa34-b4118fc378c1/ \"\"\" def __init__ ( self , model_name = 'bert-base-uncased' , max_len = 128 ): self . device = 'cuda' if torch . cuda . is_available () else 'cpu' self . model_name = model_name self . tokenizer = transformers . AutoTokenizer . from_pretrained ( self . model_name ) self . model = transformers . AutoModel . from_pretrained ( self . model_name ) self . model = self . model . to ( self . device ) self . max_len = max_len def vectorize ( self , sentence : str ) -> np . array : inp = self . tokenizer . encode ( sentence ) len_inp = len ( inp ) if len_inp >= self . max_len : inputs = inp [: self . max_len ] masks = [ 1 ] * self . max_len else : inputs = inp + [ 0 ] * ( self . max_len - len_inp ) masks = [ 1 ] * len_inp + [ 0 ] * ( self . max_len - len_inp ) inputs_tensor = torch . tensor ([ inputs ], dtype = torch . long ) . to ( self . device ) masks_tensor = torch . tensor ([ masks ], dtype = torch . long ) . to ( self . device ) output = self . model ( inputs_tensor , masks_tensor ) seq_out , pooled_out = output [ 'last_hidden_state' ], output [ 'pooler_output' ] if torch . cuda . is_available (): return seq_out [ 0 ][ 0 ] . cpu () . detach () . numpy () # 0\u756a\u76ee\u306f [CLS] token, 768 dim \u306e\u6587\u7ae0\u7279\u5fb4\u91cf else : return seq_out [ 0 ][ 0 ] . detach () . numpy () def get_basic_text_features ( input_df ): output_df = basic_text_features_transforme ( input_df , text_columns = [ \"DESCRIPTION_TRANSLATED\" , \"LOAN_USE\" ]) return output_df def get_tfidf_features__svd64 ( input_df ): output_df = vectorize_text ( input_df , text_columns = [ \"DESCRIPTION_TRANSLATED\" , \"LOAN_USE\" ], vectorizer = TfidfVectorizer ( min_df = 0.001 , max_df = 0.99 ), transformer = TruncatedSVD ( n_components = 64 ), name = \"tfidf_svd_\" ) return output_df def get_count_features__svd64 ( input_df ): output_df = vectorize_text ( input_df , text_columns = [ \"DESCRIPTION_TRANSLATED\" , \"LOAN_USE\" ], vectorizer = CountVectorizer ( min_df = 0.001 , max_df = 0.99 ), transformer = TruncatedSVD ( n_components = 64 ), name = \"count_svd_\" ) return output_df def get_bert_feature_description ( input_df ): vectorizer = BERT_Vectorizer ( model_name = CONFIG . bert_model_name ) texts = input_df [ 'DESCRIPTION_TRANSLATED' ] . fillna ( '' ) text_vecs = np . array ([ vectorizer . vectorize ( x ) for x in texts ]) pca = PCA ( n_components = 32 ) text_vecs = pca . fit_transform ( text_vecs ) output_df = pd . DataFrame ( text_vecs , columns = [ f 'bert_description_vecs= { i : 03 } ' for i in range ( text_vecs . shape [ 1 ])]) output_df . index = input_df . index return output_df def get_bert_feature_loan ( input_df ): vectorizer = BERT_Vectorizer ( model_name = CONFIG . bert_model_name ) texts = input_df [ 'LOAN_USE' ] . fillna ( '' ) text_vecs = np . array ([ vectorizer . vectorize ( x ) for x in texts ]) pca = PCA ( n_components = 32 ) text_vecs = pca . fit_transform ( text_vecs ) output_df = pd . DataFrame ( text_vecs , columns = [ f 'bert_loan_vecs= { i : 03 } ' for i in range ( text_vecs . shape [ 1 ])]) output_df . index = input_df . index return output_df # ==================================================== # Feature Engineering of Category # ==================================================== def target_encoding ( input_df ): target_cols = [ 'ACTIVITY_NAME' , 'SECTOR_NAME' , 'COUNTRY_NAME' , 'TOWN_NAME' , 'CURRENCY' ] target = input_df [ \"LOAN_AMOUNT\" ] features = input_df [ target_cols ] encoder = ce . TargetEncoder () . fit ( features . values , target ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"TE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) output_df = output_df . iloc [:, - len ( target_cols ):] return output_df def label_encoding ( input_df ): target_cols = [ 'ACTIVITY_NAME' , 'SECTOR_NAME' , 'COUNTRY_NAME' , 'TOWN_NAME' , 'CURRENCY_POLICY' , 'CURRENCY' , 'REPAYMENT_INTERVAL' , 'DISTRIBUTION_MODEL' ] features = input_df [ target_cols ] encoder = ce . OrdinalEncoder () . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"LE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) output_df = output_df . iloc [:, - len ( target_cols ):] return output_df \u524d\u51e6\u7406\u306e\u5b9f\u884c \u00b6 def preprocess ( train , test ): \"\"\"\u524d\u51e6\u7406\u306e\u5b9f\u884c\u95a2\u6570\"\"\" input_df = pd . concat ([ train , test ]) . reset_index ( drop = True ) funcs = [ basic_preprocess , get_basic_text_features , get_tfidf_features__svd64 , get_count_features__svd64 , #get_bert_feature_description, #get_bert_feature_loan, target_encoding , label_encoding ] output = [] for func in funcs : _df = func ( input_df ) output . append ( _df ) output = pd . concat ( output , axis = 1 ) train_x = output . iloc [: len ( train )] test_x = output . iloc [ len ( train ):] . reset_index ( drop = True ) return train_x , test_x train_x , test_x = preprocess ( train , test ) \u5b66\u7fd2 \u00b6 # ==================================================== # Model Preparation # ==================================================== class SingleLgb : def __init__ ( self , cat_col , seed = CONFIG . seed , dry_run = False ): self . train_param = self . get_param ( seed ) if dry_run : self . num_rounds = 10000 else : self . num_rounds = 10000 self . cat_col = cat_col def do_train_direct ( self , x_train , x_test , y_train , y_test ): lgb_train = lgb . Dataset ( x_train , y_train ) lgb_eval = lgb . Dataset ( x_test , y_test ) print ( 'Start training...' ) model = lgb . train ( self . train_param , lgb_train , valid_sets = [ lgb_eval ], verbose_eval = 100 , num_boost_round = self . num_rounds , early_stopping_rounds = 100 , categorical_feature = self . cat_col ) print ( 'End training...' ) return model @staticmethod def show_feature_importance ( model , filename = None ): fi = pd . DataFrame ({ \"name\" : model . feature_name (), \"importance_split\" : model . feature_importance ( importance_type = \"split\" ) . astype ( int ), \"importance_gain\" : model . feature_importance ( importance_type = \"gain\" ) . astype ( int ), }) fi = fi . sort_values ( by = \"importance_gain\" , ascending = False ) print ( fi ) @staticmethod def get_param ( seed = CONFIG . seed ): return { 'num_leaves' : 1023 , 'min_data_in_leaf' : 50 , 'objective' : 'regression' , 'metric' : 'mae' , 'max_depth' : - 1 , 'learning_rate' : 0.05 , \"boosting\" : \"gbdt\" , \"feature_fraction\" : 0.9 , \"verbosity\" : - 1 , \"random_state\" : seed , } class SingleTrainer : def __init__ ( self , pred_col , cat_col , dry_run = False ): self . pred_col = pred_col self . target_col = 'LOAN_AMOUNT' self . dry_run = dry_run self . val_size = 1000 * 1000 self . cat_col = cat_col def train_model ( self , df ): X = df [ self . pred_col ] y = df [ self . target_col ] kf = model_selection . KFold ( n_splits = CONFIG . fold ) models , scores = list (), list () for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): print ( \"---------\" ) print ( \"fold=\" , fold ) X_train , X_val = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train , y_val = y . iloc [ train_idx ], y . iloc [ test_idx ] print ( X_train . shape , X_val . shape ) lgbm = SingleLgb ( seed = CONFIG . seed , dry_run = self . dry_run , cat_col = self . cat_col ) model = lgbm . do_train_direct ( X_train , X_val , y_train , y_val ) score = model . best_score [ \"valid_0\" ][ \"mae\" ] if fold == 0 : lgbm . show_feature_importance ( model ) models . append ( model ) scores . append ( score ) print ( f 'fold= { fold } MAE Score' ) self . eval_model ( model , X_val , y_val ) return models , np . mean ( scores ) def eval_model ( self , model , X_val , y_val ): temp_pred = model . predict ( X_val ) #temp_pred = np.argmax(temp_pred, axis=1) mae_score = temp_pred . mean () print ( pd . DataFrame ( temp_pred ) . value_counts ()) print ( \"Score=\" , np . exp ( mae_score )) # ==================================================== # Make Model Input # ==================================================== #drop unrequired_columns train_input = train_x . drop ( columns = [ \"TAGS\" , \"LOAN_ID\" , 'DESCRIPTION_TRANSLATED' , 'LOAN_USE' , 'ACTIVITY_NAME' , 'SECTOR_NAME' , 'COUNTRY_NAME' , 'TOWN_NAME' , 'CURRENCY_POLICY' , 'CURRENCY' , 'REPAYMENT_INTERVAL' , 'DISTRIBUTION_MODEL' ]) display ( train_input . shape ) test_input = test_x . drop ( columns = [ \"TAGS\" , \"LOAN_ID\" , 'DESCRIPTION_TRANSLATED' , \"LOAN_AMOUNT\" , 'LOAN_USE' , 'ACTIVITY_NAME' , 'SECTOR_NAME' , 'COUNTRY_NAME' , 'TOWN_NAME' , 'CURRENCY_POLICY' , 'CURRENCY' , 'REPAYMENT_INTERVAL' , 'DISTRIBUTION_MODEL' ]) display ( test_input . shape ) #Target\u306e\u5bfe\u6570\u5909\u63db train_input = train_input . copy () train_input [ \"LOAN_AMOUNT\" ] = np . log ( train_input [ \"LOAN_AMOUNT\" ]) #train_x.select_dtypes(include='object').columns cat_col = [ 'LE_ACTIVITY_NAME' , 'LE_SECTOR_NAME' , 'LE_COUNTRY_NAME' , 'LE_TOWN_NAME' , 'LE_CURRENCY_POLICY' , 'LE_CURRENCY' , 'LE_REPAYMENT_INTERVAL' , 'LE_DISTRIBUTION_MODEL' ] pred_col = train_input . columns len ( train_input . columns . unique ()) len ( test_input . columns . unique ()) # ==================================================== # Training # ==================================================== trainer = SingleTrainer ( pred_col , cat_col = cat_col , dry_run = True ) models , score = trainer . train_model ( train_input ) print ( \"MAE_SCORE=\" + np . exp ( score )) \u63a8\u8ad6 \u00b6 # ==================================================== # Prediction # ==================================================== preds = [] for m in models : preds . append ( m . predict ( test_input [ pred_col ])) pred = np . exp ( np . mean ( preds , axis = 0 )) sub_predict = sub . copy () sub_predict [ \"LOAN_AMOUNT\" ] = pred sub_predict sub_predict = sub . copy () sub_predict . to_csv ( submission_dir + CONFIG . exp + \"/\" + \"submission.csv\" )","title":"NLP"},{"location":"nlp/#bert-signate","text":"","title":"BERT (Signate)"},{"location":"nlp/#_1","text":"input_dir = \"/content/drive/MyDrive/signate/SystematicReviewWorkShop-PeerSupportGroup/data/\" output_dir = \"/content/drive/MyDrive/signate/SystematicReviewWorkShop-PeerSupportGroup/log/\" submission_dir = \"/content/drive/MyDrive/signate/SystematicReviewWorkShop-PeerSupportGroup/submission/\" model_dir = \"/content/drive/MyDrive/signate/SystematicReviewWorkShop-PeerSupportGroup/model_bin/\" pred_dir = \"/content/drive/MyDrive/signate/SystematicReviewWorkShop-PeerSupportGroup/pred/\"","title":"\u30d5\u30a9\u30eb\u30c0\u6307\u5b9a"},{"location":"nlp/#_2","text":"import os import math import random import pandas as pd import numpy as np from glob import glob import gc gc . enable () import torch import torch.nn as nn import torch.optim as optim import torch.optim as optim from torch.optim.optimizer import Optimizer import torch.optim.lr_scheduler as lr_scheduler from torch.utils.data import Dataset , DataLoader , SequentialSampler , RandomSampler from sklearn.model_selection import StratifiedKFold from sklearn.metrics import fbeta_score from transformers import BertConfig , RobertaConfig from transformers import ( get_cosine_schedule_with_warmup , get_cosine_with_hard_restarts_schedule_with_warmup ) from transformers import BertTokenizer , RobertaTokenizer from transformers import BertModel , RobertaModel from transformers import AutoConfig from transformers import BertForSequenceClassification , RobertaForSequenceClassification from torch import cuda import time from transformers import AdamW from transformers import AutoTokenizer from transformers import AutoModel , AutoModelForSequenceClassification from transformers import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING from IPython.display import clear_output from tqdm import tqdm , trange","title":"\u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"nlp/#config","text":"class CFG : exp = \"exp002\" seed = 71 fold = 5 max_len = 71 epochs = 3 train_batch_size = 16 valid_batch_size = 32 model_name = \"bert-base-uncased\" CONFIG = CFG ()","title":"Config \u8a2d\u5b9a"},{"location":"nlp/#_3","text":"os . makedirs ( model_dir + CONFIG . exp + \"/\" , exist_ok = True ) os . makedirs ( pred_dir + CONFIG . exp + \"/\" , exist_ok = True ) os . makedirs ( output_dir + CONFIG . exp + \"/\" , exist_ok = True )","title":"\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u4f5c\u6210"},{"location":"nlp/#_4","text":"def set_random_seed ( random_seed ): random . seed ( random_seed ) np . random . seed ( random_seed ) os . environ [ \"PYTHONHASHSEED\" ] = str ( random_seed ) torch . manual_seed ( random_seed ) torch . cuda . manual_seed ( random_seed ) torch . cuda . manual_seed_all ( random_seed ) torch . backends . cudnn . deterministic = True set_random_seed ( CONFIG . seed )","title":"\u30b7\u30fc\u30c9\u306e\u56fa\u5b9a"},{"location":"nlp/#_5","text":"def init_logger ( log_file = output_dir + CONFIG . exp + f \"/ { CONFIG . exp } _train.log\" ): from logging import INFO , FileHandler , Formatter , StreamHandler , getLogger logger = getLogger ( __name__ ) logger . setLevel ( INFO ) handler1 = StreamHandler () handler1 . setFormatter ( Formatter ( \" %(message)s \" )) handler2 = FileHandler ( filename = log_file ) handler2 . setFormatter ( Formatter ( \" %(message)s \" )) logger . addHandler ( handler1 ) logger . addHandler ( handler2 ) return logger LOGGER = init_logger ()","title":"\u30ed\u30b0\u306e\u53d6\u5f97"},{"location":"nlp/#cpugpu","text":"device = 'cuda' if cuda . is_available () else 'cpu' print ( f ' { device } is used' )","title":"CPU/GPU\u306e\u6307\u5b9a"},{"location":"nlp/#_6","text":"def get_train_data ( train ): # \u4ea4\u5dee\u691c\u8a3c \u7528\u306e\u756a\u53f7\u3092\u632f\u308a\u307e\u3059\u3002 Fold = StratifiedKFold ( n_splits = CONFIG . fold , shuffle = True , random_state = CONFIG . seed ) for n , ( train_index , val_index ) in enumerate ( Fold . split ( train , train [ \"judgement\" ])): train . loc [ val_index , \"fold\" ] = int ( n ) train [ \"fold\" ] = train [ \"fold\" ] . astype ( np . uint8 ) return train def get_test_data ( test ): return test","title":"\u30c7\u30fc\u30bf\u5206\u5272"},{"location":"nlp/#_7","text":"import re import unicodedata def clean_text ( text ): replaced_text = text . lower () replaced_text = re . sub ( r '[\u3010\u3011]' , ' ' , replaced_text ) # \u3010\u3011\u306e\u9664\u53bb replaced_text = re . sub ( r '[\uff08\uff09()]' , ' ' , replaced_text ) # \uff08\uff09\u306e\u9664\u53bb replaced_text = re . sub ( r '[\uff3b\uff3d\\[\\]]' , ' ' , replaced_text ) # \uff3b\uff3d\u306e\u9664\u53bb replaced_text = re . sub ( r '[@\uff20]\\w+' , '' , replaced_text ) # \u30e1\u30f3\u30b7\u30e7\u30f3\u306e\u9664\u53bb replaced_text = re . sub ( r 'https?:\\/\\/.*?[\\r\\n ]' , '' , replaced_text ) # URL\u306e\u9664\u53bb replaced_text = re . sub ( r '\u3000' , ' ' , replaced_text ) # \u5168\u89d2\u7a7a\u767d\u306e\u9664\u53bb return replaced_text def normalize ( text ): normalized_text = normalize_unicode ( text ) normalized_text = normalize_number ( normalized_text ) normalized_text = lower_text ( normalized_text ) return normalized_text def lower_text ( text ): return text . lower () def normalize_unicode ( text , form = 'NFKC' ): normalized_text = unicodedata . normalize ( form , text ) return normalized_text def normalize_number ( text ): replaced_text = re . sub ( r '\\d+' , '0' , text ) return replaced_text def text_cleaning ( text ): text = clean_text ( text ) text = normalize ( text ) text = lower_text ( text ) text = normalize_unicode ( text ) return text def data_cleaning ( data ): return [ text_cleaning ( text ) for text in data ]","title":"\u524d\u51e6\u7406"},{"location":"nlp/#_8","text":"pd . set_option ( \"display.max_colwidth\" , 50 ) train = pd . read_csv ( input_dir + \"train.csv\" ) test = pd . read_csv ( input_dir + \"test.csv\" ) sub = pd . read_csv ( input_dir + \"sample_submit.csv\" , header = None ) sub . columns = [ \"id\" , \"judgement\" ] # \u6587\u5b57\u5316\u3051\u30c7\u30fc\u30bf\u3092\u9664\u5916\u3059\u308b\u306a\u3069\u306e\u51e6\u7406\u306f\u5165\u308c\u305f\u65b9\u304c\u3044\u3044?\u03b1\u3068\u304b\u304c\u6587\u5b57\u5316\u3051\u306e\u539f\u56e0 train [ \"title\" ] = data_cleaning ( train [ \"title\" ]) train . head () train = get_train_data ( train ) train . head ()","title":"\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"nlp/#_9","text":"class SRWSDataset ( Dataset ): def __init__ ( self , df , model_name , include_labels = True ): tokenizer = BertTokenizer . from_pretrained ( model_name ) self . df = df self . include_labels = include_labels self . title = df [ \"title\" ] . tolist () self . encoded = tokenizer . batch_encode_plus ( self . title , padding = \"max_length\" , max_length = CONFIG . max_len , truncation = True , return_attention_mask = True ) if self . include_labels : self . labels = df [ \"judgement\" ] . values def __len__ ( self ): return len ( self . df ) def __getitem__ ( self , idx ): input_ids = torch . tensor ( self . encoded [ \"input_ids\" ][ idx ]) attention_mask = torch . tensor ( self . encoded [ \"attention_mask\" ][ idx ]) if self . include_labels : label = torch . tensor ( self . labels [ idx ]) . float () return input_ids , attention_mask , label return input_ids , attention_mask","title":"\u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc"},{"location":"nlp/#bert","text":"class SRWSModel ( nn . Module ): def __init__ ( self , model_name ): super () . __init__ () self . model = BertForSequenceClassification . from_pretrained ( model_name , num_labels = 1 ) self . sigmoid = nn . Sigmoid () def forward ( self , input_ids , attention_mask ): out = self . model ( input_ids = input_ids , attention_mask = attention_mask ) out = self . sigmoid ( out . logits ) . squeeze () return out","title":"BERT\u30e2\u30c7\u30eb"},{"location":"nlp/#_10","text":"class AverageMeter ( object ): \"\"\"Computes and stores the average and current value\"\"\" def __init__ ( self ): self . reset () def reset ( self ): self . val = 0 self . avg = 0 self . sum = 0 self . count = 0 def update ( self , val , n = 1 ): self . val = val self . sum += val * n self . count += n self . avg = self . sum / self . count def asMinutes ( s ): m = math . floor ( s / 60 ) s -= m * 60 return \" %d m %d s\" % ( m , s ) def timeSince ( since , percent ): now = time . time () s = now - since es = s / ( percent ) rs = es - s return \" %s (remain %s )\" % ( asMinutes ( s ), asMinutes ( rs ))","title":"\u30e6\u30fc\u30c6\u30a3\u30ea\u30c6\u30a3\u30fc"},{"location":"nlp/#_11","text":"def train_fn ( train_loader , model , criterion , optimizer , epoch , device ): start = end = time . time () losses = AverageMeter () # switch to train mode model . train () for step , ( input_ids , attention_mask , labels ) in enumerate ( train_loader ): optimizer . zero_grad () input_ids = input_ids . to ( device ) attention_mask = attention_mask . to ( device ) labels = labels . to ( device ) batch_size = labels . size ( 0 ) y_preds = model ( input_ids , attention_mask ) loss = criterion ( y_preds , labels ) # record loss losses . update ( loss . item (), batch_size ) loss . backward () optimizer . step () if step % 100 == 0 or step == ( len ( train_loader ) - 1 ): print ( f \"Epoch: [ { epoch + 1 } ][ { step } / { len ( train_loader ) } ] \" f \"Elapsed { timeSince ( start , float ( step + 1 ) / len ( train_loader )) : s } \" f \"Loss: { losses . avg : .4f } \" ) return losses . avg def valid_fn ( valid_loader , model , criterion , device ): start = end = time . time () losses = AverageMeter () # switch to evaluation mode model . eval () preds = [] for step , ( input_ids , attention_mask , labels ) in enumerate ( valid_loader ): input_ids = input_ids . to ( device ) attention_mask = attention_mask . to ( device ) labels = labels . to ( device ) batch_size = labels . size ( 0 ) # compute loss with torch . no_grad (): y_preds = model ( input_ids , attention_mask ) loss = criterion ( y_preds , labels ) losses . update ( loss . item (), batch_size ) # record score preds . append ( y_preds . to ( \"cpu\" ) . numpy ()) if step % 100 == 0 or step == ( len ( valid_loader ) - 1 ): print ( f \"EVAL: [ { step } / { len ( valid_loader ) } ] \" f \"Elapsed { timeSince ( start , float ( step + 1 ) / len ( valid_loader )) : s } \" f \"Loss: { losses . avg : .4f } \" ) predictions = np . concatenate ( preds ) return losses . avg , predictions def inference (): predictions = [] test_dataset = SRWSDataset ( test , CONFIG . model_name , include_labels = False ) test_loader = DataLoader ( test_dataset , batch_size = 16 , shuffle = False , num_workers = 4 , pin_memory = True ) for fold in range ( CONFIG . fold ): LOGGER . info ( f \"========== model: { CONFIG . model_name } fold: { fold } inference ==========\" ) model = SRWSModel ( CONFIG . model_name ) model . to ( device ) model . load_state_dict ( torch . load ( model_dir + CONFIG . exp + \"/\" + f \" { CONFIG . model_name } _fold { fold } _best.pth\" )[ \"model\" ]) model . eval () preds = [] for i , ( input_ids , attention_mask ) in tqdm ( enumerate ( test_loader ), total = len ( test_loader )): input_ids = input_ids . to ( device ) attention_mask = attention_mask . to ( device ) with torch . no_grad (): y_preds = model ( input_ids , attention_mask ) preds . append ( y_preds . to ( \"cpu\" ) . numpy ()) preds = np . concatenate ( preds ) predictions . append ( preds ) predictions = np . mean ( predictions , axis = 0 ) return predictions","title":"\u8a13\u7df4/\u63a8\u8ad6"},{"location":"nlp/#_12","text":"def train_loop ( train , fold ): LOGGER . info ( f \"========== fold: { fold } training ==========\" ) # ==================================================== # Data Loader # ==================================================== trn_idx = train [ train [ \"fold\" ] != fold ] . index val_idx = train [ train [ \"fold\" ] == fold ] . index train_folds = train . loc [ trn_idx ] . reset_index ( drop = True ) valid_folds = train . loc [ val_idx ] . reset_index ( drop = True ) train_dataset = SRWSDataset ( train_folds , CONFIG . model_name ) valid_dataset = SRWSDataset ( valid_folds , CONFIG . model_name ) train_loader = DataLoader ( train_dataset , batch_size = CONFIG . train_batch_size , shuffle = True , num_workers = 4 , pin_memory = True , drop_last = True , ) valid_loader = DataLoader ( valid_dataset , batch_size = CONFIG . valid_batch_size , shuffle = False , num_workers = 4 , pin_memory = True , drop_last = False , ) # ==================================================== # Model # ==================================================== model = SRWSModel ( CONFIG . model_name ) model . to ( device ) optimizer = AdamW ( model . parameters (), lr = 2e-5 ) criterion = nn . BCELoss () # ==================================================== # Loop # ==================================================== best_score = - 1 best_loss = np . inf for epoch in range ( CONFIG . epochs ): start_time = time . time () # train avg_loss = train_fn ( train_loader , model , criterion , optimizer , epoch , device ) # eval avg_val_loss , preds = valid_fn ( valid_loader , model , criterion , device ) valid_labels = valid_folds [ \"judgement\" ] . values # scoring score = fbeta_score ( valid_labels , np . where ( preds < border , 0 , 1 ), beta = 7.0 ) elapsed = time . time () - start_time LOGGER . info ( f \"Epoch { epoch + 1 } - avg_train_loss: { avg_loss : .4f } avg_val_loss: { avg_val_loss : .4f } time: { elapsed : .0f } s\" ) LOGGER . info ( f \"Epoch { epoch + 1 } - Score: { score } \" ) if score > best_score : best_score = score LOGGER . info ( f \"Epoch { epoch + 1 } - Save Best Score: { best_score : .4f } Model\" ) torch . save ( { \"model\" : model . state_dict (), \"preds\" : preds }, model_dir + CONFIG . exp + \"/\" + f \" { CONFIG . model_name } _fold { fold } _best.pth\" ) check_point = torch . load ( model_dir + CONFIG . exp + \"/\" + f \" { CONFIG . model_name } _fold { fold } _best.pth\" ) valid_folds [ \"preds\" ] = check_point [ \"preds\" ] return valid_folds def get_result ( result_df ): preds = result_df [ \"preds\" ] . values labels = result_df [ \"judgement\" ] . values score = fbeta_score ( labels , np . where ( preds < border , 0 , 1 ), beta = 7.0 ) LOGGER . info ( f \"Score: { score : <.5f } \" )","title":"\u8a13\u7df4\u5b9f\u884c"},{"location":"nlp/#_13","text":"# Training border = len ( train [ train [ \"judgement\" ] == 1 ]) / len ( train [ \"judgement\" ]) oof_df = pd . DataFrame () for fold in range ( CONFIG . fold ): _oof_df = train_loop ( train , fold ) oof_df = pd . concat ([ oof_df , _oof_df ]) LOGGER . info ( f \"========== fold: { fold } result ==========\" ) get_result ( _oof_df ) # CV result LOGGER . info ( f \"========== CV ==========\" ) get_result ( oof_df ) # Save OOF result oof_df . to_csv ( pred_dir + CONFIG . exp + \"/oof_df.csv\" , index = False ) # Inference predictions = inference () predictions = np . where ( predictions < border , 0 , 1 ) # submission sub [ \"judgement\" ] = predictions sub . to_csv ( submission_dir + CONFIG . exp + \"_submission.csv\" , index = False , header = False )","title":"\u5b66\u7fd2\u306e\u5b9f\u884c"},{"location":"nlp/#light-gbm-kiva","text":"","title":"Light GBM (Kiva)"},{"location":"nlp/#_14","text":"import logging import datetime import warnings import joblib import pandas as pd pd . set_option ( \"display.max_colwidth\" , 50 ) import numpy as np import matplotlib.pyplot as plt from currency_converter import CurrencyConverter import seaborn as sns import os import re from tqdm import tqdm from sklearn.metrics import fbeta_score from sklearn.utils import class_weight from sklearn.model_selection import StratifiedKFold from sklearn.metrics import confusion_matrix from sklearn.decomposition import TruncatedSVD , PCA from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer from gensim.models.doc2vec import Doc2Vec , TaggedDocument from scipy.optimize import minimize , minimize_scalar from contextlib import contextmanager from sklearn import model_selection , metrics import category_encoders as ce import torch import lightgbm as lgb from time import time import texthero as hero import transformers import pycld2 as cld2 #\u30e2\u30b8\u30e5\u30fc\u30eb\u8aad\u307f\u8fbc\u307f import sys , os sys . path . append ( '/home/azureuser/cloudfiles/code/Users/hirahara.kazuki/module' ) import visualize_module import preprocess_module import importlib importlib . reload ( visualize_module ) importlib . reload ( preprocess_module ) vs = visualize_module . Feature_Confirmation () sc = preprocess_module . ScalerModule () en = preprocess_module . Encoding_Module ()","title":"\u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"nlp/#config_1","text":"# ==================================================== # CFG # ==================================================== class CONFIG : exp = 'baseline' bert_model_name = 'bert-base-uncased' fold = 5 seed = 71","title":"CONFIG"},{"location":"nlp/#_15","text":"# ==================================================== # Folders # ==================================================== input_dir = \"../data/\" interm_dir = \"../interm/\" submission_dir = \"../submission/\" model_dir = \"../model/\" pred_dir = \"../pred/\" os . makedirs ( model_dir + CONFIG . exp + \"/\" , exist_ok = True ) os . makedirs ( pred_dir + CONFIG . exp + \"/\" , exist_ok = True ) os . makedirs ( interm_dir + CONFIG . exp + \"/\" , exist_ok = True ) os . makedirs ( submission_dir + CONFIG . exp + \"/\" , exist_ok = True )","title":"\u30d5\u30a9\u30eb\u30c0\u30fc\u306e\u6307\u5b9a\u3068\u8aad\u307f\u8fbc\u307f"},{"location":"nlp/#_16","text":"# ==================================================== # Read Data # ==================================================== train = pd . read_csv ( input_dir + \"train.csv\" ) test = pd . read_csv ( input_dir + \"test.csv\" ) sub = pd . read_csv ( input_dir + \"sample_submission.csv\" ) #Train_Dataset display ( train . head ()) display ( train . shape ) #Test_Dataset display ( test . head ()) display ( test . shape )","title":"\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f"},{"location":"nlp/#_17","text":"# ==================================================== # Delete Unrequired Columns # ==================================================== # \u4e0d\u8981\u306a\u30ab\u30e9\u30e0\u306e\u524a\u9664 drop_columns = [ #'LOAN_ID', 'ORIGINAL_LANGUAGE' , 'DESCRIPTION' , #\u3068\u308a\u3042\u3048\u305a\u7ffb\u8a33\u3092\u4fe1\u983c\u3059\u308b 'IMAGE_ID' , 'COUNTRY_CODE' ] train = train . drop ( drop_columns , axis = 1 ) test = test . drop ( drop_columns , axis = 1 )","title":"\u4e0d\u8981\u306a\u30ab\u30e9\u30e0\u306e\u524a\u9664"},{"location":"nlp/#_18","text":"# ==================================================== # Basic Preprocess of Words # ==================================================== #Text hero\u306b\u3088\u308b\u524d\u51e6\u7406 def cleansing_hero_only_text ( input_df , text_col ): ## get text only custom_pipeline = [ hero . preprocessing . fillna , hero . preprocessing . lowercase , #\u5c0f\u6587\u5b57\u5909\u63db #hero.preprocessing.remove_digits, #hero.preprocessing.remove_punctuation,# \u53e5\u8aad\u70b9\u306e\u524a\u9664 hero . preprocessing . remove_diacritics , # \u30c0\u30a4\u30a2\u30af\u30ea\u30c6\u30a3\u30ab\u30eb\u30de\u30fc\u30af\uff08\u767a\u97f3\u533a\u5225\u7b26\u53f7\u3002\u00e0\u3084\u00e9\u306a\u3069\uff09\u306e\u524a\u9664 hero . preprocessing . remove_stopwords , # \u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u306e\u524a\u9664 hero . preprocessing . remove_whitespace , # \u30b9\u30da\u30fc\u30b9\u306e\u524a\u9664 #hero.preprocessing.stem ] texts = hero . clean ( input_df [ text_col ], custom_pipeline ) return texts #Punct\u524a\u9664 def clean_puncts ( x ): puncts = [ ',' , '.' , '\"' , ':' , ')' , '(' , '-' , '!' , '?' , '|' , ';' , \"'\" , '$' , '&' , '/' , '[' , ']' , '>' , '%' , '=' , '#' , '*' , '+' , ' \\\\ ' , '\u2022' , '~' , '@' , '\u00a3' , '\u00b7' , '_' , '{' , '}' , '\u00a9' , '^' , '\u00ae' , '`' , '<' , '\u2192' , '\u00b0' , '\u20ac' , '\u2122' , '\u203a' , '\u2665' , '\u2190' , '\u00d7' , '\u00a7' , '\u2033' , '\u2032' , '\u00c2' , '\u2588' , '\u00bd' , '\u00e0' , '\u2026' , '\u201c' , '\u2605' , '\u201d' , '\u2013' , '\u25cf' , '\u00e2' , '\u25ba' , '\u2212' , '\u00a2' , '\u00b2' , '\u00ac' , '\u2591' , '\u00b6' , '\u2191' , '\u00b1' , '\u00bf' , '\u25be' , '\u2550' , '\u00a6' , '\u2551' , '\u2015' , '\u00a5' , '\u2593' , '\u2014' , '\u2039' , '\u2500' , '\u2592' , '\uff1a' , '\u00bc' , '\u2295' , '\u25bc' , '\u25aa' , '\u2020' , '\u25a0' , '\u2019' , '\u2580' , '\u00a8' , '\u2584' , '\u266b' , '\u2606' , '\u00e9' , '\u00af' , '\u2666' , '\u00a4' , '\u25b2' , '\u00e8' , '\u00b8' , '\u00be' , '\u00c3' , '\u22c5' , '\u2018' , '\u221e' , '\u00ab' , '\u2219' , '\uff09' , '\u2193' , '\u3001' , '\u2502' , '\uff08' , '\u00bb' , '\uff0c' , '\u266a' , '\u2569' , '\u255a' , '\u00b3' , '\u30fb' , '\u2566' , '\u2563' , '\u2554' , '\u2557' , '\u25ac' , '\u2764' , '\u00ef' , '\u00d8' , '\u00b9' , '\u2264' , '\u2021' , '\u221a' , '\uff08' , '\uff09' , '\uff5e' , '\u27a1' , '\uff05' , '\u21d2' , '\u25b6' , '\u300c' , '\u2784' , '\u2786' , '\u278a' , '\u278b' , '\u278c' , '\u278d' , '\u24ea' , '\u2460' , '\u2461' , '\u2462' , '\u2463' , '\u2464' , '\u2470' , '\u2776' , '\u2777' , '\u2778' , '\u2779' , '\u277a' , '\u277b' , '\u277c' , '\u277d' , '\uff1d' , '\u203b' , '\u3231' , '\uff64' , '\u25b3' , '\u212e' , '\u217c' , '\u2010' , '\uff63' , '\u251d' , '\u21b3' , '\u25c9' , '\uff0f' , '\uff0b' , '\u25cb' , '\u3010' , '\u3011' , '\u2705' , '\u2611' , '\u27a4' , '\uff9e' , '\u21b3' , '\u3036' , '\u261b' , '\uff62' , '\u207a' , '\u300e' , '\u226b' , '\u00c2\u00a9' , '<sub>' , 'A\uff8e\uff72' , '\uff8e\uff72' , \"\uff83\uff69\" ] for punct in puncts : x = x . replace ( punct , '' ) return x #\u6539\u884c\u30b3\u30fc\u30c9\u306e\u524a\u9664 def remove_br ( text ): text = text . replace ( \"br\" , \"\" ) #\u534a\u89d2\u30b9\u30da\u30fc\u30b9\u306e\u524a\u9664 text = text . replace ( \" \" , \" \" ) text = text . replace ( \" \" , \" \" ) return text #\u91d1\u984d\u306e\u53d6\u5f97 def get_price ( x ): # #usd\u306b\u5909\u63db\u3067\u304d\u308b\u8ca8\u5e63\u306f\u5909\u63db\u3059\u308b # def change_usd(x): # rate_unit=x[-3:].upper() # price=re.sub(\"[a-z\\s]\",\"\",x) # c = CurrencyConverter() # try: # x=c.convert(price, rate_unit, 'USD') # except: # x=price # return x re_text = ( \"\\d+\\sphp|\\d+\\scop|\\d+\\shnl|\\d+\\skes|\\d+\\susd|\\d+\\srwf|\" \"\\d+\\spyg|\\d+\\spen|\\d+\\svnd|\\d+\\smga|\\d+\\sinr|\\d+\\smzn|\" \"\\d+\\spkr|\\d+\\sjod|\\d+\\sxof|\\d+\\slrd|\\d+\\sugx|\\d+\\sgtq|\" \"\\d+\\smxn|\\d+\\skhr|\\d+\\stjs|\\d+\\shtg|\\d+\\segp|\\d+\\sidr|\" \"\\d+\\snio|\\d+\\sghs|\\d+\\sfjd|\\d+\\sbob|\\d+\\ssbd|\\d+\\swst|\" \"\\d+\\sbrl|\\d+\\sngn|\\d+\\sxaf|\\d+\\seur|\\d+\\skgs|\\d+\\smwk|\" \"\\d+\\sgel|\\d+\\sall|\\d+\\szmw|\\d+\\scrc|\\d+\\stop|\\d+\\smdl|\" \"\\d+\\slsl|\\d+\\sdop|\\d+\\ssll|\\d+\\stry|\\d+\\snpr|\\d+\\sthb|\" \"\\d+\\spgk|\\d+\\sils|\\d+\\samd\" ) results = re . findall ( re_text , x ) if len ( results ) == 0 : return 0 else : price_list = [] for result in results : price_v = re . sub ( \"[a-z\\s]\" , \"\" , result ) price_list . append ( price_v ) price_list = [ s . strip ( ' ' ) for s in price_list ] #\u4e00\u6642\u7684\u306b\u6700\u5927\u5024\u3092\u53d6\u5f97 price_max = max ( price_list ) #\u4e00\u756a\u306f\u3058\u3081\u306e\u5024\u3092\u7533\u8acb\u8005\u306e\u5e74\u9f62\u3068\u4eee\u5b9a return price_max #\u6b63\u898f\u5316\u8868\u73fe\u306b\u3088\u308b\u306b\u5e74\u9f62\u53d6\u5f97 def get_age ( x ): result_1 = re . findall ( '\\d {2} \\syears' , x ) result_2 = re . findall ( '\\d {2} year' , x ) result = result_1 + result_2 if len ( result ) == 0 : return 0 else : result = [ s . strip ( 'years' ) for s in result ] result = [ s . strip ( 'year' ) for s in result ] result = [ s . strip ( ' ' ) for s in result ] result = [ s . strip ( ' ' ) for s in result ] #\u4e00\u756a\u306f\u3058\u3081\u306e\u5024\u3092\u7533\u8acb\u8005\u306e\u5e74\u9f62\u3068\u4eee\u5b9a result = result [ 0 ] return result def basic_preprocess ( input_df ): output_df = input_df . copy () output_df [ \"DESCRIPTION_TRANSLATED\" ] = cleansing_hero_only_text ( output_df , \"DESCRIPTION_TRANSLATED\" ) output_df [ \"DESCRIPTION_TRANSLATED\" ] = output_df [ \"DESCRIPTION_TRANSLATED\" ] . apply ( lambda x : clean_puncts ( x )) output_df [ \"DESCRIPTION_TRANSLATED\" ] = output_df [ \"DESCRIPTION_TRANSLATED\" ] . apply ( lambda x : remove_br ( x )) output_df [ \"age\" ] = output_df [ \"DESCRIPTION_TRANSLATED\" ] . apply ( lambda x : get_age ( x )) output_df [ \"price\" ] = output_df [ \"DESCRIPTION_TRANSLATED\" ] . apply ( lambda x : get_price ( x )) output_df [ \"age\" ] = output_df [ \"age\" ] . astype ( \"int\" ) output_df [ \"price\" ] = output_df [ \"price\" ] . astype ( \"int\" ) return output_df","title":"\u524d\u51e6\u7406"},{"location":"nlp/#_19","text":"# ==================================================== # Feature Engineering of WORDS # ==================================================== #\u5404\u7279\u5fb4\u91cf\u306e\u53d6\u5f97 def basic_text_features_transforme ( input_df , text_columns , name = \"\" ): \"\"\"basic \u306a text \u7279\u5fb4\u91cf\"\"\" def _get_features ( dataframe , column ): _df = pd . DataFrame () _df [ column + name + '_num_chars' ] = dataframe [ column ] . apply ( len ) _df [ column + name + '_num_exclamation_marks' ] = dataframe [ column ] . apply ( lambda x : x . count ( '!' )) _df [ column + name + '_num_question_marks' ] = dataframe [ column ] . apply ( lambda x : x . count ( '?' )) _df [ column + name + '_num_punctuation' ] = dataframe [ column ] . apply ( lambda x : sum ( x . count ( w ) for w in '.,;:' )) _df [ column + name + '_num_symbols' ] = dataframe [ column ] . apply ( lambda x : sum ( x . count ( w ) for w in '*&$%' )) _df [ column + name + '_num_words' ] = dataframe [ column ] . apply ( lambda x : len ( x . split ())) _df [ column + name + '_num_unique_words' ] = dataframe [ column ] . apply ( lambda x : len ( set ( w for w in x . split ()))) _df [ column + name + '_words_vs_unique' ] = _df [ column + name + '_num_unique_words' ] / _df [ column + name + '_num_words' ] _df [ column + name + '_words_vs_chars' ] = _df [ column + name + '_num_words' ] / _df [ column + name + '_num_chars' ] return _df def vectorize_text ( input_df , text_columns , vectorizer = CountVectorizer (), transformer = TruncatedSVD ( n_components = 128 ), name = 'html_count_svd' ): output_df = pd . DataFrame () output_df [ text_columns ] = input_df [ text_columns ] . fillna ( 'missing' ) . astype ( str ) features = [] for c in text_columns : sentence = vectorizer . fit_transform ( output_df [ c ]) feature = transformer . fit_transform ( sentence ) num_p = feature . shape [ 1 ] feature = pd . DataFrame ( feature , columns = [ name + str ( c ) + str ( num_p ) + f '= { i : 03 } ' for i in range ( num_p )]) features . append ( feature ) output_df = pd . concat ( features , axis = 1 ) return output_df class BERT_Vectorizer : \"\"\" \u4e8b\u524d\u5b66\u7fd2\u6e08\u307f BERT \u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u305f\u30c6\u30ad\u30b9\u30c8\u7279\u5fb4\u62bd\u51fa https://www.guruguru.science/competitions/16/discussions/fb792c87-6bad-445d-aa34-b4118fc378c1/ \"\"\" def __init__ ( self , model_name = 'bert-base-uncased' , max_len = 128 ): self . device = 'cuda' if torch . cuda . is_available () else 'cpu' self . model_name = model_name self . tokenizer = transformers . AutoTokenizer . from_pretrained ( self . model_name ) self . model = transformers . AutoModel . from_pretrained ( self . model_name ) self . model = self . model . to ( self . device ) self . max_len = max_len def vectorize ( self , sentence : str ) -> np . array : inp = self . tokenizer . encode ( sentence ) len_inp = len ( inp ) if len_inp >= self . max_len : inputs = inp [: self . max_len ] masks = [ 1 ] * self . max_len else : inputs = inp + [ 0 ] * ( self . max_len - len_inp ) masks = [ 1 ] * len_inp + [ 0 ] * ( self . max_len - len_inp ) inputs_tensor = torch . tensor ([ inputs ], dtype = torch . long ) . to ( self . device ) masks_tensor = torch . tensor ([ masks ], dtype = torch . long ) . to ( self . device ) output = self . model ( inputs_tensor , masks_tensor ) seq_out , pooled_out = output [ 'last_hidden_state' ], output [ 'pooler_output' ] if torch . cuda . is_available (): return seq_out [ 0 ][ 0 ] . cpu () . detach () . numpy () # 0\u756a\u76ee\u306f [CLS] token, 768 dim \u306e\u6587\u7ae0\u7279\u5fb4\u91cf else : return seq_out [ 0 ][ 0 ] . detach () . numpy () def get_basic_text_features ( input_df ): output_df = basic_text_features_transforme ( input_df , text_columns = [ \"DESCRIPTION_TRANSLATED\" , \"LOAN_USE\" ]) return output_df def get_tfidf_features__svd64 ( input_df ): output_df = vectorize_text ( input_df , text_columns = [ \"DESCRIPTION_TRANSLATED\" , \"LOAN_USE\" ], vectorizer = TfidfVectorizer ( min_df = 0.001 , max_df = 0.99 ), transformer = TruncatedSVD ( n_components = 64 ), name = \"tfidf_svd_\" ) return output_df def get_count_features__svd64 ( input_df ): output_df = vectorize_text ( input_df , text_columns = [ \"DESCRIPTION_TRANSLATED\" , \"LOAN_USE\" ], vectorizer = CountVectorizer ( min_df = 0.001 , max_df = 0.99 ), transformer = TruncatedSVD ( n_components = 64 ), name = \"count_svd_\" ) return output_df def get_bert_feature_description ( input_df ): vectorizer = BERT_Vectorizer ( model_name = CONFIG . bert_model_name ) texts = input_df [ 'DESCRIPTION_TRANSLATED' ] . fillna ( '' ) text_vecs = np . array ([ vectorizer . vectorize ( x ) for x in texts ]) pca = PCA ( n_components = 32 ) text_vecs = pca . fit_transform ( text_vecs ) output_df = pd . DataFrame ( text_vecs , columns = [ f 'bert_description_vecs= { i : 03 } ' for i in range ( text_vecs . shape [ 1 ])]) output_df . index = input_df . index return output_df def get_bert_feature_loan ( input_df ): vectorizer = BERT_Vectorizer ( model_name = CONFIG . bert_model_name ) texts = input_df [ 'LOAN_USE' ] . fillna ( '' ) text_vecs = np . array ([ vectorizer . vectorize ( x ) for x in texts ]) pca = PCA ( n_components = 32 ) text_vecs = pca . fit_transform ( text_vecs ) output_df = pd . DataFrame ( text_vecs , columns = [ f 'bert_loan_vecs= { i : 03 } ' for i in range ( text_vecs . shape [ 1 ])]) output_df . index = input_df . index return output_df # ==================================================== # Feature Engineering of Category # ==================================================== def target_encoding ( input_df ): target_cols = [ 'ACTIVITY_NAME' , 'SECTOR_NAME' , 'COUNTRY_NAME' , 'TOWN_NAME' , 'CURRENCY' ] target = input_df [ \"LOAN_AMOUNT\" ] features = input_df [ target_cols ] encoder = ce . TargetEncoder () . fit ( features . values , target ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"TE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) output_df = output_df . iloc [:, - len ( target_cols ):] return output_df def label_encoding ( input_df ): target_cols = [ 'ACTIVITY_NAME' , 'SECTOR_NAME' , 'COUNTRY_NAME' , 'TOWN_NAME' , 'CURRENCY_POLICY' , 'CURRENCY' , 'REPAYMENT_INTERVAL' , 'DISTRIBUTION_MODEL' ] features = input_df [ target_cols ] encoder = ce . OrdinalEncoder () . fit ( features . values ) output_df = pd . DataFrame ( encoder . transform ( features . values )) output_df . columns = target_cols output_df = output_df . add_prefix ( \"LE_\" ) output_df = pd . concat ([ input_df , output_df ], axis = 1 ) output_df = output_df . iloc [:, - len ( target_cols ):] return output_df","title":"\u7279\u5fb4\u91cf\u306e\u62bd\u51fa"},{"location":"nlp/#_20","text":"def preprocess ( train , test ): \"\"\"\u524d\u51e6\u7406\u306e\u5b9f\u884c\u95a2\u6570\"\"\" input_df = pd . concat ([ train , test ]) . reset_index ( drop = True ) funcs = [ basic_preprocess , get_basic_text_features , get_tfidf_features__svd64 , get_count_features__svd64 , #get_bert_feature_description, #get_bert_feature_loan, target_encoding , label_encoding ] output = [] for func in funcs : _df = func ( input_df ) output . append ( _df ) output = pd . concat ( output , axis = 1 ) train_x = output . iloc [: len ( train )] test_x = output . iloc [ len ( train ):] . reset_index ( drop = True ) return train_x , test_x train_x , test_x = preprocess ( train , test )","title":"\u524d\u51e6\u7406\u306e\u5b9f\u884c"},{"location":"nlp/#_21","text":"# ==================================================== # Model Preparation # ==================================================== class SingleLgb : def __init__ ( self , cat_col , seed = CONFIG . seed , dry_run = False ): self . train_param = self . get_param ( seed ) if dry_run : self . num_rounds = 10000 else : self . num_rounds = 10000 self . cat_col = cat_col def do_train_direct ( self , x_train , x_test , y_train , y_test ): lgb_train = lgb . Dataset ( x_train , y_train ) lgb_eval = lgb . Dataset ( x_test , y_test ) print ( 'Start training...' ) model = lgb . train ( self . train_param , lgb_train , valid_sets = [ lgb_eval ], verbose_eval = 100 , num_boost_round = self . num_rounds , early_stopping_rounds = 100 , categorical_feature = self . cat_col ) print ( 'End training...' ) return model @staticmethod def show_feature_importance ( model , filename = None ): fi = pd . DataFrame ({ \"name\" : model . feature_name (), \"importance_split\" : model . feature_importance ( importance_type = \"split\" ) . astype ( int ), \"importance_gain\" : model . feature_importance ( importance_type = \"gain\" ) . astype ( int ), }) fi = fi . sort_values ( by = \"importance_gain\" , ascending = False ) print ( fi ) @staticmethod def get_param ( seed = CONFIG . seed ): return { 'num_leaves' : 1023 , 'min_data_in_leaf' : 50 , 'objective' : 'regression' , 'metric' : 'mae' , 'max_depth' : - 1 , 'learning_rate' : 0.05 , \"boosting\" : \"gbdt\" , \"feature_fraction\" : 0.9 , \"verbosity\" : - 1 , \"random_state\" : seed , } class SingleTrainer : def __init__ ( self , pred_col , cat_col , dry_run = False ): self . pred_col = pred_col self . target_col = 'LOAN_AMOUNT' self . dry_run = dry_run self . val_size = 1000 * 1000 self . cat_col = cat_col def train_model ( self , df ): X = df [ self . pred_col ] y = df [ self . target_col ] kf = model_selection . KFold ( n_splits = CONFIG . fold ) models , scores = list (), list () for fold , ( train_idx , test_idx ) in enumerate ( kf . split ( X )): print ( \"---------\" ) print ( \"fold=\" , fold ) X_train , X_val = X . iloc [ train_idx ], X . iloc [ test_idx ] y_train , y_val = y . iloc [ train_idx ], y . iloc [ test_idx ] print ( X_train . shape , X_val . shape ) lgbm = SingleLgb ( seed = CONFIG . seed , dry_run = self . dry_run , cat_col = self . cat_col ) model = lgbm . do_train_direct ( X_train , X_val , y_train , y_val ) score = model . best_score [ \"valid_0\" ][ \"mae\" ] if fold == 0 : lgbm . show_feature_importance ( model ) models . append ( model ) scores . append ( score ) print ( f 'fold= { fold } MAE Score' ) self . eval_model ( model , X_val , y_val ) return models , np . mean ( scores ) def eval_model ( self , model , X_val , y_val ): temp_pred = model . predict ( X_val ) #temp_pred = np.argmax(temp_pred, axis=1) mae_score = temp_pred . mean () print ( pd . DataFrame ( temp_pred ) . value_counts ()) print ( \"Score=\" , np . exp ( mae_score )) # ==================================================== # Make Model Input # ==================================================== #drop unrequired_columns train_input = train_x . drop ( columns = [ \"TAGS\" , \"LOAN_ID\" , 'DESCRIPTION_TRANSLATED' , 'LOAN_USE' , 'ACTIVITY_NAME' , 'SECTOR_NAME' , 'COUNTRY_NAME' , 'TOWN_NAME' , 'CURRENCY_POLICY' , 'CURRENCY' , 'REPAYMENT_INTERVAL' , 'DISTRIBUTION_MODEL' ]) display ( train_input . shape ) test_input = test_x . drop ( columns = [ \"TAGS\" , \"LOAN_ID\" , 'DESCRIPTION_TRANSLATED' , \"LOAN_AMOUNT\" , 'LOAN_USE' , 'ACTIVITY_NAME' , 'SECTOR_NAME' , 'COUNTRY_NAME' , 'TOWN_NAME' , 'CURRENCY_POLICY' , 'CURRENCY' , 'REPAYMENT_INTERVAL' , 'DISTRIBUTION_MODEL' ]) display ( test_input . shape ) #Target\u306e\u5bfe\u6570\u5909\u63db train_input = train_input . copy () train_input [ \"LOAN_AMOUNT\" ] = np . log ( train_input [ \"LOAN_AMOUNT\" ]) #train_x.select_dtypes(include='object').columns cat_col = [ 'LE_ACTIVITY_NAME' , 'LE_SECTOR_NAME' , 'LE_COUNTRY_NAME' , 'LE_TOWN_NAME' , 'LE_CURRENCY_POLICY' , 'LE_CURRENCY' , 'LE_REPAYMENT_INTERVAL' , 'LE_DISTRIBUTION_MODEL' ] pred_col = train_input . columns len ( train_input . columns . unique ()) len ( test_input . columns . unique ()) # ==================================================== # Training # ==================================================== trainer = SingleTrainer ( pred_col , cat_col = cat_col , dry_run = True ) models , score = trainer . train_model ( train_input ) print ( \"MAE_SCORE=\" + np . exp ( score ))","title":"\u5b66\u7fd2"},{"location":"nlp/#_22","text":"# ==================================================== # Prediction # ==================================================== preds = [] for m in models : preds . append ( m . predict ( test_input [ pred_col ])) pred = np . exp ( np . mean ( preds , axis = 0 )) sub_predict = sub . copy () sub_predict [ \"LOAN_AMOUNT\" ] = pred sub_predict sub_predict = sub . copy () sub_predict . to_csv ( submission_dir + CONFIG . exp + \"/\" + \"submission.csv\" )","title":"\u63a8\u8ad6"},{"location":"nlp_basic/","text":"NLP Basic \u00b6 \u30c6\u30ad\u30b9\u30c8\u306e\u30af\u30ea\u30fc\u30cb\u30f3\u30b0(\u30d4\u30ea\u30aa\u30c9\u306e\u524a\u9664/\u5927\u6587\u5b57\u5909\u63db/\u6b63\u898f\u62bd\u51fa\u5024\u306e\u5909\u63db) \u00b6 # -*- coding: utf-8 -*- # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 text_data = [ \" Interrobang. By Aishwarya Henriette \" , \"Parking And Going. By Karl Gautier\" , \" Today Is The night. By Jarek Prakash \" ] # \u30db\u30ef\u30a4\u30c8\u30b9\u30da\u30fc\u30b9\u3092\u524a\u9664 strip_whitespace = [ string . strip () for string in text_data ] # \u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a strip_whitespace ########## # \u30d4\u30ea\u30aa\u30c9\u3092\u524a\u9664 remove_periods = [ string . replace ( \".\" , \"\" ) for string in strip_whitespace ] # \u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a remove_periods ########## # \u95a2\u6570\u3092\u5b9a\u7fa9 def capitalizer ( string : str ) -> str : return string . upper () # \u95a2\u6570\u3092\u9069\u7528 [ capitalizer ( string ) for string in remove_periods ] ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import re # \u95a2\u6570\u3092\u5b9a\u7fa9 def replace_letters_with_X ( string : str ) -> str : return re . sub ( r \"[a-zA-Z]\" , \"X\" , string ) # \u95a2\u6570\u3092\u9069\u7528 [ replace_letters_with_X ( string ) for string in remove_periods ] HTML\u306e\u30d1\u30fc\u30b9\u3068\u30af\u30ea\u30fc\u30cb\u30f3\u30b0 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from bs4 import BeautifulSoup # HTML\u30c6\u30ad\u30b9\u30c8\u3092\u4f5c\u6210 html = \"\"\" <div class='full_name'><span style='font-weight:bold'> Masego</span> Azra</div> \"\"\" # HTML\u3092\u30d1\u30fc\u30b9 soup = BeautifulSoup ( html , \"lxml\" ) # class\u304c\"full_name\"\u3068\u306a\u3063\u3066\u3044\u308bdiv\u3092\u898b\u3064\u3051\u3066\u3001\u305d\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a soup . find ( \"div\" , { \"class\" : \"full_name\" }) . text . strip () \u53e5\u8aad\u70b9\u306e\u9664\u53bb \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import unicodedata import sys # \u30c6\u30ad\u30b9\u30c8\u3092\u4f5c\u6210 text_data = [ 'Hi!!!! I. Love. This. Song....' , '10000% Agree!!!! #LoveIT' , 'Right?!?!' ] # \u53e5\u8aad\u70b9\u6587\u5b57\u3092\u542b\u3080\u8f9e\u66f8\u3092\u4f5c\u6210 punctuation = dict . fromkeys ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( 'P' )) # \u305d\u308c\u305e\u308c\u306e\u6587\u5b57\u5217\u304b\u3089\u3001\u53e5\u8aad\u70b9\u6587\u5b57\u3092\u3059\u3079\u3066\u9664\u53bb [ string . translate ( punctuation ) for string in text_data ] \u30c6\u30ad\u30b9\u30c8\u306e\u30c8\u30fc\u30af\u30f3\u5316 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.tokenize import word_tokenize # \u6700\u521d\u306e1\u56de\u306f\u4e0b\u306e\u30b3\u30e1\u30f3\u30c8\u5916\u3057\u3066\u30ea\u30bd\u30fc\u30b9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 # import nltk # nltk.download('punkt') # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 string = \"The science of today is the technology of tomorrow\" # \u5358\u8a9e\u5358\u4f4d\u3067\u30c8\u30fc\u30af\u30f3\u5316 word_tokenize ( string ) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.tokenize import sent_tokenize # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 string = \"The science of today is the technology of tomorrow. Tomorrow is today.\" # \u6587\u7ae0\u5358\u4f4d\u3067\u30c8\u30fc\u30af\u30f3\u5316 sent_tokenize ( string ) \u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u306e\u9664\u53bb\uff08\u52a9\u8a5e\u306e\u524a\u9664) \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.stem.porter import PorterStemmer # \u5358\u8a9e\u30c8\u30fc\u30af\u30f3\u3092\u4f5c\u6210 tokenized_words = [ 'i' , 'am' , 'humbled' , 'by' , 'this' , 'traditional' , 'meeting' ] # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u4f5c\u6210 porter = PorterStemmer () # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u9069\u7528 [ porter . stem ( word ) for word in tokenized_words ] \u8a9e\u5e79\u306e\u62bd\u51fa \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.stem.porter import PorterStemmer # \u5358\u8a9e\u30c8\u30fc\u30af\u30f3\u3092\u4f5c\u6210 tokenized_words = [ 'i' , 'am' , 'humbled' , 'by' , 'this' , 'traditional' , 'meeting' ] # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u4f5c\u6210 porter = PorterStemmer () # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u9069\u7528 [ porter . stem ( word ) for word in tokenized_words ] \u54c1\u8a5e\u306e\u30bf\u30b0\u4ed8\u3051 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import nltk from nltk import pos_tag from nltk import word_tokenize # \u6700\u521d\u306e1\u56de\u306f\u4e0b\u306e\u30b3\u30e1\u30f3\u30c8\u5916\u3057\u3066\u30ea\u30bd\u30fc\u30b9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 # nltk.download('averaged_perceptron_tagger') # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 text_data = \"Chris loved outdoor running\" # \u8a13\u7df4\u6e08\u307f\u54c1\u8a5e\u30bf\u30b0\u4ed8\u3051\u5668\u3092\u9069\u7528 text_tagged = pos_tag ( word_tokenize ( text_data )) # \u54c1\u8a5e\u3092\u8868\u793a text_tagged ########## # \u54c1\u8a5e\u3092\u7528\u3044\u3066\u5358\u8a9e\u3092\u9078\u629e [ word for word , tag in text_tagged if tag in [ 'NN' , 'NNS' , 'NNP' , 'NNPS' ] ] ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.preprocessing import MultiLabelBinarizer # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 tweets = [ \"I am eating a burrito for breakfast\" , \"Political science is an amazing field\" , \"San Francisco is an awesome city\" ] # \u30ea\u30b9\u30c8\u3092\u4f5c\u6210 tagged_tweets = [] # \u30c4\u30a4\u30fc\u30c8\u4e2d\u306e\u5358\u8a9e\u306b\u30bf\u30b0\u4ed8\u3051 for tweet in tweets : tweet_tag = pos_tag ( word_tokenize ( tweet )) tagged_tweets . append ([ tag for word , tag in tweet_tag ]) # \u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c9\u3092\u7528\u3044\u3066\u3001\u30bf\u30b0\u3092\u7279\u5fb4\u91cf\u306b\u5909\u63db one_hot_multi = MultiLabelBinarizer () one_hot_multi . fit_transform ( tagged_tweets ) ########## # \u7279\u5fb4\u91cf\u540d\u3092\u8868\u793a one_hot_multi . classes_ ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.corpus import brown from nltk.tag import UnigramTagger from nltk.tag import BigramTagger from nltk.tag import TrigramTagger # \u6700\u521d\u306e1\u56de\u306f\u4e0b\u306e\u30b3\u30e1\u30f3\u30c8\u5916\u3057\u3066\u30ea\u30bd\u30fc\u30b9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 # nltk.download('brown') # Brown Corpus\u304b\u3089\u30c6\u30ad\u30b9\u30c8\u3092\u53d6\u5f97\u3057\u3066\u6587\u7ae0\u306b\u5206\u5272 sentences = brown . tagged_sents ( categories = 'news' ) # 4000\u6587\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u306b\u3001\u6b8b\u308a623\u6587\u3092\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b train = sentences [: 4000 ] test = sentences [ 4000 :] # \u30d0\u30c3\u30af\u30aa\u30d5\u4ed8\u304d\u30bf\u30b0\u4ed8\u3051\u5668\u3092\u4f5c\u6210 unigram = UnigramTagger ( train ) bigram = BigramTagger ( train , backoff = unigram ) trigram = TrigramTagger ( train , backoff = bigram ) # \u7cbe\u5ea6\u3092\u8868\u793a trigram . evaluate ( test ) BoW\u306b\u3088\u308b\u30c6\u30ad\u30b9\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.feature_extraction.text import CountVectorizer # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 text_data = np . array ([ 'I love Brazil. Brazil!' , 'Sweden is best' , 'Germany beats both' ]) # BoW\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 count = CountVectorizer () bag_of_words = count . fit_transform ( text_data ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a bag_of_words ########## bag_of_words . toarray () ########## # \u7279\u5fb4\u91cf\u540d\u3092\u8868\u793a count . get_feature_names () ########## # \u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6307\u5b9a\u3057\u3066\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 count_2gram = CountVectorizer ( ngram_range = ( 1 , 2 ), stop_words = \"english\" , vocabulary = [ 'brazil' ]) bag = count_2gram . fit_transform ( text_data ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a bag . toarray () ########## # 1-gram\u30682-gram\u3092\u8868\u793a count_2gram . vocabulary_ \u5358\u8a9e\u3078\u306e\u91cd\u307f\u4ed8\u3051\uff08TFIDF) \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 text_data = np . array ([ 'I love Brazil. Brazil!' , 'Sweden is best' , 'Germany beats both' ]) # tf-idf\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 tfidf = TfidfVectorizer () feature_matrix = tfidf . fit_transform ( text_data ) # tf-idf\u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a feature_matrix ########## # tf-idf\u7279\u5fb4\u91cf\u884c\u5217\u3092\u5bc6\u884c\u5217\u3068\u3057\u3066\u8868\u793a feature_matrix . toarray () ########## # \u7279\u5fb4\u91cf\u540d\u3092\u8868\u793a tfidf . vocabulary_ \u5168\u89d2\u30fb\u534a\u89d2\u3078\u306e\u5909\u63db \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.stem.porter import PorterStemmer # \u5358\u8a9e\u30c8\u30fc\u30af\u30f3\u3092\u4f5c\u6210 tokenized_words = [ 'i' , 'am' , 'humbled' , 'by' , 'this' , 'traditional' , 'meeting' ] # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u4f5c\u6210 porter = PorterStemmer () # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u9069\u7528 [ porter . stem ( word ) for word in tokenized_words ]","title":"NLP Basic"},{"location":"nlp_basic/#nlp-basic","text":"","title":"NLP Basic"},{"location":"nlp_basic/#_1","text":"# -*- coding: utf-8 -*- # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 text_data = [ \" Interrobang. By Aishwarya Henriette \" , \"Parking And Going. By Karl Gautier\" , \" Today Is The night. By Jarek Prakash \" ] # \u30db\u30ef\u30a4\u30c8\u30b9\u30da\u30fc\u30b9\u3092\u524a\u9664 strip_whitespace = [ string . strip () for string in text_data ] # \u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a strip_whitespace ########## # \u30d4\u30ea\u30aa\u30c9\u3092\u524a\u9664 remove_periods = [ string . replace ( \".\" , \"\" ) for string in strip_whitespace ] # \u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a remove_periods ########## # \u95a2\u6570\u3092\u5b9a\u7fa9 def capitalizer ( string : str ) -> str : return string . upper () # \u95a2\u6570\u3092\u9069\u7528 [ capitalizer ( string ) for string in remove_periods ] ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import re # \u95a2\u6570\u3092\u5b9a\u7fa9 def replace_letters_with_X ( string : str ) -> str : return re . sub ( r \"[a-zA-Z]\" , \"X\" , string ) # \u95a2\u6570\u3092\u9069\u7528 [ replace_letters_with_X ( string ) for string in remove_periods ]","title":"\u30c6\u30ad\u30b9\u30c8\u306e\u30af\u30ea\u30fc\u30cb\u30f3\u30b0(\u30d4\u30ea\u30aa\u30c9\u306e\u524a\u9664/\u5927\u6587\u5b57\u5909\u63db/\u6b63\u898f\u62bd\u51fa\u5024\u306e\u5909\u63db)"},{"location":"nlp_basic/#html","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from bs4 import BeautifulSoup # HTML\u30c6\u30ad\u30b9\u30c8\u3092\u4f5c\u6210 html = \"\"\" <div class='full_name'><span style='font-weight:bold'> Masego</span> Azra</div> \"\"\" # HTML\u3092\u30d1\u30fc\u30b9 soup = BeautifulSoup ( html , \"lxml\" ) # class\u304c\"full_name\"\u3068\u306a\u3063\u3066\u3044\u308bdiv\u3092\u898b\u3064\u3051\u3066\u3001\u305d\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a soup . find ( \"div\" , { \"class\" : \"full_name\" }) . text . strip ()","title":"HTML\u306e\u30d1\u30fc\u30b9\u3068\u30af\u30ea\u30fc\u30cb\u30f3\u30b0"},{"location":"nlp_basic/#_2","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import unicodedata import sys # \u30c6\u30ad\u30b9\u30c8\u3092\u4f5c\u6210 text_data = [ 'Hi!!!! I. Love. This. Song....' , '10000% Agree!!!! #LoveIT' , 'Right?!?!' ] # \u53e5\u8aad\u70b9\u6587\u5b57\u3092\u542b\u3080\u8f9e\u66f8\u3092\u4f5c\u6210 punctuation = dict . fromkeys ( i for i in range ( sys . maxunicode ) if unicodedata . category ( chr ( i )) . startswith ( 'P' )) # \u305d\u308c\u305e\u308c\u306e\u6587\u5b57\u5217\u304b\u3089\u3001\u53e5\u8aad\u70b9\u6587\u5b57\u3092\u3059\u3079\u3066\u9664\u53bb [ string . translate ( punctuation ) for string in text_data ]","title":"\u53e5\u8aad\u70b9\u306e\u9664\u53bb"},{"location":"nlp_basic/#_3","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.tokenize import word_tokenize # \u6700\u521d\u306e1\u56de\u306f\u4e0b\u306e\u30b3\u30e1\u30f3\u30c8\u5916\u3057\u3066\u30ea\u30bd\u30fc\u30b9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 # import nltk # nltk.download('punkt') # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 string = \"The science of today is the technology of tomorrow\" # \u5358\u8a9e\u5358\u4f4d\u3067\u30c8\u30fc\u30af\u30f3\u5316 word_tokenize ( string ) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.tokenize import sent_tokenize # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 string = \"The science of today is the technology of tomorrow. Tomorrow is today.\" # \u6587\u7ae0\u5358\u4f4d\u3067\u30c8\u30fc\u30af\u30f3\u5316 sent_tokenize ( string )","title":"\u30c6\u30ad\u30b9\u30c8\u306e\u30c8\u30fc\u30af\u30f3\u5316"},{"location":"nlp_basic/#_4","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.stem.porter import PorterStemmer # \u5358\u8a9e\u30c8\u30fc\u30af\u30f3\u3092\u4f5c\u6210 tokenized_words = [ 'i' , 'am' , 'humbled' , 'by' , 'this' , 'traditional' , 'meeting' ] # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u4f5c\u6210 porter = PorterStemmer () # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u9069\u7528 [ porter . stem ( word ) for word in tokenized_words ]","title":"\u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u306e\u9664\u53bb\uff08\u52a9\u8a5e\u306e\u524a\u9664)"},{"location":"nlp_basic/#_5","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.stem.porter import PorterStemmer # \u5358\u8a9e\u30c8\u30fc\u30af\u30f3\u3092\u4f5c\u6210 tokenized_words = [ 'i' , 'am' , 'humbled' , 'by' , 'this' , 'traditional' , 'meeting' ] # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u4f5c\u6210 porter = PorterStemmer () # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u9069\u7528 [ porter . stem ( word ) for word in tokenized_words ]","title":"\u8a9e\u5e79\u306e\u62bd\u51fa"},{"location":"nlp_basic/#_6","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import nltk from nltk import pos_tag from nltk import word_tokenize # \u6700\u521d\u306e1\u56de\u306f\u4e0b\u306e\u30b3\u30e1\u30f3\u30c8\u5916\u3057\u3066\u30ea\u30bd\u30fc\u30b9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 # nltk.download('averaged_perceptron_tagger') # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 text_data = \"Chris loved outdoor running\" # \u8a13\u7df4\u6e08\u307f\u54c1\u8a5e\u30bf\u30b0\u4ed8\u3051\u5668\u3092\u9069\u7528 text_tagged = pos_tag ( word_tokenize ( text_data )) # \u54c1\u8a5e\u3092\u8868\u793a text_tagged ########## # \u54c1\u8a5e\u3092\u7528\u3044\u3066\u5358\u8a9e\u3092\u9078\u629e [ word for word , tag in text_tagged if tag in [ 'NN' , 'NNS' , 'NNP' , 'NNPS' ] ] ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.preprocessing import MultiLabelBinarizer # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 tweets = [ \"I am eating a burrito for breakfast\" , \"Political science is an amazing field\" , \"San Francisco is an awesome city\" ] # \u30ea\u30b9\u30c8\u3092\u4f5c\u6210 tagged_tweets = [] # \u30c4\u30a4\u30fc\u30c8\u4e2d\u306e\u5358\u8a9e\u306b\u30bf\u30b0\u4ed8\u3051 for tweet in tweets : tweet_tag = pos_tag ( word_tokenize ( tweet )) tagged_tweets . append ([ tag for word , tag in tweet_tag ]) # \u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c9\u3092\u7528\u3044\u3066\u3001\u30bf\u30b0\u3092\u7279\u5fb4\u91cf\u306b\u5909\u63db one_hot_multi = MultiLabelBinarizer () one_hot_multi . fit_transform ( tagged_tweets ) ########## # \u7279\u5fb4\u91cf\u540d\u3092\u8868\u793a one_hot_multi . classes_ ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.corpus import brown from nltk.tag import UnigramTagger from nltk.tag import BigramTagger from nltk.tag import TrigramTagger # \u6700\u521d\u306e1\u56de\u306f\u4e0b\u306e\u30b3\u30e1\u30f3\u30c8\u5916\u3057\u3066\u30ea\u30bd\u30fc\u30b9\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9 # nltk.download('brown') # Brown Corpus\u304b\u3089\u30c6\u30ad\u30b9\u30c8\u3092\u53d6\u5f97\u3057\u3066\u6587\u7ae0\u306b\u5206\u5272 sentences = brown . tagged_sents ( categories = 'news' ) # 4000\u6587\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u306b\u3001\u6b8b\u308a623\u6587\u3092\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b train = sentences [: 4000 ] test = sentences [ 4000 :] # \u30d0\u30c3\u30af\u30aa\u30d5\u4ed8\u304d\u30bf\u30b0\u4ed8\u3051\u5668\u3092\u4f5c\u6210 unigram = UnigramTagger ( train ) bigram = BigramTagger ( train , backoff = unigram ) trigram = TrigramTagger ( train , backoff = bigram ) # \u7cbe\u5ea6\u3092\u8868\u793a trigram . evaluate ( test )","title":"\u54c1\u8a5e\u306e\u30bf\u30b0\u4ed8\u3051"},{"location":"nlp_basic/#bow","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.feature_extraction.text import CountVectorizer # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 text_data = np . array ([ 'I love Brazil. Brazil!' , 'Sweden is best' , 'Germany beats both' ]) # BoW\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 count = CountVectorizer () bag_of_words = count . fit_transform ( text_data ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a bag_of_words ########## bag_of_words . toarray () ########## # \u7279\u5fb4\u91cf\u540d\u3092\u8868\u793a count . get_feature_names () ########## # \u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6307\u5b9a\u3057\u3066\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 count_2gram = CountVectorizer ( ngram_range = ( 1 , 2 ), stop_words = \"english\" , vocabulary = [ 'brazil' ]) bag = count_2gram . fit_transform ( text_data ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a bag . toarray () ########## # 1-gram\u30682-gram\u3092\u8868\u793a count_2gram . vocabulary_","title":"BoW\u306b\u3088\u308b\u30c6\u30ad\u30b9\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0"},{"location":"nlp_basic/#tfidf","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.feature_extraction.text import TfidfVectorizer # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 text_data = np . array ([ 'I love Brazil. Brazil!' , 'Sweden is best' , 'Germany beats both' ]) # tf-idf\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 tfidf = TfidfVectorizer () feature_matrix = tfidf . fit_transform ( text_data ) # tf-idf\u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a feature_matrix ########## # tf-idf\u7279\u5fb4\u91cf\u884c\u5217\u3092\u5bc6\u884c\u5217\u3068\u3057\u3066\u8868\u793a feature_matrix . toarray () ########## # \u7279\u5fb4\u91cf\u540d\u3092\u8868\u793a tfidf . vocabulary_","title":"\u5358\u8a9e\u3078\u306e\u91cd\u307f\u4ed8\u3051\uff08TFIDF)"},{"location":"nlp_basic/#_7","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from nltk.stem.porter import PorterStemmer # \u5358\u8a9e\u30c8\u30fc\u30af\u30f3\u3092\u4f5c\u6210 tokenized_words = [ 'i' , 'am' , 'humbled' , 'by' , 'this' , 'traditional' , 'meeting' ] # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u4f5c\u6210 porter = PorterStemmer () # \u8a9e\u5e79\u62bd\u51fa\u5668\u3092\u9069\u7528 [ porter . stem ( word ) for word in tokenized_words ]","title":"\u5168\u89d2\u30fb\u534a\u89d2\u3078\u306e\u5909\u63db"},{"location":"nn/","text":"NN Model \u00b6 \u30e2\u30b8\u30e5\u30fc\u30eb\u8aad\u307f\u8fbc\u307f \u00b6 import numpy as np import pandas as pd import lightgbm as lgb from tqdm import tqdm from matplotlib import pyplot as plt from sklearn.preprocessing import OrdinalEncoder , StandardScaler from sklearn.model_selection import GroupKFold from sklearn.metrics import accuracy_score , log_loss import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f \u00b6 INPUT_DIR = \"../input/uec2021-exercise-2/\" OUTPUT_DIR = \"./\" df = pd . read_csv ( INPUT_DIR + \"train_student_info.csv\" ) df_test = pd . read_csv ( INPUT_DIR + \"test_student_info.csv\" ) df_all = pd . concat ([ df , df_test ], sort = False , ignore_index = True ) df_registration = pd . read_csv ( INPUT_DIR + \"student_registration.csv\" ) df_all = pd . merge ( df_all , df_registration , how = \"left\" ) df_all df_assessment = pd . read_csv ( INPUT_DIR + \"student_assessment.csv\" ) df_assessment_meta = pd . read_csv ( INPUT_DIR + \"assessments.csv\" ) df_assessment = pd . merge ( df_assessment , df_assessment_meta , how = \"left\" ) df_assessment \u30c6\u30fc\u30d6\u30eb\u51e6\u7406 \u00b6 # is_banked (sum, mean) group by (module, presentation, student) keys = [ \"code_module\" , \"code_presentation\" , \"id_student\" ] df_agg = df_assessment . groupby ( keys )[ \"is_banked\" ] . agg ([ \"sum\" , \"mean\" ]) . reset_index () df_agg . columns = keys + [ \"is_banked_sum\" , \"is_banked_mean\" ] df_all = pd . merge ( df_all , df_agg , how = \"left\" ) # is_banked (sum, mean) group by (student) keys = [ \"id_student\" ] df_agg = df_assessment . groupby ( keys )[ \"is_banked\" ] . agg ([ \"sum\" , \"mean\" ]) . reset_index () df_agg . columns = keys + [ \"is_banked_sum_student\" , \"is_banked_mean_student\" ] df_all = pd . merge ( df_all , df_agg , how = \"left\" ) # \u7de0\u5207\u4f55\u65e5\u524d\u306b\u63d0\u51fa\u3057\u305f\u304b df_assessment [ \"date_submitted_before\" ] = df_assessment [ \"date\" ] - df_assessment [ \"date_submitted\" ] # \u5e73\u5747\u63d0\u51fa\u65e5\u304b\u3089\u306e\u5dee df_assessment [ \"date_submitted_mean\" ] = df_assessment . groupby ( \"id_assessment\" )[ \"date_submitted\" ] . transform ( \"mean\" ) df_assessment [ \"date_submitted_diff\" ] = df_assessment [ \"date_submitted\" ] - df_assessment [ \"date_submitted_mean\" ] # group by (module, presentation, student) keys = [ \"code_module\" , \"code_presentation\" , \"id_student\" ] df_agg = df_assessment . groupby ( keys , as_index = False )[[ \"date_submitted_before\" , \"date_submitted_diff\" ]] . mean () df_agg . columns = keys + [ \"date_submitted_before_mean\" , \"date_submitted_diff_mean\" ] df = pd . merge ( df , df_agg , how = \"left\" ) # by student (student) keys = [ \"id_student\" ] df_agg = df_assessment . groupby ( keys , as_index = False )[[ \"date_submitted_before\" , \"date_submitted_diff\" ]] . mean () df_agg . columns = keys + [ \"date_submitted_before_mean_student\" , \"date_submitted_diff_mean_student\" ] df = pd . merge ( df , df_agg , how = \"left\" ) # num of assessment groupby (module, presentation, student) keys = [ \"code_module\" , \"code_presentation\" , \"id_student\" ] df_agg = df_assessment . groupby ( keys , as_index = False ) . size () df_agg . columns = keys + [ \"n_assessment\" ] # \u5e73\u5747assessment\u6570\u304b\u3089\u306e\u5dee df_agg [ \"n_assessment_diff\" ] = df_agg [ \"n_assessment\" ] - df_agg . groupby ([ \"code_module\" , \"code_presentation\" ])[ \"n_assessment\" ] . transform ( \"mean\" ) df_all = pd . merge ( df_all , df_agg , how = \"left\" ) df_vle = pd . read_csv ( INPUT_DIR + \"student_vle.csv\" ) df_vle_meta = pd . read_csv ( INPUT_DIR + \"vle.csv\" ) df_vle = pd . merge ( df_vle , df_vle_meta , how = \"left\" ) df_vle keys = [ \"code_module\" , \"code_presentation\" , \"id_student\" ] # num of unique site, num of day, sum of click group by (module, presentation, student) df_agg = df_vle . groupby ( keys )[[ \"id_site\" , \"sum_click\" ]] . agg ({ \"id_site\" : \"nunique\" , \"sum_click\" : [ \"count\" , \"sum\" ]}) . reset_index () df_agg . columns = keys + [ \"n_site\" , \"n_day\" , \"sum_click\" ] df_agg [ \"click_per_day\" ] = df_agg [ \"sum_click\" ] / df_agg [ \"n_day\" ] df_agg [ \"click_per_site\" ] = df_agg [ \"sum_click\" ] / df_agg [ \"n_site\" ] df_agg [ \"day_per_site\" ] = df_agg [ \"n_day\" ] / df_agg [ \"n_site\" ] # log transformation for NN for col in [ \"n_site\" , \"n_day\" , \"sum_click\" , \"click_per_day\" , \"click_per_site\" , \"day_per_site\" ]: df_agg [ col ] = np . log1p ( df_agg [ col ]) df_all = pd . merge ( df_all , df_agg , how = \"left\" ) for t in tqdm ( df_vle [ \"activity_type\" ] . unique ()): keys = [ \"code_module\" , \"code_presentation\" , \"id_student\" ] # num of unique site, num of day, sum of click group by (module, presentation, student) df_agg = df_vle . query ( \"activity_type == @t\" ) . groupby ( keys )[[ \"id_site\" , \"sum_click\" ]] . agg ({ \"id_site\" : \"nunique\" , \"sum_click\" : [ \"count\" , \"sum\" ]}) . reset_index () df_agg . columns = keys + [ \"n_site\" , \"n_day\" , \"sum_click\" ] df_agg [ \"click_per_day\" ] = df_agg [ \"sum_click\" ] / df_agg [ \"n_day\" ] df_agg [ \"click_per_site\" ] = df_agg [ \"sum_click\" ] / df_agg [ \"n_site\" ] df_agg [ \"day_per_site\" ] = df_agg [ \"n_day\" ] / df_agg [ \"n_site\" ] # log transformation for NN for col in [ \"n_site\" , \"n_day\" , \"sum_click\" , \"click_per_day\" , \"click_per_site\" , \"day_per_site\" ]: df_agg [ col ] = np . log1p ( df_agg [ col ]) df_all = pd . merge ( df_all , df_agg , how = \"left\" ) df_all = pd . merge ( df_all , df_agg , on = keys , how = \"left\" , suffixes = [ \"\" , f \"_ { t } \" ]) \u524d\u51e6\u7406 \u00b6 df_all [ \"gender\" ] = ( df_all [ \"gender\" ] == \"M\" ) . astype ( int ) df_all [ \"disability\" ] = ( df_all [ \"disability\" ] == \"Y\" ) . astype ( int ) df_all [ \"imd_band\" ] = df_all [ \"imd_band\" ] . str [ 0 ] . astype ( float ) df_all [ \"age_band\" ] = df_all [ \"age_band\" ] . str [ 0 ] . astype ( float ) cat_vars = [ \"code_module\" , \"code_presentation\" , \"region\" , \"highest_education\" ] enc = OrdinalEncoder ( dtype = int ) df_all [ cat_vars ] = enc . fit_transform ( df_all [ cat_vars ] . fillna ( \"\" )) fig , axes = plt . subplots ( figsize = ( 15 , 80 ), nrows = 35 , ncols = 4 ) for i , col in enumerate ( df_all . columns . drop ([ \"id\" , \"id_student\" , \"final_result\" ] + cat_vars )): ax = axes [ i // 4 , i % 4 ] ax . hist ( df_all [ col ], bins = 100 ) ax . set_title ( col ) fig . tight_layout () plt . show () df = df_all . iloc [: len ( df ), :] . reset_index ( drop = True ) df_test = df_all . iloc [ len ( df ):, :] . reset_index ( drop = True ) result_map = { \"Withdrawn\" : 0 , \"Fail\" : 1 , \"Pass\" : 2 , \"Distinction\" : 3 } result_inv_map = { 0 : \"Withdrawn\" , 1 : \"Fail\" , 2 : \"Pass\" , 3 : \"Distinction\" } \u30e2\u30c7\u30ea\u30f3\u30b0 \u00b6 class MyDataset ( Dataset ): def __init__ ( self , X_num , X_cat , y = None ): self . X_num = torch . FloatTensor ( X_num ) self . X_cat = torch . LongTensor ( X_cat ) if y is not None : self . y = torch . LongTensor ( y ) def __len__ ( self ): return self . X_num . shape [ 0 ] def __getitem__ ( self , idx ): if \"y\" in dir ( self ): return ( self . X_num [ idx , :], self . X_cat [ idx , :], self . y [ idx ]) else : return ( self . X_num [ idx , :], self . X_cat [ idx , :]) class NNModel ( nn . Module ): def __init__ ( self , input_size_num , output_size , n_categories , emb_size , hidden_sizes , dropout ): super () . __init__ () self . embs = nn . ModuleList () for i in range ( len ( n_categories )): self . embs . append ( nn . Embedding ( n_categories [ i ], emb_size )) input_size = input_size_num + sum ( emb . embedding_dim for emb in self . embs ) self . mlp = nn . Sequential ( nn . Linear ( input_size , hidden_sizes [ 0 ]), nn . BatchNorm1d ( hidden_sizes [ 0 ]), nn . Dropout ( dropout ), nn . ReLU (), nn . Linear ( hidden_sizes [ 0 ], hidden_sizes [ 1 ]), nn . BatchNorm1d ( hidden_sizes [ 1 ]), nn . Dropout ( dropout ), nn . ReLU (), nn . Linear ( hidden_sizes [ 1 ], output_size ), ) def forward ( self , x_num , x_cat ): x_cat = [ emb ( x_cat [:, i ]) for i , emb in enumerate ( self . embs )] x = torch . cat ([ x_num ] + x_cat , axis = 1 ) x = self . mlp ( x ) return x def train ( model , data_loader , optimizer , criterion , device ): model . train () for batch in data_loader : X_num = batch [ 0 ] . to ( device ) X_cat = batch [ 1 ] . to ( device ) y = batch [ 2 ] . to ( device ) preds = model ( X_num , X_cat ) loss = criterion ( preds , y ) optimizer . zero_grad () loss . backward () optimizer . step () def evaluate ( model , data_loader , criterion , device ): model . eval () n = 0 total_loss = 0.0 for batch in data_loader : X_num = batch [ 0 ] . to ( device ) X_cat = batch [ 1 ] . to ( device ) y = batch [ 2 ] . to ( device ) with torch . no_grad (): preds = model ( X_num , X_cat ) loss = criterion ( preds , y ) total_loss += loss . item () n += X_num . shape [ 0 ] avg_loss = total_loss / n return avg_loss def predict ( model , data_loader , device ): model . eval () preds_all = [] for batch in data_loader : X_num = batch [ 0 ] . to ( device ) X_cat = batch [ 1 ] . to ( device ) with torch . no_grad (): preds = model ( X_num , X_cat ) preds = torch . softmax ( preds , axis = 1 ) preds = preds . cpu () . numpy () preds_all . append ( preds ) preds_all = np . concatenate ( preds_all ) return preds_all n_categories = ( df_all [ cat_vars ] . max ( axis = 0 ) + 1 ) . tolist () %% time X = df . drop ([ \"id\" , \"id_student\" , \"final_result\" ], axis = 1 ) y = df [ \"final_result\" ] . map ( result_map ) groups = df [ \"id_student\" ] . values n_splits = 5 batch_size = 256 preds_valid = np . zeros (( len ( df ), 4 )) preds_test = np . zeros (( len ( df_test ), 4 )) gkf = GroupKFold ( n_splits = n_splits ) for i , ( idx_train , idx_valid ) in enumerate ( gkf . split ( X , y , groups = groups )): print ( f \"fold: { i } \" ) X_train_num = X . iloc [ idx_train , :] . drop ( cat_vars , axis = 1 ) . fillna ( 0 ) X_train_cat = X . iloc [ idx_train , :][ cat_vars ] y_train = y . iloc [ idx_train ] X_valid_num = X . iloc [ idx_valid , :] . drop ( cat_vars , axis = 1 ) . fillna ( 0 ) X_valid_cat = X . iloc [ idx_valid , :][ cat_vars ] y_valid = y . iloc [ idx_valid ] X_test_num = df_test [ X . columns ] . drop ( cat_vars , axis = 1 ) . fillna ( 0 ) X_test_cat = df_test [ cat_vars ] # standardization scaler = StandardScaler () scaler . fit ( X_train_num ) X_train_num = scaler . transform ( X_train_num ) X_valid_num = scaler . transform ( X_valid_num ) X_test_num = scaler . transform ( X_test_num ) # dataset ds_train = MyDataset ( X_train_num , X_train_cat . values , y_train . values ) ds_valid = MyDataset ( X_valid_num , X_valid_cat . values , y_valid . values ) ds_test = MyDataset ( X_test_num , X_test_cat . values ) # dataloader dl_train = DataLoader ( ds_train , batch_size = batch_size , shuffle = True , num_workers = 0 , pin_memory = True , drop_last = True ) dl_valid = DataLoader ( ds_valid , batch_size = batch_size , shuffle = False , num_workers = 0 , pin_memory = True , drop_last = False ) dl_test = DataLoader ( ds_test , batch_size = batch_size , shuffle = False , num_workers = 0 , pin_memory = True , drop_last = False ) # build model torch . manual_seed ( 0 ) device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model = NNModel ( input_size_num = X_train_num . shape [ 1 ], output_size = 4 , n_categories = n_categories , emb_size = 10 , hidden_sizes = ( 128 , 64 ), dropout = 0.5 ) model . to ( device ) optimizer = optim . Adam ( model . parameters (), lr = 5e-4 ) criterion = nn . CrossEntropyLoss ( reduction = \"sum\" ) best_loss = np . inf for epoch in range ( 50 ): train ( model , dl_train , optimizer , criterion , device ) loss = evaluate ( model , dl_valid , criterion , device ) if loss < best_loss : best_loss = loss torch . save ( model . state_dict (), f \"model.pth\" ) print ( f \"epoch: { epoch } \\t valid-loss: { loss } \\t best!\" ) else : print ( f \"epoch: { epoch } \\t valid-loss: { loss } \" ) with torch . no_grad (): model . load_state_dict ( torch . load ( f \"model.pth\" )) preds_valid [ idx_valid ] = predict ( model , dl_valid , device ) preds_test += predict ( model , dl_test , device ) / n_splits print () logloss = log_loss ( y , preds_valid ) acc = accuracy_score ( y , np . argmax ( preds_valid , axis = 1 )) print ( f \"logloss: { logloss : .5f } \\t acc: { acc : .5f } \" ) \u63a8\u8ad6 \u00b6 submission = pd . read_csv ( INPUT_DIR + \"sample_submission.csv\" ) submission [ \"pred\" ] = pd . Series ( np . argmax ( preds_test , axis = 1 )) . map ( result_inv_map ) submission . to_csv ( OUTPUT_DIR + \"nn.csv\" , index = False ) submission [ \"pred\" ] . value_counts ()","title":"NN"},{"location":"nn/#nn-model","text":"","title":"NN Model"},{"location":"nn/#_1","text":"import numpy as np import pandas as pd import lightgbm as lgb from tqdm import tqdm from matplotlib import pyplot as plt from sklearn.preprocessing import OrdinalEncoder , StandardScaler from sklearn.model_selection import GroupKFold from sklearn.metrics import accuracy_score , log_loss import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torch.utils.data import Dataset , DataLoader","title":"\u30e2\u30b8\u30e5\u30fc\u30eb\u8aad\u307f\u8fbc\u307f"},{"location":"nn/#_2","text":"INPUT_DIR = \"../input/uec2021-exercise-2/\" OUTPUT_DIR = \"./\" df = pd . read_csv ( INPUT_DIR + \"train_student_info.csv\" ) df_test = pd . read_csv ( INPUT_DIR + \"test_student_info.csv\" ) df_all = pd . concat ([ df , df_test ], sort = False , ignore_index = True ) df_registration = pd . read_csv ( INPUT_DIR + \"student_registration.csv\" ) df_all = pd . merge ( df_all , df_registration , how = \"left\" ) df_all df_assessment = pd . read_csv ( INPUT_DIR + \"student_assessment.csv\" ) df_assessment_meta = pd . read_csv ( INPUT_DIR + \"assessments.csv\" ) df_assessment = pd . merge ( df_assessment , df_assessment_meta , how = \"left\" ) df_assessment","title":"\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f"},{"location":"nn/#_3","text":"# is_banked (sum, mean) group by (module, presentation, student) keys = [ \"code_module\" , \"code_presentation\" , \"id_student\" ] df_agg = df_assessment . groupby ( keys )[ \"is_banked\" ] . agg ([ \"sum\" , \"mean\" ]) . reset_index () df_agg . columns = keys + [ \"is_banked_sum\" , \"is_banked_mean\" ] df_all = pd . merge ( df_all , df_agg , how = \"left\" ) # is_banked (sum, mean) group by (student) keys = [ \"id_student\" ] df_agg = df_assessment . groupby ( keys )[ \"is_banked\" ] . agg ([ \"sum\" , \"mean\" ]) . reset_index () df_agg . columns = keys + [ \"is_banked_sum_student\" , \"is_banked_mean_student\" ] df_all = pd . merge ( df_all , df_agg , how = \"left\" ) # \u7de0\u5207\u4f55\u65e5\u524d\u306b\u63d0\u51fa\u3057\u305f\u304b df_assessment [ \"date_submitted_before\" ] = df_assessment [ \"date\" ] - df_assessment [ \"date_submitted\" ] # \u5e73\u5747\u63d0\u51fa\u65e5\u304b\u3089\u306e\u5dee df_assessment [ \"date_submitted_mean\" ] = df_assessment . groupby ( \"id_assessment\" )[ \"date_submitted\" ] . transform ( \"mean\" ) df_assessment [ \"date_submitted_diff\" ] = df_assessment [ \"date_submitted\" ] - df_assessment [ \"date_submitted_mean\" ] # group by (module, presentation, student) keys = [ \"code_module\" , \"code_presentation\" , \"id_student\" ] df_agg = df_assessment . groupby ( keys , as_index = False )[[ \"date_submitted_before\" , \"date_submitted_diff\" ]] . mean () df_agg . columns = keys + [ \"date_submitted_before_mean\" , \"date_submitted_diff_mean\" ] df = pd . merge ( df , df_agg , how = \"left\" ) # by student (student) keys = [ \"id_student\" ] df_agg = df_assessment . groupby ( keys , as_index = False )[[ \"date_submitted_before\" , \"date_submitted_diff\" ]] . mean () df_agg . columns = keys + [ \"date_submitted_before_mean_student\" , \"date_submitted_diff_mean_student\" ] df = pd . merge ( df , df_agg , how = \"left\" ) # num of assessment groupby (module, presentation, student) keys = [ \"code_module\" , \"code_presentation\" , \"id_student\" ] df_agg = df_assessment . groupby ( keys , as_index = False ) . size () df_agg . columns = keys + [ \"n_assessment\" ] # \u5e73\u5747assessment\u6570\u304b\u3089\u306e\u5dee df_agg [ \"n_assessment_diff\" ] = df_agg [ \"n_assessment\" ] - df_agg . groupby ([ \"code_module\" , \"code_presentation\" ])[ \"n_assessment\" ] . transform ( \"mean\" ) df_all = pd . merge ( df_all , df_agg , how = \"left\" ) df_vle = pd . read_csv ( INPUT_DIR + \"student_vle.csv\" ) df_vle_meta = pd . read_csv ( INPUT_DIR + \"vle.csv\" ) df_vle = pd . merge ( df_vle , df_vle_meta , how = \"left\" ) df_vle keys = [ \"code_module\" , \"code_presentation\" , \"id_student\" ] # num of unique site, num of day, sum of click group by (module, presentation, student) df_agg = df_vle . groupby ( keys )[[ \"id_site\" , \"sum_click\" ]] . agg ({ \"id_site\" : \"nunique\" , \"sum_click\" : [ \"count\" , \"sum\" ]}) . reset_index () df_agg . columns = keys + [ \"n_site\" , \"n_day\" , \"sum_click\" ] df_agg [ \"click_per_day\" ] = df_agg [ \"sum_click\" ] / df_agg [ \"n_day\" ] df_agg [ \"click_per_site\" ] = df_agg [ \"sum_click\" ] / df_agg [ \"n_site\" ] df_agg [ \"day_per_site\" ] = df_agg [ \"n_day\" ] / df_agg [ \"n_site\" ] # log transformation for NN for col in [ \"n_site\" , \"n_day\" , \"sum_click\" , \"click_per_day\" , \"click_per_site\" , \"day_per_site\" ]: df_agg [ col ] = np . log1p ( df_agg [ col ]) df_all = pd . merge ( df_all , df_agg , how = \"left\" ) for t in tqdm ( df_vle [ \"activity_type\" ] . unique ()): keys = [ \"code_module\" , \"code_presentation\" , \"id_student\" ] # num of unique site, num of day, sum of click group by (module, presentation, student) df_agg = df_vle . query ( \"activity_type == @t\" ) . groupby ( keys )[[ \"id_site\" , \"sum_click\" ]] . agg ({ \"id_site\" : \"nunique\" , \"sum_click\" : [ \"count\" , \"sum\" ]}) . reset_index () df_agg . columns = keys + [ \"n_site\" , \"n_day\" , \"sum_click\" ] df_agg [ \"click_per_day\" ] = df_agg [ \"sum_click\" ] / df_agg [ \"n_day\" ] df_agg [ \"click_per_site\" ] = df_agg [ \"sum_click\" ] / df_agg [ \"n_site\" ] df_agg [ \"day_per_site\" ] = df_agg [ \"n_day\" ] / df_agg [ \"n_site\" ] # log transformation for NN for col in [ \"n_site\" , \"n_day\" , \"sum_click\" , \"click_per_day\" , \"click_per_site\" , \"day_per_site\" ]: df_agg [ col ] = np . log1p ( df_agg [ col ]) df_all = pd . merge ( df_all , df_agg , how = \"left\" ) df_all = pd . merge ( df_all , df_agg , on = keys , how = \"left\" , suffixes = [ \"\" , f \"_ { t } \" ])","title":"\u30c6\u30fc\u30d6\u30eb\u51e6\u7406"},{"location":"nn/#_4","text":"df_all [ \"gender\" ] = ( df_all [ \"gender\" ] == \"M\" ) . astype ( int ) df_all [ \"disability\" ] = ( df_all [ \"disability\" ] == \"Y\" ) . astype ( int ) df_all [ \"imd_band\" ] = df_all [ \"imd_band\" ] . str [ 0 ] . astype ( float ) df_all [ \"age_band\" ] = df_all [ \"age_band\" ] . str [ 0 ] . astype ( float ) cat_vars = [ \"code_module\" , \"code_presentation\" , \"region\" , \"highest_education\" ] enc = OrdinalEncoder ( dtype = int ) df_all [ cat_vars ] = enc . fit_transform ( df_all [ cat_vars ] . fillna ( \"\" )) fig , axes = plt . subplots ( figsize = ( 15 , 80 ), nrows = 35 , ncols = 4 ) for i , col in enumerate ( df_all . columns . drop ([ \"id\" , \"id_student\" , \"final_result\" ] + cat_vars )): ax = axes [ i // 4 , i % 4 ] ax . hist ( df_all [ col ], bins = 100 ) ax . set_title ( col ) fig . tight_layout () plt . show () df = df_all . iloc [: len ( df ), :] . reset_index ( drop = True ) df_test = df_all . iloc [ len ( df ):, :] . reset_index ( drop = True ) result_map = { \"Withdrawn\" : 0 , \"Fail\" : 1 , \"Pass\" : 2 , \"Distinction\" : 3 } result_inv_map = { 0 : \"Withdrawn\" , 1 : \"Fail\" , 2 : \"Pass\" , 3 : \"Distinction\" }","title":"\u524d\u51e6\u7406"},{"location":"nn/#_5","text":"class MyDataset ( Dataset ): def __init__ ( self , X_num , X_cat , y = None ): self . X_num = torch . FloatTensor ( X_num ) self . X_cat = torch . LongTensor ( X_cat ) if y is not None : self . y = torch . LongTensor ( y ) def __len__ ( self ): return self . X_num . shape [ 0 ] def __getitem__ ( self , idx ): if \"y\" in dir ( self ): return ( self . X_num [ idx , :], self . X_cat [ idx , :], self . y [ idx ]) else : return ( self . X_num [ idx , :], self . X_cat [ idx , :]) class NNModel ( nn . Module ): def __init__ ( self , input_size_num , output_size , n_categories , emb_size , hidden_sizes , dropout ): super () . __init__ () self . embs = nn . ModuleList () for i in range ( len ( n_categories )): self . embs . append ( nn . Embedding ( n_categories [ i ], emb_size )) input_size = input_size_num + sum ( emb . embedding_dim for emb in self . embs ) self . mlp = nn . Sequential ( nn . Linear ( input_size , hidden_sizes [ 0 ]), nn . BatchNorm1d ( hidden_sizes [ 0 ]), nn . Dropout ( dropout ), nn . ReLU (), nn . Linear ( hidden_sizes [ 0 ], hidden_sizes [ 1 ]), nn . BatchNorm1d ( hidden_sizes [ 1 ]), nn . Dropout ( dropout ), nn . ReLU (), nn . Linear ( hidden_sizes [ 1 ], output_size ), ) def forward ( self , x_num , x_cat ): x_cat = [ emb ( x_cat [:, i ]) for i , emb in enumerate ( self . embs )] x = torch . cat ([ x_num ] + x_cat , axis = 1 ) x = self . mlp ( x ) return x def train ( model , data_loader , optimizer , criterion , device ): model . train () for batch in data_loader : X_num = batch [ 0 ] . to ( device ) X_cat = batch [ 1 ] . to ( device ) y = batch [ 2 ] . to ( device ) preds = model ( X_num , X_cat ) loss = criterion ( preds , y ) optimizer . zero_grad () loss . backward () optimizer . step () def evaluate ( model , data_loader , criterion , device ): model . eval () n = 0 total_loss = 0.0 for batch in data_loader : X_num = batch [ 0 ] . to ( device ) X_cat = batch [ 1 ] . to ( device ) y = batch [ 2 ] . to ( device ) with torch . no_grad (): preds = model ( X_num , X_cat ) loss = criterion ( preds , y ) total_loss += loss . item () n += X_num . shape [ 0 ] avg_loss = total_loss / n return avg_loss def predict ( model , data_loader , device ): model . eval () preds_all = [] for batch in data_loader : X_num = batch [ 0 ] . to ( device ) X_cat = batch [ 1 ] . to ( device ) with torch . no_grad (): preds = model ( X_num , X_cat ) preds = torch . softmax ( preds , axis = 1 ) preds = preds . cpu () . numpy () preds_all . append ( preds ) preds_all = np . concatenate ( preds_all ) return preds_all n_categories = ( df_all [ cat_vars ] . max ( axis = 0 ) + 1 ) . tolist () %% time X = df . drop ([ \"id\" , \"id_student\" , \"final_result\" ], axis = 1 ) y = df [ \"final_result\" ] . map ( result_map ) groups = df [ \"id_student\" ] . values n_splits = 5 batch_size = 256 preds_valid = np . zeros (( len ( df ), 4 )) preds_test = np . zeros (( len ( df_test ), 4 )) gkf = GroupKFold ( n_splits = n_splits ) for i , ( idx_train , idx_valid ) in enumerate ( gkf . split ( X , y , groups = groups )): print ( f \"fold: { i } \" ) X_train_num = X . iloc [ idx_train , :] . drop ( cat_vars , axis = 1 ) . fillna ( 0 ) X_train_cat = X . iloc [ idx_train , :][ cat_vars ] y_train = y . iloc [ idx_train ] X_valid_num = X . iloc [ idx_valid , :] . drop ( cat_vars , axis = 1 ) . fillna ( 0 ) X_valid_cat = X . iloc [ idx_valid , :][ cat_vars ] y_valid = y . iloc [ idx_valid ] X_test_num = df_test [ X . columns ] . drop ( cat_vars , axis = 1 ) . fillna ( 0 ) X_test_cat = df_test [ cat_vars ] # standardization scaler = StandardScaler () scaler . fit ( X_train_num ) X_train_num = scaler . transform ( X_train_num ) X_valid_num = scaler . transform ( X_valid_num ) X_test_num = scaler . transform ( X_test_num ) # dataset ds_train = MyDataset ( X_train_num , X_train_cat . values , y_train . values ) ds_valid = MyDataset ( X_valid_num , X_valid_cat . values , y_valid . values ) ds_test = MyDataset ( X_test_num , X_test_cat . values ) # dataloader dl_train = DataLoader ( ds_train , batch_size = batch_size , shuffle = True , num_workers = 0 , pin_memory = True , drop_last = True ) dl_valid = DataLoader ( ds_valid , batch_size = batch_size , shuffle = False , num_workers = 0 , pin_memory = True , drop_last = False ) dl_test = DataLoader ( ds_test , batch_size = batch_size , shuffle = False , num_workers = 0 , pin_memory = True , drop_last = False ) # build model torch . manual_seed ( 0 ) device = torch . device ( \"cuda:0\" if torch . cuda . is_available () else \"cpu\" ) model = NNModel ( input_size_num = X_train_num . shape [ 1 ], output_size = 4 , n_categories = n_categories , emb_size = 10 , hidden_sizes = ( 128 , 64 ), dropout = 0.5 ) model . to ( device ) optimizer = optim . Adam ( model . parameters (), lr = 5e-4 ) criterion = nn . CrossEntropyLoss ( reduction = \"sum\" ) best_loss = np . inf for epoch in range ( 50 ): train ( model , dl_train , optimizer , criterion , device ) loss = evaluate ( model , dl_valid , criterion , device ) if loss < best_loss : best_loss = loss torch . save ( model . state_dict (), f \"model.pth\" ) print ( f \"epoch: { epoch } \\t valid-loss: { loss } \\t best!\" ) else : print ( f \"epoch: { epoch } \\t valid-loss: { loss } \" ) with torch . no_grad (): model . load_state_dict ( torch . load ( f \"model.pth\" )) preds_valid [ idx_valid ] = predict ( model , dl_valid , device ) preds_test += predict ( model , dl_test , device ) / n_splits print () logloss = log_loss ( y , preds_valid ) acc = accuracy_score ( y , np . argmax ( preds_valid , axis = 1 )) print ( f \"logloss: { logloss : .5f } \\t acc: { acc : .5f } \" )","title":"\u30e2\u30c7\u30ea\u30f3\u30b0"},{"location":"nn/#_6","text":"submission = pd . read_csv ( INPUT_DIR + \"sample_submission.csv\" ) submission [ \"pred\" ] = pd . Series ( np . argmax ( preds_test , axis = 1 )) . map ( result_inv_map ) submission . to_csv ( OUTPUT_DIR + \"nn.csv\" , index = False ) submission [ \"pred\" ] . value_counts ()","title":"\u63a8\u8ad6"},{"location":"numpy_basic/","text":"Numpy Basic \u00b6 1\u6b21\u5143\u914d\u5217\u306e\u4f5c\u6210 \u00b6 # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u3068\u3057\u3066\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 vector_row = np . array ([ 1 , 2 , 3 ]) # \u5217\u3068\u3057\u3066\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 vector_column = np . array ([[ 1 ], [ 2 ], [ 3 ]]) \uff12\u6b21\u5143\u914d\u5217\u306e\u4f5c\u6210 \u00b6 # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 ], [ 1 , 2 ], [ 1 , 2 ]]) ########## matrix_object = np . mat ([[ 1 , 2 ], [ 1 , 2 ], [ 1 , 2 ]]) \u758e\u884c\u5217\u306e\u4f5c\u6210(\u975e\u30bc\u30ed\u884c\u5217\uff09 \u00b6 # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from scipy import sparse # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 0 , 0 ], [ 0 , 1 ], [ 3 , 0 ]]) # CSR(compressed sparse row) \u5f62\u5f0f\u306e\u884c\u5217\u3092\u4f5c\u6210 matrix_sparse = sparse . csr_matrix ( matrix ) # \u758e\u884c\u5217\u3092\u8868\u793a print ( matrix_sparse ) \u884c\u5217\u306e\u30b5\u30a4\u30ba/\u8981\u7d20\u6570/\u6b21\u5143\u306e\u53d6\u5f97 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 , 4 ], [ 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 ]]) # \u884c\u6570\u3001\u5217\u6570\u3092\u8868\u793a matrix . shape ########## # \u8981\u7d20\u6570\uff08\u884c\u6570 * \u5217\u6570\uff09\u3092\u8868\u793a matrix . size ########## # \u6b21\u5143\u6570\u3092\u8868\u793a matrix . ndim \u914d\u5217\u8981\u7d20\u3078\u306e\u56de\u6570\u306e\u9069\u7528 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) # \u5f15\u6570\u306b100\u3092\u52a0\u3048\u308b\u95a2\u6570\u3092\u4f5c\u6210 add_100 = lambda i : i + 100 # \u30d9\u30af\u30c8\u30eb\u5316\u3055\u308c\u305f\u95a2\u6570\u3092\u4f5c\u6210 vectorized_add_100 = np . vectorize ( add_100 ) # \u3053\u306e\u95a2\u6570\u3092matrix\u306e\u3059\u3079\u3066\u306e\u8981\u7d20\u306b\u9069\u7528 vectorized_add_100 ( matrix ) \u884c\u5217\u5185\u306e\u6700\u5927/\u6700\u5c0f\u8981\u7d20\u306e\u53d6\u5f97 \u00b6 # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) # \u6700\u5927\u306e\u8981\u7d20\u3092\u8fd4\u3059 # Return maximum element np . max ( matrix ) ########## # \u6700\u5c0f\u306e\u8981\u7d20\u3092\u8fd4\u3059 np . min ( matrix ) ########## # \u5404\u5217\u306b\u304a\u3051\u308b\u6700\u5927\u8981\u7d20\u3092\u898b\u3064\u3051\u308b np . max ( matrix , axis = 0 ) ########## # \u5404\u884c\u306b\u304a\u3051\u308b\u6700\u5927\u8981\u7d20\u3092\u898b\u3064\u3051\u308b np . max ( matrix , axis = 1 ) \u884c\u5217\u5185\u306e\u5e73\u5747/\u5206\u6563/\u6a19\u6e96\u504f\u5dee\u306e\u53d6\u5f97 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) # \u5e73\u5747\u5024\u3092\u7b97\u51fa np . mean ( matrix ) # \u5206\u6563\u3092\u7b97\u51fa np . var ( matrix ) # \u6a19\u6e96\u504f\u5dee\u3092\u7b97\u51fa np . std ( matrix ) ########### # \u5404\u5217\u306e\u5e73\u5747\u5024\u3092\u7b97\u51fa np . mean ( matrix , axis = 0 ) \u884c\u5217\u5f62\u72b6\u306e\u5909\u66f4 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # 4x3\u306e\u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]) # 2x6\u306e\u884c\u5217\u306b\u5f62\u72b6\u5909\u66f4 matrix . reshape ( 2 , 6 ) matrix . size ########## matrix . reshape ( 1 , - 1 ) ########## matrix . reshape ( 12 ) \u8ee2\u7f6e\u884c\u5217\u306e\u4f5c\u6210 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) # \u884c\u5217\u3092\u8ee2\u7f6e matrix . T ############ # \u30d9\u30af\u30c8\u30eb\u306e\u8ee2\u7f6e np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) . T ############ # \u884c\u30d9\u30af\u30c8\u30eb\u306e\u8ee2\u7f6e np . array ([[ 1 , 2 , 3 , 4 , 5 , 6 ]]) . T \u884c\u5217\u306e\u30d5\u30e9\u30c3\u30c8\u5316\uff081\u6b21\u5143\u306b\u843d\u3068\u3059\uff09 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) # \u884c\u5217\u3092\u30d5\u30e9\u30c3\u30c8\u5316 matrix . flatten () ########### matrix . reshape ( 1 , - 1 ) \u884c\u5217\u30e9\u30f3\u30af\u306e\u53d6\u5f97 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 1 , 1 ], [ 1 , 1 , 10 ], [ 1 , 1 , 15 ]]) # \u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u8fd4\u3059 np . linalg . matrix_rank ( matrix ) \u884c\u5217\u5f0f\u306e\u8a08\u7b97(det\u306e\u8a08\u7b97\uff09 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 2 , 4 , 6 ], [ 3 , 8 , 9 ]]) # \u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u7b97\u51fa np . linalg . det ( matrix ) \u884c\u5217\u5bfe\u89d2\u8981\u7d20\u306e\u53d6\u5f97 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 2 , 4 , 6 ], [ 3 , 8 , 9 ]]) # \u5bfe\u89d2\u8981\u7d20\u3092\u8fd4\u3059 matrix . diagonal () ########## # \u4e3b\u5bfe\u89d2\u8981\u7d20\u306e\u4e00\u3064\u4e0a\u306e\u526f\u5bfe\u89d2\u8981\u7d20\u3092\u8fd4\u3059 matrix . diagonal ( offset = 1 ) ########## # \u4e3b\u5bfe\u89d2\u8981\u7d20\u306e\u4e00\u3064\u4e0b\u306e\u526f\u5bfe\u89d2\u8981\u7d20\u3092\u8fd4\u3059 matrix . diagonal ( offset =- 1 ) \u884c\u5217\u30c8\u30ec\u30fc\u30b9\u306e\u8a08\u7b97(\u5bfe\u89d2\u8981\u7d20\u306e\u7dcf\u548c\uff09 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 2 , 4 , 6 ], [ 3 , 8 , 9 ]]) # \u30c8\u30ec\u30fc\u30b9\u3092\u8fd4\u3059 matrix . trace () ########## # \u5bfe\u89d2\u8981\u7d20\u3092\u8db3\u3057\u5408\u308f\u305b\u305f\u3082\u306e\u3092\u8fd4\u3059 sum ( matrix . diagonal ()) \u56fa\u6709\u5024/\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u53d6\u5f97 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , - 1 , 3 ], [ 1 , 1 , 6 ], [ 3 , 8 , 9 ]]) # \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97 eigenvalues , eigenvectors = np . linalg . eig ( matrix ) # \u56fa\u6709\u5024\u3092\u8868\u793a eigenvalues # \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a eigenvectors \u5185\u7a4d\u306e\u8a08\u7b97 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # 2\u3064\u306e\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 vector_a = np . array ([ 1 , 2 , 3 ]) vector_b = np . array ([ 4 , 5 , 6 ]) # \u5185\u7a4d\u3092\u8a08\u7b97 np . dot ( vector_a , vector_b ) ########## # \u5185\u7a4d\u3092\u8a08\u7b97 vector_a @ vector_b \u884c\u5217\u306e\u52a0\u7b97/\u6e1b\u7b97 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix_a = np . array ([[ 1 , 1 , 1 ], [ 1 , 1 , 1 ], [ 1 , 1 , 2 ]]) # \u884c\u5217\u3092\u4f5c\u6210 matrix_b = np . array ([[ 1 , 3 , 1 ], [ 1 , 3 , 1 ], [ 1 , 3 , 8 ]]) # 2\u3064\u306e\u884c\u5217\u3092\u52a0\u7b97 np . add ( matrix_a , matrix_b ) ########## # 2\u3064\u306e\u884c\u5217\u306e\u6e1b\u7b97 np . subtract ( matrix_a , matrix_b ) ########## # 2\u3064\u306e\u884c\u5217\u306e\u52a0\u7b97 matrix_a + matrix_b \u884c\u5217\u306e\u4e57\u7b97 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix_a = np . array ([[ 1 , 1 ], [ 1 , 2 ]]) # \u884c\u5217\u3092\u4f5c\u6210 matrix_b = np . array ([[ 1 , 3 ], [ 1 , 2 ]]) # 2\u3064\u306e\u884c\u5217\u3092\u4e57\u7b97 np . dot ( matrix_a , matrix_b ) ########## # 2\u3064\u306e\u884c\u5217\u3092\u4e57\u7b97 matrix_a @ matrix_b ########## # 2\u3064\u306e\u884c\u5217\u306e\u8981\u7d20\u3054\u3068\u306e\u4e57\u7b97 matrix_a * matrix_b \u9006\u884c\u5217\u306e\u7b97\u51fa \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 4 ], [ 2 , 5 ]]) # \u9006\u884c\u5217\u3092\u7b97\u51fa # Calculate inverse of matrix np . linalg . inv ( matrix ) ########## # \u884c\u5217\u3068\u305d\u306e\u9006\u884c\u5217\u3092\u4e57\u7b97 matrix @ np . linalg . inv ( matrix ) \u4e71\u6570\u306e\u751f\u6210 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u30b7\u30fc\u30c9\u5024\u3092\u8a2d\u5b9a np . random . seed ( 0 ) # 0.0\u30681.0\u306e\u9593\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u6d6e\u52d5\u5c0f\u6570\u70b9\u5024\u30923\u3064\u751f\u6210\u3059\u308b\u3002 np . random . random ( 3 ) ########## # 1\u306810\u306e\u9593\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u6574\u6570\u5024\u30923\u3064\u751f\u6210 np . random . randint ( 0 , 11 , 3 ) ########## # \u5e73\u57470.0\u3001\u6a19\u6e96\u504f\u5dee1.0\u306e\u6b63\u898f\u5206\u5e03\u304b\u30893\u3064\u306e\u6570\u5024\u3092\u751f\u6210\u3059\u308b\u3002 np . random . normal ( 0.0 , 1.0 , 3 ) ########## # \u5e73\u57470.0\u3001\u30b9\u30b1\u30fc\u30eb1.0\u306e\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u5206\u5e03\u304b\u30893\u3064\u306e\u6570\u5024\u3092\u751f\u6210 np . random . logistic ( 0.0 , 1.0 , 3 ) ########## # 1.0\u4ee5\u4e0a\u3067\u30012.0\u3088\u308a\u5c0f\u3055\u30443\u3064\u306e\u6570\u5024\u3092\u751f\u6210 np . random . uniform ( 1.0 , 2.0 , 3 )","title":"Numpy Basic"},{"location":"numpy_basic/#numpy-basic","text":"","title":"Numpy Basic"},{"location":"numpy_basic/#1","text":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u3068\u3057\u3066\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 vector_row = np . array ([ 1 , 2 , 3 ]) # \u5217\u3068\u3057\u3066\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 vector_column = np . array ([[ 1 ], [ 2 ], [ 3 ]])","title":"1\u6b21\u5143\u914d\u5217\u306e\u4f5c\u6210"},{"location":"numpy_basic/#2","text":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 ], [ 1 , 2 ], [ 1 , 2 ]]) ########## matrix_object = np . mat ([[ 1 , 2 ], [ 1 , 2 ], [ 1 , 2 ]])","title":"\uff12\u6b21\u5143\u914d\u5217\u306e\u4f5c\u6210"},{"location":"numpy_basic/#_1","text":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from scipy import sparse # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 0 , 0 ], [ 0 , 1 ], [ 3 , 0 ]]) # CSR(compressed sparse row) \u5f62\u5f0f\u306e\u884c\u5217\u3092\u4f5c\u6210 matrix_sparse = sparse . csr_matrix ( matrix ) # \u758e\u884c\u5217\u3092\u8868\u793a print ( matrix_sparse )","title":"\u758e\u884c\u5217\u306e\u4f5c\u6210(\u975e\u30bc\u30ed\u884c\u5217\uff09"},{"location":"numpy_basic/#_2","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 , 4 ], [ 5 , 6 , 7 , 8 ], [ 9 , 10 , 11 , 12 ]]) # \u884c\u6570\u3001\u5217\u6570\u3092\u8868\u793a matrix . shape ########## # \u8981\u7d20\u6570\uff08\u884c\u6570 * \u5217\u6570\uff09\u3092\u8868\u793a matrix . size ########## # \u6b21\u5143\u6570\u3092\u8868\u793a matrix . ndim","title":"\u884c\u5217\u306e\u30b5\u30a4\u30ba/\u8981\u7d20\u6570/\u6b21\u5143\u306e\u53d6\u5f97"},{"location":"numpy_basic/#_3","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) # \u5f15\u6570\u306b100\u3092\u52a0\u3048\u308b\u95a2\u6570\u3092\u4f5c\u6210 add_100 = lambda i : i + 100 # \u30d9\u30af\u30c8\u30eb\u5316\u3055\u308c\u305f\u95a2\u6570\u3092\u4f5c\u6210 vectorized_add_100 = np . vectorize ( add_100 ) # \u3053\u306e\u95a2\u6570\u3092matrix\u306e\u3059\u3079\u3066\u306e\u8981\u7d20\u306b\u9069\u7528 vectorized_add_100 ( matrix )","title":"\u914d\u5217\u8981\u7d20\u3078\u306e\u56de\u6570\u306e\u9069\u7528"},{"location":"numpy_basic/#_4","text":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) # \u6700\u5927\u306e\u8981\u7d20\u3092\u8fd4\u3059 # Return maximum element np . max ( matrix ) ########## # \u6700\u5c0f\u306e\u8981\u7d20\u3092\u8fd4\u3059 np . min ( matrix ) ########## # \u5404\u5217\u306b\u304a\u3051\u308b\u6700\u5927\u8981\u7d20\u3092\u898b\u3064\u3051\u308b np . max ( matrix , axis = 0 ) ########## # \u5404\u884c\u306b\u304a\u3051\u308b\u6700\u5927\u8981\u7d20\u3092\u898b\u3064\u3051\u308b np . max ( matrix , axis = 1 )","title":"\u884c\u5217\u5185\u306e\u6700\u5927/\u6700\u5c0f\u8981\u7d20\u306e\u53d6\u5f97"},{"location":"numpy_basic/#_5","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) # \u5e73\u5747\u5024\u3092\u7b97\u51fa np . mean ( matrix ) # \u5206\u6563\u3092\u7b97\u51fa np . var ( matrix ) # \u6a19\u6e96\u504f\u5dee\u3092\u7b97\u51fa np . std ( matrix ) ########### # \u5404\u5217\u306e\u5e73\u5747\u5024\u3092\u7b97\u51fa np . mean ( matrix , axis = 0 )","title":"\u884c\u5217\u5185\u306e\u5e73\u5747/\u5206\u6563/\u6a19\u6e96\u504f\u5dee\u306e\u53d6\u5f97"},{"location":"numpy_basic/#_6","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # 4x3\u306e\u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ], [ 10 , 11 , 12 ]]) # 2x6\u306e\u884c\u5217\u306b\u5f62\u72b6\u5909\u66f4 matrix . reshape ( 2 , 6 ) matrix . size ########## matrix . reshape ( 1 , - 1 ) ########## matrix . reshape ( 12 )","title":"\u884c\u5217\u5f62\u72b6\u306e\u5909\u66f4"},{"location":"numpy_basic/#_7","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) # \u884c\u5217\u3092\u8ee2\u7f6e matrix . T ############ # \u30d9\u30af\u30c8\u30eb\u306e\u8ee2\u7f6e np . array ([ 1 , 2 , 3 , 4 , 5 , 6 ]) . T ############ # \u884c\u30d9\u30af\u30c8\u30eb\u306e\u8ee2\u7f6e np . array ([[ 1 , 2 , 3 , 4 , 5 , 6 ]]) . T","title":"\u8ee2\u7f6e\u884c\u5217\u306e\u4f5c\u6210"},{"location":"numpy_basic/#1_1","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 4 , 5 , 6 ], [ 7 , 8 , 9 ]]) # \u884c\u5217\u3092\u30d5\u30e9\u30c3\u30c8\u5316 matrix . flatten () ########### matrix . reshape ( 1 , - 1 )","title":"\u884c\u5217\u306e\u30d5\u30e9\u30c3\u30c8\u5316\uff081\u6b21\u5143\u306b\u843d\u3068\u3059\uff09"},{"location":"numpy_basic/#_8","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 1 , 1 ], [ 1 , 1 , 10 ], [ 1 , 1 , 15 ]]) # \u884c\u5217\u306e\u30e9\u30f3\u30af\u3092\u8fd4\u3059 np . linalg . matrix_rank ( matrix )","title":"\u884c\u5217\u30e9\u30f3\u30af\u306e\u53d6\u5f97"},{"location":"numpy_basic/#det","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 2 , 4 , 6 ], [ 3 , 8 , 9 ]]) # \u884c\u5217\u306e\u884c\u5217\u5f0f\u3092\u7b97\u51fa np . linalg . det ( matrix )","title":"\u884c\u5217\u5f0f\u306e\u8a08\u7b97(det\u306e\u8a08\u7b97\uff09"},{"location":"numpy_basic/#_9","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 2 , 4 , 6 ], [ 3 , 8 , 9 ]]) # \u5bfe\u89d2\u8981\u7d20\u3092\u8fd4\u3059 matrix . diagonal () ########## # \u4e3b\u5bfe\u89d2\u8981\u7d20\u306e\u4e00\u3064\u4e0a\u306e\u526f\u5bfe\u89d2\u8981\u7d20\u3092\u8fd4\u3059 matrix . diagonal ( offset = 1 ) ########## # \u4e3b\u5bfe\u89d2\u8981\u7d20\u306e\u4e00\u3064\u4e0b\u306e\u526f\u5bfe\u89d2\u8981\u7d20\u3092\u8fd4\u3059 matrix . diagonal ( offset =- 1 )","title":"\u884c\u5217\u5bfe\u89d2\u8981\u7d20\u306e\u53d6\u5f97"},{"location":"numpy_basic/#_10","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 2 , 3 ], [ 2 , 4 , 6 ], [ 3 , 8 , 9 ]]) # \u30c8\u30ec\u30fc\u30b9\u3092\u8fd4\u3059 matrix . trace () ########## # \u5bfe\u89d2\u8981\u7d20\u3092\u8db3\u3057\u5408\u308f\u305b\u305f\u3082\u306e\u3092\u8fd4\u3059 sum ( matrix . diagonal ())","title":"\u884c\u5217\u30c8\u30ec\u30fc\u30b9\u306e\u8a08\u7b97(\u5bfe\u89d2\u8981\u7d20\u306e\u7dcf\u548c\uff09"},{"location":"numpy_basic/#_11","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , - 1 , 3 ], [ 1 , 1 , 6 ], [ 3 , 8 , 9 ]]) # \u56fa\u6709\u5024\u3068\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8a08\u7b97 eigenvalues , eigenvectors = np . linalg . eig ( matrix ) # \u56fa\u6709\u5024\u3092\u8868\u793a eigenvalues # \u56fa\u6709\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a eigenvectors","title":"\u56fa\u6709\u5024/\u56fa\u6709\u30d9\u30af\u30c8\u30eb\u306e\u53d6\u5f97"},{"location":"numpy_basic/#_12","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # 2\u3064\u306e\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 vector_a = np . array ([ 1 , 2 , 3 ]) vector_b = np . array ([ 4 , 5 , 6 ]) # \u5185\u7a4d\u3092\u8a08\u7b97 np . dot ( vector_a , vector_b ) ########## # \u5185\u7a4d\u3092\u8a08\u7b97 vector_a @ vector_b","title":"\u5185\u7a4d\u306e\u8a08\u7b97"},{"location":"numpy_basic/#_13","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix_a = np . array ([[ 1 , 1 , 1 ], [ 1 , 1 , 1 ], [ 1 , 1 , 2 ]]) # \u884c\u5217\u3092\u4f5c\u6210 matrix_b = np . array ([[ 1 , 3 , 1 ], [ 1 , 3 , 1 ], [ 1 , 3 , 8 ]]) # 2\u3064\u306e\u884c\u5217\u3092\u52a0\u7b97 np . add ( matrix_a , matrix_b ) ########## # 2\u3064\u306e\u884c\u5217\u306e\u6e1b\u7b97 np . subtract ( matrix_a , matrix_b ) ########## # 2\u3064\u306e\u884c\u5217\u306e\u52a0\u7b97 matrix_a + matrix_b","title":"\u884c\u5217\u306e\u52a0\u7b97/\u6e1b\u7b97"},{"location":"numpy_basic/#_14","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix_a = np . array ([[ 1 , 1 ], [ 1 , 2 ]]) # \u884c\u5217\u3092\u4f5c\u6210 matrix_b = np . array ([[ 1 , 3 ], [ 1 , 2 ]]) # 2\u3064\u306e\u884c\u5217\u3092\u4e57\u7b97 np . dot ( matrix_a , matrix_b ) ########## # 2\u3064\u306e\u884c\u5217\u3092\u4e57\u7b97 matrix_a @ matrix_b ########## # 2\u3064\u306e\u884c\u5217\u306e\u8981\u7d20\u3054\u3068\u306e\u4e57\u7b97 matrix_a * matrix_b","title":"\u884c\u5217\u306e\u4e57\u7b97"},{"location":"numpy_basic/#_15","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u884c\u5217\u3092\u4f5c\u6210 matrix = np . array ([[ 1 , 4 ], [ 2 , 5 ]]) # \u9006\u884c\u5217\u3092\u7b97\u51fa # Calculate inverse of matrix np . linalg . inv ( matrix ) ########## # \u884c\u5217\u3068\u305d\u306e\u9006\u884c\u5217\u3092\u4e57\u7b97 matrix @ np . linalg . inv ( matrix )","title":"\u9006\u884c\u5217\u306e\u7b97\u51fa"},{"location":"numpy_basic/#_16","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u30b7\u30fc\u30c9\u5024\u3092\u8a2d\u5b9a np . random . seed ( 0 ) # 0.0\u30681.0\u306e\u9593\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u6d6e\u52d5\u5c0f\u6570\u70b9\u5024\u30923\u3064\u751f\u6210\u3059\u308b\u3002 np . random . random ( 3 ) ########## # 1\u306810\u306e\u9593\u306e\u30e9\u30f3\u30c0\u30e0\u306a\u6574\u6570\u5024\u30923\u3064\u751f\u6210 np . random . randint ( 0 , 11 , 3 ) ########## # \u5e73\u57470.0\u3001\u6a19\u6e96\u504f\u5dee1.0\u306e\u6b63\u898f\u5206\u5e03\u304b\u30893\u3064\u306e\u6570\u5024\u3092\u751f\u6210\u3059\u308b\u3002 np . random . normal ( 0.0 , 1.0 , 3 ) ########## # \u5e73\u57470.0\u3001\u30b9\u30b1\u30fc\u30eb1.0\u306e\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u5206\u5e03\u304b\u30893\u3064\u306e\u6570\u5024\u3092\u751f\u6210 np . random . logistic ( 0.0 , 1.0 , 3 ) ########## # 1.0\u4ee5\u4e0a\u3067\u30012.0\u3088\u308a\u5c0f\u3055\u30443\u3064\u306e\u6570\u5024\u3092\u751f\u6210 np . random . uniform ( 1.0 , 2.0 , 3 )","title":"\u4e71\u6570\u306e\u751f\u6210"},{"location":"pandas_basic/","text":"Pandas Basic \u00b6 \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u4f5c\u6210 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # DataFrame\u3092\u4f5c\u6210 # Create DataFrame dataframe = pd . DataFrame () # \u5217\u3092\u8ffd\u52a0 dataframe [ 'Name' ] = [ 'Jacky Jackson' , 'Steven Stevenson' ] dataframe [ 'Age' ] = [ 38 , 25 ] dataframe [ 'Driver' ] = [ True , False ] # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u8868\u793a dataframe ########## # \u884c\u3092\u4f5c\u6210 new_person = pd . Series ([ 'Molly Mooney' , 40 , True ], index = [ 'Name' , 'Age' , 'Driver' ]) # \u884c\u3092\u8ffd\u52a0 dataframe . append ( new_person , ignore_index = True ) \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u64cd\u4f5c(loc/iloc) \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u6700\u521d\u306e\u884c\u3092\u9078\u629e dataframe . iloc [ 0 ] ############ # 3\u884c\u3092\u9078\u629e dataframe . iloc [ 1 : 4 ] ############ # 4\u884c\u3092\u9078\u629e dataframe . iloc [: 4 ] ############ # \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8a2d\u5b9a dataframe = dataframe . set_index ( dataframe [ 'Name' ]) # \u884c\u3092\u8868\u793a dataframe . loc [ 'Allen, Miss Elisabeth Walton' ] \u30c7\u30fc\u30bf\u306e\u6761\u4ef6\u4ed8\u304d\u30d5\u30a1\u30a4\u30bf\u30ea\u30f3\u30b0 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # 'sex'\u5217\u304c'female'\u306e\u884c\u306e\u3046\u3061\u3001\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ dataframe [ 'Sex' ] == 'female' ] . head ( 2 ) ########## # \u884c\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0 dataframe [( dataframe [ 'Sex' ] == 'female' ) & ( dataframe [ 'Age' ] >= 65 )] \u30c7\u30fc\u30bf\u306e\u5165\u308c\u66ff\u3048 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u5024\u3092\u7f6e\u304d\u63db\u3048\u3066\u3001\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ 'Sex' ] . replace ( \"female\" , \"Woman\" ) . head ( 2 ) ########## # \"female\"\u3068\"male\"\u3092\"Woman\"\u3068\"Man\"\u306b\u305d\u308c\u305e\u308c\u7f6e\u304d\u63db\u3048\u308b\u3002 dataframe [ 'Sex' ] . replace ([ \"female\" , \"male\" ], [ \"Woman\" , \"Man\" ]) . head ( 5 ) ########## # \u5024\u3092\u7f6e\u304d\u63db\u3048\u3066\u3001\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . replace ( 1 , \"One\" ) . head ( 2 ) ########## # \u5024\u3092\u7f6e\u304d\u63db\u3048\u3066\u3001\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . replace ( r \"1st\" , \"First\" , regex = True ) . head ( 2 ) \u5217\u540d\u306e\u5909\u66f4(rename) \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u5217\u540d\u3092\u5909\u66f4\u3057\u3066, \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . rename ( columns = { 'PClass' : 'Passenger Class' }) . head ( 2 ) ########## # \u5217\u540d\u3092\u5909\u66f4\u3057\u3066, \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . rename ( columns = { 'PClass' : 'Passenger Class' , 'Sex' : 'Gender' }) . head ( 2 ) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import collections # \u30c7\u30a3\u30af\u30b7\u30e7\u30ca\u30ea\u3092\u4f5c\u6210 column_names = collections . defaultdict ( str ) # \u30ad\u30fc\u3092\u4f5c\u6210 for name in dataframe . columns : column_names [ name ] # \u30c7\u30a3\u30af\u30b7\u30e7\u30ca\u30ea\u3092\u8868\u793a column_names \u5404\u7d71\u8a08\u91cf\u306e\u8868\u793a \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u7d71\u8a08\u5024\u3092\u8a08\u7b97 print ( 'Maximum:' , dataframe [ 'Age' ] . max ()) print ( 'Minimum:' , dataframe [ 'Age' ] . min ()) print ( 'Mean:' , dataframe [ 'Age' ] . mean ()) print ( 'Sum:' , dataframe [ 'Age' ] . sum ()) print ( 'Count:' , dataframe [ 'Age' ] . count ()) ########## # \u30ab\u30a6\u30f3\u30c8\u5024\u3092\u8868\u793a dataframe . count () \u5024\u306e\u7dcf\u6570/\u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u8868\u793a \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u30ea\u30b9\u30c8\u3092\u53d6\u5f97 dataframe [ 'Sex' ] . unique () ########## # \u73fe\u308c\u305f\u56de\u6570\u3092\u8868\u793a dataframe [ 'Sex' ] . value_counts () ########## # \u30ab\u30a6\u30f3\u30c8\u6570\u3092\u8868\u793a dataframe [ 'PClass' ] . value_counts () ########## # \u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u6570\u3092\u8868\u793a dataframe [ 'PClass' ] . nunique () \u6b20\u640d\u5024\u30c7\u30fc\u30bf\u306e\u53d6\u6271(\u5024\u306e\u5165\u308c\u66ff\u3048) \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) ## \u6b20\u640d\u5024\u3092\u9078\u629e\u3057\u30012\u3064\u3092\u8868\u793a dataframe [ dataframe [ 'Age' ] . isnull ()] . head ( 2 ) ########## # \u5024\u3092NaN\u3067\u7f6e\u304d\u63db\u3048\u308b\u3053\u3068\u3092\u8a66\u307f\u308b dataframe [ 'Sex' ] = dataframe [ 'Sex' ] . replace ( 'male' , NaN ) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u5024\u3092NaN\u3067\u7f6e\u304d\u63db\u3048\u308b dataframe [ 'Sex' ] = dataframe [ 'Sex' ] . replace ( 'male' , np . nan ) ########## # \u6b20\u640d\u5024\u306e\u8868\u73fe\u3092\u6307\u5b9a\u3057\u3066\u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url , na_values = [ np . nan , 'NONE' , - 999 ]) \u5217\u306e\u524a\u9664 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u5217\u3092\u524a\u9664 dataframe . drop ( 'Age' , axis = 1 ) . head ( 2 ) ########## # \u8907\u6570\u306e\u5217\u3092\u524a\u9664 dataframe . drop ([ 'Age' , 'Sex' ], axis = 1 ) . head ( 2 ) ########## # \u5217\u3092\u524a\u9664 dataframe . drop ( dataframe . columns [ 1 ], axis = 1 ) . head ( 2 ) ########## # \u524a\u9664\u3057\u3066\u65b0\u3057\u3044DataFrame\u3092\u4f5c\u6210 dataframe_name_dropped = dataframe . drop ( dataframe . columns [ 0 ], axis = 1 ) \u884c\u306e\u524a\u9664 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u884c\u3092\u524a\u9664\u3057\u3066\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ dataframe [ 'Sex' ] != 'male' ] . head ( 2 ) ########## # \u884c\u3092\u524a\u9664\u3057\u3066\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ dataframe [ 'Name' ] != 'Allison, Miss Helen Loraine' ] . head ( 2 ) ########## # \u884c\u3092\u524a\u9664\u3057\u3066\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ dataframe . index != 0 ] . head ( 2 ) \u91cd\u8907\u306e\u524a\u9664 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u4f5c\u6210 dataframe = pd . read_csv ( url ) # \u91cd\u8907\u3057\u305f\u884c\u3092\u524a\u9664\u3057\u3001\u6700\u521d\u306e2\u884c\u3092\u51fa\u529b dataframe . drop_duplicates () . head ( 2 ) ########## # \u884c\u6570\u3092\u8868\u793a print ( \"\u3082\u3068\u306eDataFrame\u4e2d\u306e\u884c\u6570:\" , len ( dataframe )) print ( \"\u91cd\u8907\u524a\u9664\u5f8c\u306e\u884c\u6570:\" , len ( dataframe . drop_duplicates ())) ########## # \u91cd\u8907\u3092\u524a\u9664 dataframe . drop_duplicates ( subset = [ 'Sex' ]) ########## # \u91cd\u8907\u3092\u524a\u9664 dataframe . drop_duplicates ( subset = [ 'Sex' ], keep = 'last' ) \u30b0\u30eb\u30fc\u30d7\u5225\u7d71\u8a08\u91cf\u306e\u78ba\u8a8d \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # 'Sex'\u5217\u306e\u5024\u3067\u884c\u3092\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3001\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e\u5e73\u5747\u5024\u3092\u8a08\u7b97 dataframe . groupby ( 'Sex' ) . mean () ########## # \u884c\u3092\u30b0\u30eb\u30fc\u30d7\u5206\u3051 dataframe . groupby ( 'Sex' ) ########## # \u884c\u3092\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3001\u884c\u6570\u3092\u30ab\u30a6\u30f3\u30c8 dataframe . groupby ( 'Survived' )[ 'Name' ] . count () ########## # \u884c\u3092\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3001\u5e73\u5747\u5024\u3092\u8a08\u7b97 dataframe . groupby ([ 'Sex' , 'Survived' ])[ 'Age' ] . mean () \u6642\u523b\u306b\u3088\u308b\u30b0\u30eb\u30fc\u30d7\u5206\u3051 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd import numpy as np # \u30b7\u30fc\u30c9\u5024\u3092\u8a2d\u5b9a np . random . seed ( 0 ) # \u65e5\u6642\u306e\u7bc4\u56f2\u3092\u4f5c\u6210 time_index = pd . date_range ( '06/06/2017' , periods = 100000 , freq = '30S' ) # DataFrame\u3092\u4f5c\u6210 dataframe = pd . DataFrame ( index = time_index ) # \u30e9\u30f3\u30c0\u30e0\u306a\u5024\u306e\u884c\u3092\u4f5c\u6210 dataframe [ 'Sale_Amount' ] = np . random . randint ( 1 , 10 , 100000 ) # \u4e00\u9031\u9593\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3066\u3001\u9031\u3054\u3068\u306b\u5024\u3092\u96c6\u8a08 dataframe . resample ( 'W' ) . sum () ########### # 3\u884c\u8868\u793a dataframe . head ( 3 ) ########### # 2\u9031\u9593\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3066\u5e73\u5747\u5024\u3092\u8a08\u7b97 dataframe . resample ( '2W' ) . mean () ########### # \u6708\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3066\u3001\u884c\u306e\u6570\u3092\u6570\u3048\u308b dataframe . resample ( 'M' ) . count () ########### # \u6708\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3066\u3001\u884c\u306e\u6570\u3092\u6570\u3048\u308b dataframe . resample ( 'M' , label = 'left' ) . count () \u5217\u30a2\u30a4\u30c6\u30e0\u3078\u306e\u30eb\u30fc\u30d7\u51e6\u7406 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u6700\u521d\u306e2\u3064\u306e\u540d\u524d\u3092\u5927\u6587\u5b57\u306b\u3057\u3066\u8868\u793a for name in dataframe [ 'Name' ][ 0 : 2 ]: print ( name . upper ()) ########## # \u6700\u521d\u306e2\u3064\u306e\u540d\u524d\u3092\u5927\u6587\u5b57\u306b\u3057\u3066\u8868\u793a [ name . upper () for name in dataframe [ 'Name' ][ 0 : 2 ]] \u95a2\u6570\u3092\u9069\u7528\u3057\u305f\u5024\u306e\u66f8\u304d\u63db\u3048 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u95a2\u6570\u3092\u5b9a\u7fa9 def uppercase ( x ): return x . upper () # \u95a2\u6570\u3092\u9069\u7528\u3057\u3066\u3001\u7d50\u679c\u306e\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ 'Name' ] . apply ( uppercase )[ 0 : 2 ] \u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3066\u95a2\u6570\u3092\u9069\u7528 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u884c\u3092\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3001\u95a2\u6570\u3092\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306b\u9069\u7528 dataframe . groupby ( 'Sex' ) . apply ( lambda x : x . count ()) \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u9023\u7d50(concat) \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # DataFrame\u3092\u4f5c\u6210 data_a = { 'id' : [ '1' , '2' , '3' ], 'first' : [ 'Alex' , 'Amy' , 'Allen' ], 'last' : [ 'Anderson' , 'Ackerman' , 'Ali' ]} dataframe_a = pd . DataFrame ( data_a , columns = [ 'id' , 'first' , 'last' ]) # DataFrame\u3092\u4f5c\u6210 data_b = { 'id' : [ '4' , '5' , '6' ], 'first' : [ 'Billy' , 'Brian' , 'Bran' ], 'last' : [ 'Bonder' , 'Black' , 'Balwner' ]} dataframe_b = pd . DataFrame ( data_b , columns = [ 'id' , 'first' , 'last' ]) # \u884c\u65b9\u5411\u306bDataFrame\u3092\u9023\u7d50 pd . concat ([ dataframe_a , dataframe_b ], axis = 0 ) ########## # \u5217\u65b9\u5411\u306bDataFrame\u3092\u9023\u7d50 pd . concat ([ dataframe_a , dataframe_b ], axis = 1 ) ########## # \u884c\u3092\u4f5c\u6210 row = pd . Series ([ 10 , 'Chris' , 'Chillon' ], index = [ 'id' , 'first' , 'last' ]) # \u884c\u3092\u8ffd\u52a0 dataframe_a . append ( row , ignore_index = True ) \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u30de\u30fc\u30b8 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # DataFrame\u3092\u4f5c\u6210 employee_data = { 'employee_id' : [ '1' , '2' , '3' , '4' ], 'name' : [ 'Amy Jones' , 'Allen Keys' , 'Alice Bees' , 'Tim Horton' ]} dataframe_employees = pd . DataFrame ( employee_data , columns = [ 'employee_id' , 'name' ]) # DataFrame\u3092\u4f5c\u6210 sales_data = { 'employee_id' : [ '3' , '4' , '5' , '6' ], 'total_sales' : [ 23456 , 2512 , 2345 , 1455 ]} dataframe_sales = pd . DataFrame ( sales_data , columns = [ 'employee_id' , 'total_sales' ]) # DataFrame\u3092\u30de\u30fc\u30b8 pd . merge ( dataframe_employees , dataframe_sales , on = 'employee_id' ) ########## # DataFrame\u3092\u30de\u30fc\u30b8 pd . merge ( dataframe_employees , dataframe_sales , on = 'employee_id' , how = 'outer' ) ########## # DataFrame\u3092\u30de\u30fc\u30b8 pd . merge ( dataframe_employees , dataframe_sales , on = 'employee_id' , how = 'left' ) ########## # DataFrame\u3092\u30de\u30fc\u30b8 pd . merge ( dataframe_employees , dataframe_sales , left_on = 'employee_id' , right_on = 'employee_id' ) encoding\u5f62\u5f0f\u30a8\u30e9\u30fc\u3067\u8aad\u307f\u8fbc\u3081\u306a\u3044\u3068\u304d\u306e\u5bfe\u5fdc \u00b6 #Action1 #cp932\u3067\u8aad\u307f\u8fbc\u3080 upload_data = pd . read_csv ( upload_file , index_col = 0 , encoding = \"cp932\" ) #Action2 import codecs #codecs\u3067shift-JIS\u3067\u8aad\u307f\u8fbc\u3080 with codecs . open ( \"File_path\" , \"r\" , \"Shift-JIS\" , \"ignore\" ) as file : temp_data = pd . read_table ( file , delimiter = \",\" ) \u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u306e\u5909\u66f4 \u00b6 a [ \"\u5e74\u9f62\" ] . astype ( float ) \u65b0\u898f\u30ab\u30e9\u30e0\u4f5c\u6210\u6642\u306e\u95a2\u6570\u5b9a\u7fa9 \u00b6 #age\u306e\u95a2\u6570\u3092\u4e8b\u524d\u306b\u5b9a\u7fa9\u3057\u3066\u304a\u304f titanic_train [ 'Age02' ] = titanic_train [ 'Age' ] . apply ( lambda x : age ( x )) \u30af\u30ed\u30b9\u8868\u306e\u4f5c\u6210 \u00b6 #\u7e26\uff05 pd . crosstab ( resume [ 'sex' ], resume [ 'race' ], normalize = 'index' ) #\u6a2a\uff05 pd . crosstab ( resume [ 'sex' ], resume [ 'race' ], normalize = 'columns' ) \u9805\u76ee\u306e\u6587\u5b57\u5165\u308c \u00b6 #format\u3067\u5165\u529b \"He is my {} .\" . format ( \"brother\" ) \u30ab\u30e9\u30e0\u306e\u9806\u5e8f\u5909\u66f4 \u00b6 df_receipt . reindex ( columns = [ 'sales_ymd' , 'customer_id' , 'product_cd' , 'amount' ]) . head ( 10 ) \u7279\u5b9a\u306e\u6587\u5b57\u5217\u3092\u542b\u3080 \u00b6 #str.contain df_store [ df_store [ 'store_cd' ] . str . contains ( 'S14' )] . head ( 10 ) \u7279\u5b9a\u306e\u6587\u5b57\u5217\u3067\u7d42\u308f\u308b \u00b6 df_customer [ df_customer [ 'customer_id' ] . str . endswith ( '1' )] . head ( 10 ) \u884c\u3092\u305a\u3089\u3059 \u00b6 df_temp . shift () \u5217\u304b\u3089\u884c\u3078\u5909\u63db \u00b6 df . stack () \u9805\u76ee\u306e\u5165\u308c\u66ff\u3048 \u00b6 df . replace ({ \"male\" : 00 , \"female\" : \"01\" , \"unknown\" : \"99\" }) Datetime\u306e\u8a18\u8f09\u5909\u66f4 \u00b6 pd . to_datetime ( df_customer [ 'birth_day' ]) . dt . strftime ( '%Y%m %d ' ) \u6587\u5b57\u5217\u306e\u5165\u308c\u66ff\u3048 \u00b6 df_customer [ \"birth_day\" ] . str . replace ( \"-\" , \"\" ) UNIX\u79d2\u306e\u5909\u63db \u00b6 pd . to_datetime ( df_receipt [ 'sales_epoch' ], unit = 's' ) \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u91cd\u8907\u524a\u9664 \u00b6 update_data = update_data . groupby ( level = 0 ) . last () \u6587\u5b57\u5217\u306e\u5206\u5272 \u00b6 tv_kind = train_new [ 'tv' ] . str . split ( \"\uff0f\" , expand = True ) Pandas\u3067\u306e\u53ef\u8996\u5316 \u00b6 df . plot . scatter ( x = \"dwnom1\" , y = \"dwnom2\" , figsize = ( 8 , 8 )) \u6570\u5024\u30c7\u30fc\u30bf\u306e\u30d3\u30f3\u5316 \u00b6 bins = [ - np . inf , 25 , 30 , np . inf ] labels = [ 'LOW_TEMP' , \"MID_TEMP\" , 'HIGH_TEMP' ] df_2018 [ 'TEMP_GROUP' ] = pd . cut ( df_2018 [ 'MR_GT_T_airin' ], bins , labels = labels ) df_2019 [ 'TEMP_GROUP' ] = pd . cut ( df_2019 [ 'MR_GT_T_airin' ], bins , labels = labels ) \u30d5\u30a1\u30a4\u30eb\u64cd\u4f5c \u00b6 Excel\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f \u00b6 # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.xlsx' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_excel ( url , sheetname = 0 , header = 1 ) # \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . head ( 2 ) Json\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f \u00b6 # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.json' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_json ( url , orient = 'columns' ) # \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . head ( 2 ) SQL\u30af\u30a8\u30ea\u306e\u8aad\u307f\u8fbc\u307f \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd from sqlalchemy import create_engine # \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u63a5\u7d9a\u3092\u4f5c\u6210 database_connection = create_engine ( 'sqlite:///sample.db' ) # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_sql_query ( 'SELECT * FROM data' , database_connection ) # \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . head ( 2 ) \u8907\u6570\u5217\u306e\u6587\u5b57\u5217\u3092\u9023\u7d50\u3057\u3066\u65b0\u305f\u306a\u5217\u3092\u4f5c\u6210 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd from sqlalchemy import create_engine # \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u63a5\u7d9a\u3092\u4f5c\u6210 database_connection = create_engine ( 'sqlite:///sample.db' ) # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_sql_query ( 'SELECT * FROM data' , database_connection ) # \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . head ( 2 ) \u91cd\u8907\u5217\u306e\u78ba\u8a8d \u00b6 import collections l = list ( train_x . columns ) print ([ k for k , v in collections . Counter ( l ) . items () if v > 1 ])","title":"Pandas Basic"},{"location":"pandas_basic/#pandas-basic","text":"","title":"Pandas Basic"},{"location":"pandas_basic/#_1","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # DataFrame\u3092\u4f5c\u6210 # Create DataFrame dataframe = pd . DataFrame () # \u5217\u3092\u8ffd\u52a0 dataframe [ 'Name' ] = [ 'Jacky Jackson' , 'Steven Stevenson' ] dataframe [ 'Age' ] = [ 38 , 25 ] dataframe [ 'Driver' ] = [ True , False ] # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u8868\u793a dataframe ########## # \u884c\u3092\u4f5c\u6210 new_person = pd . Series ([ 'Molly Mooney' , 40 , True ], index = [ 'Name' , 'Age' , 'Driver' ]) # \u884c\u3092\u8ffd\u52a0 dataframe . append ( new_person , ignore_index = True )","title":"\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u4f5c\u6210"},{"location":"pandas_basic/#lociloc","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u6700\u521d\u306e\u884c\u3092\u9078\u629e dataframe . iloc [ 0 ] ############ # 3\u884c\u3092\u9078\u629e dataframe . iloc [ 1 : 4 ] ############ # 4\u884c\u3092\u9078\u629e dataframe . iloc [: 4 ] ############ # \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8a2d\u5b9a dataframe = dataframe . set_index ( dataframe [ 'Name' ]) # \u884c\u3092\u8868\u793a dataframe . loc [ 'Allen, Miss Elisabeth Walton' ]","title":"\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u64cd\u4f5c(loc/iloc)"},{"location":"pandas_basic/#_2","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # 'sex'\u5217\u304c'female'\u306e\u884c\u306e\u3046\u3061\u3001\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ dataframe [ 'Sex' ] == 'female' ] . head ( 2 ) ########## # \u884c\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0 dataframe [( dataframe [ 'Sex' ] == 'female' ) & ( dataframe [ 'Age' ] >= 65 )]","title":"\u30c7\u30fc\u30bf\u306e\u6761\u4ef6\u4ed8\u304d\u30d5\u30a1\u30a4\u30bf\u30ea\u30f3\u30b0"},{"location":"pandas_basic/#_3","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u5024\u3092\u7f6e\u304d\u63db\u3048\u3066\u3001\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ 'Sex' ] . replace ( \"female\" , \"Woman\" ) . head ( 2 ) ########## # \"female\"\u3068\"male\"\u3092\"Woman\"\u3068\"Man\"\u306b\u305d\u308c\u305e\u308c\u7f6e\u304d\u63db\u3048\u308b\u3002 dataframe [ 'Sex' ] . replace ([ \"female\" , \"male\" ], [ \"Woman\" , \"Man\" ]) . head ( 5 ) ########## # \u5024\u3092\u7f6e\u304d\u63db\u3048\u3066\u3001\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . replace ( 1 , \"One\" ) . head ( 2 ) ########## # \u5024\u3092\u7f6e\u304d\u63db\u3048\u3066\u3001\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . replace ( r \"1st\" , \"First\" , regex = True ) . head ( 2 )","title":"\u30c7\u30fc\u30bf\u306e\u5165\u308c\u66ff\u3048"},{"location":"pandas_basic/#rename","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u5217\u540d\u3092\u5909\u66f4\u3057\u3066, \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . rename ( columns = { 'PClass' : 'Passenger Class' }) . head ( 2 ) ########## # \u5217\u540d\u3092\u5909\u66f4\u3057\u3066, \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . rename ( columns = { 'PClass' : 'Passenger Class' , 'Sex' : 'Gender' }) . head ( 2 ) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import collections # \u30c7\u30a3\u30af\u30b7\u30e7\u30ca\u30ea\u3092\u4f5c\u6210 column_names = collections . defaultdict ( str ) # \u30ad\u30fc\u3092\u4f5c\u6210 for name in dataframe . columns : column_names [ name ] # \u30c7\u30a3\u30af\u30b7\u30e7\u30ca\u30ea\u3092\u8868\u793a column_names","title":"\u5217\u540d\u306e\u5909\u66f4(rename)"},{"location":"pandas_basic/#_4","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u7d71\u8a08\u5024\u3092\u8a08\u7b97 print ( 'Maximum:' , dataframe [ 'Age' ] . max ()) print ( 'Minimum:' , dataframe [ 'Age' ] . min ()) print ( 'Mean:' , dataframe [ 'Age' ] . mean ()) print ( 'Sum:' , dataframe [ 'Age' ] . sum ()) print ( 'Count:' , dataframe [ 'Age' ] . count ()) ########## # \u30ab\u30a6\u30f3\u30c8\u5024\u3092\u8868\u793a dataframe . count ()","title":"\u5404\u7d71\u8a08\u91cf\u306e\u8868\u793a"},{"location":"pandas_basic/#_5","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u30ea\u30b9\u30c8\u3092\u53d6\u5f97 dataframe [ 'Sex' ] . unique () ########## # \u73fe\u308c\u305f\u56de\u6570\u3092\u8868\u793a dataframe [ 'Sex' ] . value_counts () ########## # \u30ab\u30a6\u30f3\u30c8\u6570\u3092\u8868\u793a dataframe [ 'PClass' ] . value_counts () ########## # \u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u6570\u3092\u8868\u793a dataframe [ 'PClass' ] . nunique ()","title":"\u5024\u306e\u7dcf\u6570/\u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u8868\u793a"},{"location":"pandas_basic/#_6","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) ## \u6b20\u640d\u5024\u3092\u9078\u629e\u3057\u30012\u3064\u3092\u8868\u793a dataframe [ dataframe [ 'Age' ] . isnull ()] . head ( 2 ) ########## # \u5024\u3092NaN\u3067\u7f6e\u304d\u63db\u3048\u308b\u3053\u3068\u3092\u8a66\u307f\u308b dataframe [ 'Sex' ] = dataframe [ 'Sex' ] . replace ( 'male' , NaN ) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u5024\u3092NaN\u3067\u7f6e\u304d\u63db\u3048\u308b dataframe [ 'Sex' ] = dataframe [ 'Sex' ] . replace ( 'male' , np . nan ) ########## # \u6b20\u640d\u5024\u306e\u8868\u73fe\u3092\u6307\u5b9a\u3057\u3066\u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url , na_values = [ np . nan , 'NONE' , - 999 ])","title":"\u6b20\u640d\u5024\u30c7\u30fc\u30bf\u306e\u53d6\u6271(\u5024\u306e\u5165\u308c\u66ff\u3048)"},{"location":"pandas_basic/#_7","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u5217\u3092\u524a\u9664 dataframe . drop ( 'Age' , axis = 1 ) . head ( 2 ) ########## # \u8907\u6570\u306e\u5217\u3092\u524a\u9664 dataframe . drop ([ 'Age' , 'Sex' ], axis = 1 ) . head ( 2 ) ########## # \u5217\u3092\u524a\u9664 dataframe . drop ( dataframe . columns [ 1 ], axis = 1 ) . head ( 2 ) ########## # \u524a\u9664\u3057\u3066\u65b0\u3057\u3044DataFrame\u3092\u4f5c\u6210 dataframe_name_dropped = dataframe . drop ( dataframe . columns [ 0 ], axis = 1 )","title":"\u5217\u306e\u524a\u9664"},{"location":"pandas_basic/#_8","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u884c\u3092\u524a\u9664\u3057\u3066\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ dataframe [ 'Sex' ] != 'male' ] . head ( 2 ) ########## # \u884c\u3092\u524a\u9664\u3057\u3066\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ dataframe [ 'Name' ] != 'Allison, Miss Helen Loraine' ] . head ( 2 ) ########## # \u884c\u3092\u524a\u9664\u3057\u3066\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ dataframe . index != 0 ] . head ( 2 )","title":"\u884c\u306e\u524a\u9664"},{"location":"pandas_basic/#_9","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u4f5c\u6210 dataframe = pd . read_csv ( url ) # \u91cd\u8907\u3057\u305f\u884c\u3092\u524a\u9664\u3057\u3001\u6700\u521d\u306e2\u884c\u3092\u51fa\u529b dataframe . drop_duplicates () . head ( 2 ) ########## # \u884c\u6570\u3092\u8868\u793a print ( \"\u3082\u3068\u306eDataFrame\u4e2d\u306e\u884c\u6570:\" , len ( dataframe )) print ( \"\u91cd\u8907\u524a\u9664\u5f8c\u306e\u884c\u6570:\" , len ( dataframe . drop_duplicates ())) ########## # \u91cd\u8907\u3092\u524a\u9664 dataframe . drop_duplicates ( subset = [ 'Sex' ]) ########## # \u91cd\u8907\u3092\u524a\u9664 dataframe . drop_duplicates ( subset = [ 'Sex' ], keep = 'last' )","title":"\u91cd\u8907\u306e\u524a\u9664"},{"location":"pandas_basic/#_10","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # 'Sex'\u5217\u306e\u5024\u3067\u884c\u3092\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3001\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306e\u5e73\u5747\u5024\u3092\u8a08\u7b97 dataframe . groupby ( 'Sex' ) . mean () ########## # \u884c\u3092\u30b0\u30eb\u30fc\u30d7\u5206\u3051 dataframe . groupby ( 'Sex' ) ########## # \u884c\u3092\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3001\u884c\u6570\u3092\u30ab\u30a6\u30f3\u30c8 dataframe . groupby ( 'Survived' )[ 'Name' ] . count () ########## # \u884c\u3092\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3001\u5e73\u5747\u5024\u3092\u8a08\u7b97 dataframe . groupby ([ 'Sex' , 'Survived' ])[ 'Age' ] . mean ()","title":"\u30b0\u30eb\u30fc\u30d7\u5225\u7d71\u8a08\u91cf\u306e\u78ba\u8a8d"},{"location":"pandas_basic/#_11","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd import numpy as np # \u30b7\u30fc\u30c9\u5024\u3092\u8a2d\u5b9a np . random . seed ( 0 ) # \u65e5\u6642\u306e\u7bc4\u56f2\u3092\u4f5c\u6210 time_index = pd . date_range ( '06/06/2017' , periods = 100000 , freq = '30S' ) # DataFrame\u3092\u4f5c\u6210 dataframe = pd . DataFrame ( index = time_index ) # \u30e9\u30f3\u30c0\u30e0\u306a\u5024\u306e\u884c\u3092\u4f5c\u6210 dataframe [ 'Sale_Amount' ] = np . random . randint ( 1 , 10 , 100000 ) # \u4e00\u9031\u9593\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3066\u3001\u9031\u3054\u3068\u306b\u5024\u3092\u96c6\u8a08 dataframe . resample ( 'W' ) . sum () ########### # 3\u884c\u8868\u793a dataframe . head ( 3 ) ########### # 2\u9031\u9593\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3066\u5e73\u5747\u5024\u3092\u8a08\u7b97 dataframe . resample ( '2W' ) . mean () ########### # \u6708\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3066\u3001\u884c\u306e\u6570\u3092\u6570\u3048\u308b dataframe . resample ( 'M' ) . count () ########### # \u6708\u3054\u3068\u306b\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3066\u3001\u884c\u306e\u6570\u3092\u6570\u3048\u308b dataframe . resample ( 'M' , label = 'left' ) . count ()","title":"\u6642\u523b\u306b\u3088\u308b\u30b0\u30eb\u30fc\u30d7\u5206\u3051"},{"location":"pandas_basic/#_12","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u6700\u521d\u306e2\u3064\u306e\u540d\u524d\u3092\u5927\u6587\u5b57\u306b\u3057\u3066\u8868\u793a for name in dataframe [ 'Name' ][ 0 : 2 ]: print ( name . upper ()) ########## # \u6700\u521d\u306e2\u3064\u306e\u540d\u524d\u3092\u5927\u6587\u5b57\u306b\u3057\u3066\u8868\u793a [ name . upper () for name in dataframe [ 'Name' ][ 0 : 2 ]]","title":"\u5217\u30a2\u30a4\u30c6\u30e0\u3078\u306e\u30eb\u30fc\u30d7\u51e6\u7406"},{"location":"pandas_basic/#_13","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u95a2\u6570\u3092\u5b9a\u7fa9 def uppercase ( x ): return x . upper () # \u95a2\u6570\u3092\u9069\u7528\u3057\u3066\u3001\u7d50\u679c\u306e\u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe [ 'Name' ] . apply ( uppercase )[ 0 : 2 ]","title":"\u95a2\u6570\u3092\u9069\u7528\u3057\u305f\u5024\u306e\u66f8\u304d\u63db\u3048"},{"location":"pandas_basic/#_14","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://tinyurl.com/titanic-csv' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_csv ( url ) # \u884c\u3092\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3001\u95a2\u6570\u3092\u30b0\u30eb\u30fc\u30d7\u3054\u3068\u306b\u9069\u7528 dataframe . groupby ( 'Sex' ) . apply ( lambda x : x . count ())","title":"\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3066\u95a2\u6570\u3092\u9069\u7528"},{"location":"pandas_basic/#concat","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # DataFrame\u3092\u4f5c\u6210 data_a = { 'id' : [ '1' , '2' , '3' ], 'first' : [ 'Alex' , 'Amy' , 'Allen' ], 'last' : [ 'Anderson' , 'Ackerman' , 'Ali' ]} dataframe_a = pd . DataFrame ( data_a , columns = [ 'id' , 'first' , 'last' ]) # DataFrame\u3092\u4f5c\u6210 data_b = { 'id' : [ '4' , '5' , '6' ], 'first' : [ 'Billy' , 'Brian' , 'Bran' ], 'last' : [ 'Bonder' , 'Black' , 'Balwner' ]} dataframe_b = pd . DataFrame ( data_b , columns = [ 'id' , 'first' , 'last' ]) # \u884c\u65b9\u5411\u306bDataFrame\u3092\u9023\u7d50 pd . concat ([ dataframe_a , dataframe_b ], axis = 0 ) ########## # \u5217\u65b9\u5411\u306bDataFrame\u3092\u9023\u7d50 pd . concat ([ dataframe_a , dataframe_b ], axis = 1 ) ########## # \u884c\u3092\u4f5c\u6210 row = pd . Series ([ 10 , 'Chris' , 'Chillon' ], index = [ 'id' , 'first' , 'last' ]) # \u884c\u3092\u8ffd\u52a0 dataframe_a . append ( row , ignore_index = True )","title":"\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u9023\u7d50(concat)"},{"location":"pandas_basic/#_15","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # DataFrame\u3092\u4f5c\u6210 employee_data = { 'employee_id' : [ '1' , '2' , '3' , '4' ], 'name' : [ 'Amy Jones' , 'Allen Keys' , 'Alice Bees' , 'Tim Horton' ]} dataframe_employees = pd . DataFrame ( employee_data , columns = [ 'employee_id' , 'name' ]) # DataFrame\u3092\u4f5c\u6210 sales_data = { 'employee_id' : [ '3' , '4' , '5' , '6' ], 'total_sales' : [ 23456 , 2512 , 2345 , 1455 ]} dataframe_sales = pd . DataFrame ( sales_data , columns = [ 'employee_id' , 'total_sales' ]) # DataFrame\u3092\u30de\u30fc\u30b8 pd . merge ( dataframe_employees , dataframe_sales , on = 'employee_id' ) ########## # DataFrame\u3092\u30de\u30fc\u30b8 pd . merge ( dataframe_employees , dataframe_sales , on = 'employee_id' , how = 'outer' ) ########## # DataFrame\u3092\u30de\u30fc\u30b8 pd . merge ( dataframe_employees , dataframe_sales , on = 'employee_id' , how = 'left' ) ########## # DataFrame\u3092\u30de\u30fc\u30b8 pd . merge ( dataframe_employees , dataframe_sales , left_on = 'employee_id' , right_on = 'employee_id' )","title":"\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u30de\u30fc\u30b8"},{"location":"pandas_basic/#encoding","text":"#Action1 #cp932\u3067\u8aad\u307f\u8fbc\u3080 upload_data = pd . read_csv ( upload_file , index_col = 0 , encoding = \"cp932\" ) #Action2 import codecs #codecs\u3067shift-JIS\u3067\u8aad\u307f\u8fbc\u3080 with codecs . open ( \"File_path\" , \"r\" , \"Shift-JIS\" , \"ignore\" ) as file : temp_data = pd . read_table ( file , delimiter = \",\" )","title":"encoding\u5f62\u5f0f\u30a8\u30e9\u30fc\u3067\u8aad\u307f\u8fbc\u3081\u306a\u3044\u3068\u304d\u306e\u5bfe\u5fdc"},{"location":"pandas_basic/#_16","text":"a [ \"\u5e74\u9f62\" ] . astype ( float )","title":"\u30c7\u30fc\u30bf\u30bf\u30a4\u30d7\u306e\u5909\u66f4"},{"location":"pandas_basic/#_17","text":"#age\u306e\u95a2\u6570\u3092\u4e8b\u524d\u306b\u5b9a\u7fa9\u3057\u3066\u304a\u304f titanic_train [ 'Age02' ] = titanic_train [ 'Age' ] . apply ( lambda x : age ( x ))","title":"\u65b0\u898f\u30ab\u30e9\u30e0\u4f5c\u6210\u6642\u306e\u95a2\u6570\u5b9a\u7fa9"},{"location":"pandas_basic/#_18","text":"#\u7e26\uff05 pd . crosstab ( resume [ 'sex' ], resume [ 'race' ], normalize = 'index' ) #\u6a2a\uff05 pd . crosstab ( resume [ 'sex' ], resume [ 'race' ], normalize = 'columns' )","title":"\u30af\u30ed\u30b9\u8868\u306e\u4f5c\u6210"},{"location":"pandas_basic/#_19","text":"#format\u3067\u5165\u529b \"He is my {} .\" . format ( \"brother\" )","title":"\u9805\u76ee\u306e\u6587\u5b57\u5165\u308c"},{"location":"pandas_basic/#_20","text":"df_receipt . reindex ( columns = [ 'sales_ymd' , 'customer_id' , 'product_cd' , 'amount' ]) . head ( 10 )","title":"\u30ab\u30e9\u30e0\u306e\u9806\u5e8f\u5909\u66f4"},{"location":"pandas_basic/#_21","text":"#str.contain df_store [ df_store [ 'store_cd' ] . str . contains ( 'S14' )] . head ( 10 )","title":"\u7279\u5b9a\u306e\u6587\u5b57\u5217\u3092\u542b\u3080"},{"location":"pandas_basic/#_22","text":"df_customer [ df_customer [ 'customer_id' ] . str . endswith ( '1' )] . head ( 10 )","title":"\u7279\u5b9a\u306e\u6587\u5b57\u5217\u3067\u7d42\u308f\u308b"},{"location":"pandas_basic/#_23","text":"df_temp . shift ()","title":"\u884c\u3092\u305a\u3089\u3059"},{"location":"pandas_basic/#_24","text":"df . stack ()","title":"\u5217\u304b\u3089\u884c\u3078\u5909\u63db"},{"location":"pandas_basic/#_25","text":"df . replace ({ \"male\" : 00 , \"female\" : \"01\" , \"unknown\" : \"99\" })","title":"\u9805\u76ee\u306e\u5165\u308c\u66ff\u3048"},{"location":"pandas_basic/#datetime","text":"pd . to_datetime ( df_customer [ 'birth_day' ]) . dt . strftime ( '%Y%m %d ' )","title":"Datetime\u306e\u8a18\u8f09\u5909\u66f4"},{"location":"pandas_basic/#_26","text":"df_customer [ \"birth_day\" ] . str . replace ( \"-\" , \"\" )","title":"\u6587\u5b57\u5217\u306e\u5165\u308c\u66ff\u3048"},{"location":"pandas_basic/#unix","text":"pd . to_datetime ( df_receipt [ 'sales_epoch' ], unit = 's' )","title":"UNIX\u79d2\u306e\u5909\u63db"},{"location":"pandas_basic/#_27","text":"update_data = update_data . groupby ( level = 0 ) . last ()","title":"\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u91cd\u8907\u524a\u9664"},{"location":"pandas_basic/#_28","text":"tv_kind = train_new [ 'tv' ] . str . split ( \"\uff0f\" , expand = True )","title":"\u6587\u5b57\u5217\u306e\u5206\u5272"},{"location":"pandas_basic/#pandas","text":"df . plot . scatter ( x = \"dwnom1\" , y = \"dwnom2\" , figsize = ( 8 , 8 ))","title":"Pandas\u3067\u306e\u53ef\u8996\u5316"},{"location":"pandas_basic/#_29","text":"bins = [ - np . inf , 25 , 30 , np . inf ] labels = [ 'LOW_TEMP' , \"MID_TEMP\" , 'HIGH_TEMP' ] df_2018 [ 'TEMP_GROUP' ] = pd . cut ( df_2018 [ 'MR_GT_T_airin' ], bins , labels = labels ) df_2019 [ 'TEMP_GROUP' ] = pd . cut ( df_2019 [ 'MR_GT_T_airin' ], bins , labels = labels )","title":"\u6570\u5024\u30c7\u30fc\u30bf\u306e\u30d3\u30f3\u5316"},{"location":"pandas_basic/#_30","text":"","title":"\u30d5\u30a1\u30a4\u30eb\u64cd\u4f5c"},{"location":"pandas_basic/#excel","text":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.xlsx' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_excel ( url , sheetname = 0 , header = 1 ) # \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . head ( 2 )","title":"Excel\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"pandas_basic/#json","text":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # URL\u3092\u4f5c\u6210 url = 'https://raw.githubusercontent.com/chrisalbon/simulated_datasets/master/data.json' # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_json ( url , orient = 'columns' ) # \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . head ( 2 )","title":"Json\u30d5\u30a1\u30a4\u30eb\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"pandas_basic/#sql","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd from sqlalchemy import create_engine # \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u63a5\u7d9a\u3092\u4f5c\u6210 database_connection = create_engine ( 'sqlite:///sample.db' ) # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_sql_query ( 'SELECT * FROM data' , database_connection ) # \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . head ( 2 )","title":"SQL\u30af\u30a8\u30ea\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"pandas_basic/#_31","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd from sqlalchemy import create_engine # \u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u63a5\u7d9a\u3092\u4f5c\u6210 database_connection = create_engine ( 'sqlite:///sample.db' ) # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . read_sql_query ( 'SELECT * FROM data' , database_connection ) # \u6700\u521d\u306e2\u884c\u3092\u8868\u793a dataframe . head ( 2 )","title":"\u8907\u6570\u5217\u306e\u6587\u5b57\u5217\u3092\u9023\u7d50\u3057\u3066\u65b0\u305f\u306a\u5217\u3092\u4f5c\u6210"},{"location":"pandas_basic/#_32","text":"import collections l = list ( train_x . columns ) print ([ k for k , v in collections . Counter ( l ) . items () if v > 1 ])","title":"\u91cd\u8907\u5217\u306e\u78ba\u8a8d"},{"location":"sk_basic/","text":"Scikit-Learn Basic \u00b6 \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u30ed\u30fc\u30c9 \u00b6 # -*- coding: utf-8 -*- # scikit-learn\u306edatasets\u3092\u30ed\u30fc\u30c9 from sklearn import datasets # digit\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30ed\u30fc\u30c9 digits = datasets . load_digits () # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = digits . data # \u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 target = digits . target # \u6700\u521d\u306e\u89b3\u6e2c\u3092\u8868\u793a features [ 0 ] \u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u308b\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u4f5c\u6210 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.datasets import make_regression # \u7279\u5fb4\u91cf\u884c\u5217\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3001\u751f\u6210\u306b\u7528\u3044\u305f\u4fc2\u6570\u306e\u771f\u306e\u5024\u3092\u751f\u6210 features , target , coefficients = make_regression ( n_samples = 100 , n_features = 3 , n_informative = 3 , n_targets = 1 , noise = 0.0 , coef = True , random_state = 1 ) # \u7279\u5fb4\u91cf\u884c\u5217\u3068\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a print ( '\u7279\u5fb4\u91cf\u884c\u5217 \\n ' , features [: 3 ]) print ( '\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb \\n ' , target [: 3 ]) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.datasets import make_classification # \u7279\u5fb4\u91cf\u884c\u5217\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u751f\u6210 features , target = make_classification ( n_samples = 100 , n_features = 3 , n_informative = 3 , n_redundant = 0 , n_classes = 2 , weights = [ .25 , .75 ], random_state = 1 ) # \u7279\u5fb4\u91cf\u884c\u5217\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a print ( '\u7279\u5fb4\u91cf\u884c\u5217 \\n ' , features [: 3 ]) print ( '\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb \\n ' , target [: 3 ]) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.datasets import make_blobs # \u7279\u5fb4\u91cf\u884c\u5217\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u751f\u6210 features , target = make_blobs ( n_samples = 100 , n_features = 2 , centers = 3 , cluster_std = 0.5 , shuffle = True , random_state = 1 ) # \u7279\u5fb4\u91cf\u884c\u5217\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a print ( '\u7279\u5fb4\u91cf\u884c\u5217 \\n ' , features [: 3 ]) print ( '\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb \\n ' , target [: 3 ]) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import matplotlib.pyplot as plt # \u6563\u5e03\u30d7\u30ed\u30c3\u30c8\u3092\u8868\u793a plt . scatter ( features [:, 0 ], features [:, 1 ], c = target ) plt . show () \u7279\u5fb4\u91cf\u306e\u30b9\u30b1\u30fc\u30eb\u5909\u63db \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn import preprocessing # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 feature = np . array ([[ - 500.5 ], [ - 100.1 ], [ 0 ], [ 100.1 ], [ 900.9 ]]) # \u30b9\u30b1\u30fc\u30eb\u5909\u63db\u5668\u3092\u4f5c\u6210 minmax_scale = preprocessing . MinMaxScaler ( feature_range = ( 0 , 1 )) # \u7279\u5fb4\u91cf\u3092\u30b9\u30b1\u30fc\u30eb\u5909\u63db scaled_feature = minmax_scale . fit_transform ( feature ) # \u7279\u5fb4\u91cf\u3092\u8868\u793a scaled_feature \u7279\u5fb4\u91cf\u306e\u6a19\u6e96\u5316 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn import preprocessing # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 x = np . array ([[ - 1000.1 ], [ - 200.2 ], [ 500.5 ], [ 600.6 ], [ 9000.9 ]]) # \u30b9\u30b1\u30fc\u30eb\u5909\u63db\u5668\u3092\u4f5c\u6210 scaler = preprocessing . StandardScaler () # \u7279\u5fb4\u91cf\u3092\u5909\u63db standardized = scaler . fit_transform ( x ) # \u7279\u5fb4\u91cf\u3092\u8868\u793a standardized ########## # \u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u3092\u8868\u793a print ( \"\u5e73\u5747:\" , round ( standardized . mean ())) print ( \"\u6a19\u6e96\u504f\u5dee:\" , standardized . std ()) ########## # \u30b9\u30b1\u30fc\u30eb\u5909\u63db\u5668\u3092\u4f5c\u6210 # \u5916\u308c\u5024\u304c\u591a\u3044\u5834\u5408\u306e\u5bfe\u51e6\uff08\u4e2d\u592e\u5024/\u5206\u4f4d\u6570\u3067\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\uff09 robust_scaler = preprocessing . RobustScaler () # \u7279\u5fb4\u91cf\u3092\u5909\u63db robust_scaler . fit_transform ( x ) #\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3078\u306e\u843d\u3068\u3057\u8fbc\u307f scaler = preprocessing . StandardScaler () standardized = scaler . fit_transform ( X ) X = pd . DataFrame ( columns = X_columns , data = standardized ) \u7279\u5fb4\u91cf\u306e\u6b63\u898f\u5316 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.preprocessing import Normalizer # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = np . array ([[ 0.5 , 0.5 ], [ 1.1 , 3.4 ], [ 1.5 , 20.2 ], [ 1.63 , 34.4 ], [ 10.9 , 3.3 ]]) # \u6b63\u898f\u5316\u5668\u3092\u4f5c\u6210 normalizer = Normalizer ( norm = \"l2\" ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u5909\u63db normalizer . transform ( features ) ########## # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u5909\u63db features_l2_norm = Normalizer ( norm = \"l2\" ) . transform ( features ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a features_l2_norm ########## # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u5909\u63db features_l1_norm = Normalizer ( norm = \"l1\" ) . transform ( features ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a features_l1_norm ########## # \u7dcf\u8a08\u3092\u8868\u793a print ( \"\u6700\u521d\u306e\u89b3\u6e2c\u5024\u306e\u5024\u306e\u7dcf\u8a08:\" , features_l1_norm [ 0 , 0 ] + features_l1_norm [ 0 , 1 ]) \u7279\u5fb4\u91cf\u306e\u30b9\u30b1\u30fc\u30eb\u5909\u63db \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.preprocessing import PolynomialFeatures # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = np . array ([[ 2 , 3 ], [ 2 , 3 ], [ 2 , 3 ]]) # PolynomialFeatures \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210 polynomial_interaction = PolynomialFeatures ( degree = 2 , include_bias = False ) # \u591a\u9805\u5f0f\u7279\u5fb4\u91cf\u3092\u4f5c\u6210 polynomial_interaction . fit_transform ( features ) ########## interaction = PolynomialFeatures ( degree = 2 , interaction_only = True , include_bias = False ) interaction . fit_transform ( features ) \u72ec\u81ea\u306e\u7279\u5fb4\u91cf\u306e\u5909\u63db\u5668\u3092\u4f5c\u308b \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.preprocessing import FunctionTransformer # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = np . array ([[ 2 , 3 ], [ 2 , 3 ], [ 2 , 3 ]]) # \u7c21\u5358\u306a\u95a2\u6570\u3092\u5b9a\u7fa9 def add_ten ( x ): return x + 10 # \u5909\u63db\u5668\u3092\u4f5c\u6210 ten_transformer = FunctionTransformer ( add_ten ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u5909\u63db ten_transformer . transform ( features ) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # DataFrame\u3092\u4f5c\u6210 df = pd . DataFrame ( features , columns = [ \"feature_1\" , \"feature_2\" ]) # \u95a2\u6570\u3092\u9069\u7528 df . apply ( add_ten ) \u5916\u308c\u5024\u306e\u691c\u51fa\u3068\u524a\u9664 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.covariance import EllipticEnvelope from sklearn.datasets import make_blobs # \u7c21\u5358\u306a\u4eba\u5de5\u30c7\u30fc\u30bf\u3092\u751f\u6210 features , _ = make_blobs ( n_samples = 10 , n_features = 2 , centers = 1 , random_state = 1 ) # \u6700\u521d\u306e\u7279\u5fb4\u91cf\u306e\u5024\u3092\u6975\u7aef\u306a\u5024\u306b\u7f6e\u63db features [ 0 , 0 ] = 10000 features [ 0 , 1 ] = 10000 # \u691c\u51fa\u5668\u3092\u4f5c\u6210 outlier_detector = EllipticEnvelope ( contamination = .1 ) # \u691c\u51fa\u5668\u3092\u8a13\u7df4 outlier_detector . fit ( features ) # \u5916\u308c\u5024\u3092\u4e88\u6e2c outlier_detector . predict ( features ) ########## # \u7279\u5fb4\u91cf\u30921\u3064\u4f5c\u6210 feature = features [:, 0 ] # \u5916\u308c\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8fd4\u3059\u95a2\u6570\u3092\u4f5c\u308b def indicies_of_outliers ( x ): q1 , q3 = np . percentile ( x , [ 25 , 75 ]) iqr = q3 - q1 lower_bound = q1 - ( iqr * 1.5 ) upper_bound = q3 + ( iqr * 1.5 ) return np . where (( x > upper_bound ) | ( x < lower_bound )) # \u95a2\u6570\u3092\u5b9f\u884c indicies_of_outliers ( feature ) \u5916\u308c\u5024\u306e\u53d6\u6271\u3044\uff08\u524a\u9664\u3001\u5916\u308c\u5024\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u3001\u7279\u5fb4\u91cf\u3092\u5bfe\u6570\u5316\uff09 \u00b6 \u5bfe\u6570\u5316\u3059\u308b\u3053\u3068\u3067\u5916\u308c\u5024\u306e\u5f71\u97ff\u304c\u5c0f\u3055\u304f\u306a\u308b # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # DataFrame\u3092\u4f5c\u6210 houses = pd . DataFrame () houses [ 'Price' ] = [ 534433 , 392333 , 293222 , 4322032 ] houses [ 'Bathrooms' ] = [ 2 , 3.5 , 2 , 116 ] houses [ 'Square_Feet' ] = [ 1500 , 2500 , 1500 , 48000 ] # \u89b3\u6e2c\u5024\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0 houses [ houses [ 'Bathrooms' ] < 20 ] ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u771f\u507d\u6761\u4ef6\u306b\u57fa\u3065\u3044\u3066\u7279\u5fb4\u91cf\u3092\u4f5c\u308b houses [ \"Outlier\" ] = np . where ( houses [ \"Bathrooms\" ] < 20 , 0 , 1 ) # \u30c7\u30fc\u30bf\u3092\u8868\u793a houses ########## # \u7279\u5fb4\u91cf\u3092\u5bfe\u6570\u306b\u3059\u308b houses [ \"Log_Of_Square_Feet\" ] = [ np . log ( x ) for x in houses [ \"Square_Feet\" ]] # \u30c7\u30fc\u30bf\u3092\u8868\u793a houses \u7279\u5fb4\u91cf\u306e\u96e2\u6563\u5316\uff08\u30d3\u30f3\u5316\uff09 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.preprocessing import Binarizer # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 age = np . array ([[ 6 ], [ 12 ], [ 20 ], [ 36 ], [ 65 ]]) # \u4e8c\u5024\u5316\u5668\u3092\u4f5c\u6210 # Create binarizer binarizer = Binarizer ( 18 ) # \u7279\u5fb4\u91cf\u3092\u5909\u63db binarizer . fit_transform ( age ) ########## # \u7279\u5fb4\u91cf\u3092\u8907\u6570\u306e\u30d3\u30f3\u306b\u5206\u5272 np . digitize ( age , bins = [ 20 , 30 , 64 ]) ########## # \u7279\u5fb4\u91cf\u3092\u8907\u6570\u306e\u30d3\u30f3\u306b\u5206\u5272 np . digitize ( age , bins = [ 20 , 30 , 64 ], right = True ) ########## # \u7279\u5fb4\u91cf\u3092\u8907\u6570\u306e\u30d3\u30f3\u306b\u5206\u5272 np . digitize ( age , bins = [ 18 ]) \u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306b\u3088\u308b\u89b3\u6e2c\u5024\u306e\u30b0\u30eb\u30fc\u30d7\u5206\u3051 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd from sklearn.datasets import make_blobs from sklearn.cluster import KMeans # \u4eba\u5de5\u7684\u306a\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features , _ = make_blobs ( n_samples = 50 , n_features = 2 , centers = 3 , random_state = 1 ) # DataFrame\u3092\u4f5c\u6210 dataframe = pd . DataFrame ( features , columns = [ \"feature_1\" , \"feature_2\" ]) # k-means\u30af\u30e9\u30b9\u30bf\u5206\u3051\u5668\u3092\u4f5c\u6210 clusterer = KMeans ( 3 , random_state = 0 ) # \u30af\u30e9\u30b9\u30bf\u5206\u3051\u5668\u3092\u8a13\u7df4 clusterer . fit ( features ) # \u30af\u30e9\u30b9\u30bf\u5206\u3051\u3092\u5b9f\u884c dataframe [ \"group\" ] = clusterer . predict ( features ) # \u89b3\u6e2c\u5024\u306e\u6700\u521d\u306e\u6570\u500b\u3092\u8868\u793a dataframe . head ( 5 ) \u6b20\u640d\u5024\u304c\u3042\u308b\u89b3\u6e2c\u5024\u306e\u9664\u5916 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = np . array ([[ 1.1 , 11.1 ], [ 2.2 , 22.2 ], [ 3.3 , 33.3 ], [ 4.4 , 44.4 ], [ np . nan , 55 ]]) # \u6b20\u640d\u5024\u306e\u306a\u3044(~ \u3067\u6761\u4ef6\u3092\u53cd\u8ee2\u3057\u3066\u3044\u308b)\u89b3\u6e2c\u5024\u3060\u3051\u3092\u6b8b\u3059 features [ ~ np . isnan ( features ) . any ( axis = 1 )] ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . DataFrame ( features , columns = [ \"feature_1\" , \"feature_2\" ]) # \u6b20\u640d\u5024\u306e\u3042\u308b\u89b3\u6e2c\u5024\u3092\u524a\u9664 dataframe . dropna () \u6b20\u640d\u5024\u306e\u88dc\u5b8c\uff08kNN\u3067\u4e88\u6e2c\u3057\u3066\u88dc\u5b8c/\u6700\u983b\u5024/\u5e73\u5747\u5024\u7b49\u3067\u88dc\u5b8c) \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from fancyimpute import KNN from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_blobs # \u4eba\u5de5\u7684\u306a\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features , _ = make_blobs ( n_samples = 1000 , n_features = 2 , random_state = 1 ) # \u7279\u5fb4\u91cf\u3092\u6a19\u6e96\u5316 scaler = StandardScaler () standardized_features = scaler . fit_transform ( features ) # \u6700\u521d\u306e\u7279\u5fb4\u91cf\u306e\u6700\u521d\u306e\u5024\u3092\u6b20\u640d\u5024\u306b\u7f6e\u63db true_value = standardized_features [ 0 , 0 ] standardized_features [ 0 , 0 ] = np . nan # \u7279\u5fb4\u91cf\u884c\u5217\u4e2d\u306e\u6b20\u640d\u5024\u3092\u88dc\u5b8c features_knn_imputed = KNN ( k = 5 , verbose = 0 ) . complete ( standardized_features ) # \u771f\u306e\u5024\u3068\u88dc\u5b8c\u3055\u308c\u305f\u5024\u3092\u6bd4\u8f03 print ( \"\u771f\u306e\u5024:\" , true_value ) print ( \"\u88dc\u5b8c\u3055\u308c\u305f\u5024:\" , features_knn_imputed [ 0 , 0 ]) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.preprocessing import Imputer # \u6b20\u640d\u5024\u88dc\u5b8c\u5668(imputer)\u3092\u4f5c\u308b # Create imputer mean_imputer = Imputer ( strategy = \"mean\" , axis = 0 ) # \u6b20\u640d\u5024\u3092\u88dc\u5b8c\u3059\u308b features_mean_imputed = mean_imputer . fit_transform ( features ) # \u771f\u306e\u5024\u3068\u88dc\u5b8c\u3055\u308c\u305f\u5024\u3092\u6bd4\u8f03 print ( \"\u771f\u306e\u5024:\" , true_value ) print ( \"\u88dc\u5b8c\u3055\u308c\u305f\u5024:\" , features_mean_imputed [ 0 , 0 ]) \u540d\u7fa9\u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u91cf\u306e\u6570\u5024\u5316 \u00b6 \u6570\u5024\u306e\u5927\u5c0f\u95a2\u4fc2\u304c\u306a\u3044\u5834\u5408\u306f\u6ce8\u610f\u3059\u308b\u3053\u3068 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.preprocessing import LabelBinarizer , MultiLabelBinarizer # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 feature = np . array ([[ \"Texas\" ], [ \"California\" ], [ \"Texas\" ], [ \"Delaware\" ], [ \"Texas\" ]]) # \u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u3092\u4f5c\u6210 one_hot = LabelBinarizer () # \u7279\u5fb4\u91cf\u3092\u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c9 one_hot . fit_transform ( feature ) ########## # \u7279\u5fb4\u91cf\u30af\u30e9\u30b9\u3092\u8868\u793a one_hot . classes_ ########## # \u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c9\u3055\u308c\u305f\u7279\u5fb4\u91cf\u3092\u9006\u5909\u63db one_hot . inverse_transform ( one_hot . transform ( feature )) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u7279\u5fb4\u91cf\u304b\u3089\u30c0\u30df\u30fc\u5909\u6570\u3092\u751f\u6210 pd . get_dummies ( feature [:, 0 ]) ########## # \u8907\u6570\u30af\u30e9\u30b9\u7279\u5fb4\u91cf\u3092\u4f5c\u6210 multiclass_feature = [( \"Texas\" , \"Florida\" ), ( \"California\" , \"Alabama\" ), ( \"Texas\" , \"Florida\" ), ( \"Delware\" , \"Florida\" ), ( \"Texas\" , \"Alabama\" )] # \u8907\u6570\u30af\u30e9\u30b9\u7528\u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u3092\u4f5c\u6210 one_hot_multiclass = MultiLabelBinarizer () # \u8907\u6570\u30af\u30e9\u30b9\u7279\u5fb4\u91cf\u3092\u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c9 one_hot_multiclass . fit_transform ( multiclass_feature ) ########## # \u30af\u30e9\u30b9\u3092\u8868\u793a one_hot_multiclass . classes \u9806\u5e8f\u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u91cf\u306e\u6570\u5024\u5316 \u00b6 \u5024\u306e\u5f37\u5ea6\u306e\u9806\u5e8f\u304c\u660e\u78ba\u306b\u306a\u308b\u3088\u3046\u306b\u6570\u5024\u5909\u63db\u3059\u308b # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 dataframe = pd . DataFrame ({ \"Score\" : [ \"Low\" , \"Low\" , \"Medium\" , \"Medium\" , \"High\" ]}) # \u30de\u30c3\u30d7\u3092\u4f5c\u6210 scale_mapper = { \"Low\" : 1 , \"Medium\" : 2 , \"High\" : 3 } # \u7279\u5fb4\u91cf\u306e\u5024\u3092\u30de\u30c3\u30d7\u3092\u4f7f\u3063\u3066\u7f6e\u63db dataframe [ \"Score\" ] . replace ( scale_mapper ) ########## dataframe = pd . DataFrame ({ \"Score\" : [ \"Low\" , \"Low\" , \"Medium\" , \"Medium\" , \"High\" , \"Barely More Than Medium\" ]}) scale_mapper = { \"Low\" : 1 , \"Medium\" : 2 , \"Barely More Than Medium\" : 3 , \"High\" : 4 } dataframe [ \"Score\" ] . replace ( scale_mapper ) ########## scale_mapper = { \"Low\" : 1 , \"Medium\" : 2 , \"Barely More Than Medium\" : 2.1 , \"High\" : 3 } dataframe [ \"Score\" ] . replace ( scale_mapper ) \u7279\u5fb4\u91cf\u8f9e\u66f8\u306e\u6570\u5024\u5316 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.feature_extraction import DictVectorizer # \u8f9e\u66f8\u3092\u4f5c\u6210 data_dict = [{ \"Red\" : 2 , \"Blue\" : 4 }, { \"Red\" : 4 , \"Blue\" : 3 }, { \"Red\" : 1 , \"Yellow\" : 2 }, { \"Red\" : 2 , \"Yellow\" : 2 }] # \u8f9e\u66f8\u30d9\u30af\u30c8\u30eb\u5909\u63db\u5668\u3092\u4f5c\u6210 dictvectorizer = DictVectorizer ( sparse = False ) # \u8f9e\u66f8\u3092\u7279\u5fb4\u91cf\u884c\u5217\u306b\u5909\u63db features = dictvectorizer . fit_transform ( data_dict ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a features ########## # \u7279\u5fb4\u91cf\u306e\u540d\u524d\u3092\u53d6\u5f97 feature_names = dictvectorizer . get_feature_names () # \u7279\u5fb4\u91cf\u306e\u540d\u524d\u3092\u8868\u793a feature_names ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u7279\u5fb4\u91cf\u304b\u3089DataFrame\u3092\u4f5c\u6210 pd . DataFrame ( features , columns = feature_names ) ########## # 4\u3064\u306e\u6587\u66f8\u306b\u5bfe\u3059\u308b\u5358\u8a9e\u30ab\u30a6\u30f3\u30c8\u8f9e\u66f8\u3092\u4f5c\u6210 doc_1_word_count = { \"Red\" : 2 , \"Blue\" : 4 } doc_2_word_count = { \"Red\" : 4 , \"Blue\" : 3 } doc_3_word_count = { \"Red\" : 1 , \"Yellow\" : 2 } doc_4_word_count = { \"Red\" : 2 , \"Yellow\" : 2 } # \u30ea\u30b9\u30c8\u3092\u4f5c\u6210 doc_word_counts = [ doc_1_word_count , doc_2_word_count , doc_3_word_count , doc_4_word_count ] # \u5358\u8a9e\u30ab\u30a6\u30f3\u30c8\u8f9e\u66f8\u306e\u30ea\u30b9\u30c8\u3092\u7279\u5fb4\u91cf\u884c\u5217\u306b\u5909\u63db dictvectorizer . fit_transform ( doc_word_counts ) \u6b20\u640d\u30af\u30e9\u30b9\u5024\u306e\u88dc\u5b8c \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.neighbors import KNeighborsClassifier # \u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u91cf\u3092\u6301\u3064\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 X = np . array ([[ 0 , 2.10 , 1.45 ], [ 1 , 1.18 , 1.33 ], [ 0 , 1.22 , 1.27 ], [ 1 , - 0.21 , - 1.19 ]]) # \u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u91cf\u306b\u6b20\u640d\u5024\u3092\u6301\u3064\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 X_with_nan = np . array ([[ np . nan , 0.87 , 1.31 ], [ np . nan , - 0.67 , - 0.22 ]]) # KNN\u30af\u30e9\u30b9\u5206\u985e\u5668\u3092\u8a13\u7df4 clf = KNeighborsClassifier ( 3 , weights = 'distance' ) trained_model = clf . fit ( X [:, 1 :], X [:, 0 ]) # \u6b20\u640d\u5024\u306e\u30af\u30e9\u30b9\u3092\u4e88\u6e2c imputed_values = trained_model . predict ( X_with_nan [:, 1 :]) # \u4e88\u6e2c\u3055\u308c\u305f\u30af\u30e9\u30b9\u5024\u3068\u4ed6\u306e\u7279\u5fb4\u91cf\u3092\u7d50\u5408 X_with_imputed = np . hstack (( imputed_values . reshape ( - 1 , 1 ), X_with_nan [:, 1 :])) # 2\u3064\u306e\u7279\u5fb4\u91cf\u884c\u5217\u3092\u7d50\u5408 np . vstack (( X_with_imputed , X )) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.preprocessing import Imputer # 2\u3064\u306e\u7279\u5fb4\u91cf\u884c\u5217\u3092\u7d50\u5408 X_complete = np . vstack (( X_with_nan , X )) imputer = Imputer ( strategy = 'most_frequent' , axis = 0 ) imputer . fit_transform ( X_complete ) \u4e0d\u5747\u8861\u306a\u30af\u30e9\u30b9\u306e\u53d6\u6271 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_iris # iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30ed\u30fc\u30c9 iris = load_iris () # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = iris . data # \u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 target = iris . target # \u6700\u521d\u306e40\u306e\u89b3\u6e2c\u5024\u3092\u524a\u9664 features = features [ 40 :,:] target = target [ 40 :] # \u30af\u30e9\u30b90\u3067\u3042\u308b\u304b\u3069\u3046\u304b\u3092\u793a\u30592\u5024\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 target = np . where (( target == 0 ), 0 , 1 ) # \u30d0\u30e9\u30f3\u30b9\u306e\u5d29\u308c\u305f\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a target ########## # \u91cd\u307f\u3092\u4f5c\u6210 weights = { 0 : .9 , 1 : 0.1 } # \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u30af\u30e9\u30b9\u5206\u985e\u5668\u3092\u3001\u91cd\u307f\u3092\u6307\u5b9a\u3057\u3066\u4f5c\u6210 RandomForestClassifier ( class_weight = weights ) ########## # \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u30af\u30e9\u30b9\u5206\u985e\u5668\u3092\u3001\u91cd\u307f\u3092balanced\u306b\u6307\u5b9a\u3057\u3066\u4f5c\u6210 RandomForestClassifier ( class_weight = \"balanced\" ) ########## # \u305d\u308c\u305e\u308c\u306e\u30af\u30e9\u30b9\u306e\u89b3\u6e2c\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u53d6\u5f97 i_class0 = np . where ( target == 0 )[ 0 ] i_class1 = np . where ( target == 1 )[ 0 ] # \u305d\u308c\u305e\u308c\u306e\u30af\u30e9\u30b9\u306e\u89b3\u6e2c\u5024\u6570\u3092\u8a08\u7b97 n_class0 = len ( i_class0 ) n_class1 = len ( i_class1 ) # \u30af\u30e9\u30b90\u306e\u305d\u308c\u305e\u308c\u306e\u89b3\u6e2c\u5024\u306b\u5bfe\u3057\u3066\u3001\u30e9\u30f3\u30c0\u30e0\u306b # \u30af\u30e9\u30b91\u304b\u3089\u975e\u5fa9\u5143\u62bd\u51fa i_class1_downsampled = np . random . choice ( i_class1 , size = n_class0 , replace = False ) # \u30af\u30e9\u30b90\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3068\u3001 # \u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u305f\u30af\u30e9\u30b91\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u7d50\u5408 np . hstack (( target [ i_class0 ], target [ i_class1_downsampled ])) ########## # \u30af\u30e9\u30b90\u306e\u7279\u5fb4\u91cf\u884c\u5217\u3068\u3001 # \u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u305f\u30af\u30e9\u30b91\u306e\u7279\u5fb4\u91cf\u884c\u5217\u3092\u7d50\u5408 np . vstack (( features [ i_class0 ,:], features [ i_class1_downsampled ,:]))[ 0 : 5 ] ########## # \u30af\u30e9\u30b91\u306e\u305d\u308c\u305e\u308c\u306e\u89b3\u6e2c\u5024\u306b\u5bfe\u3057\u3066\u3001\u30e9\u30f3\u30c0\u30e0\u306b\u30af\u30e9\u30b90\u304b\u3089\u5fa9\u5143\u62bd\u51fa i_class0_upsampled = np . random . choice ( i_class0 , size = n_class1 , replace = True ) # \u30af\u30e9\u30b90\u306e\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3055\u308c\u305f\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3068\u30af\u30e9\u30b91\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u7d50\u5408 np . concatenate (( target [ i_class0_upsampled ], target [ i_class1 ])) ########## # \u30af\u30e9\u30b90\u3092\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u305f\u7279\u5fb4\u91cf\u884c\u5217\u3068\u3001\u30af\u30e9\u30b91\u306e\u7279\u5fb4\u91cf\u884c\u5217\u3092\u7d50\u5408 np . vstack (( features [ i_class0_upsampled ,:], features [ i_class1 ,:]))[ 0 : 5 ] \u30c6\u30ad\u30b9\u30c8\u306e\u30af\u30ea\u30fc\u30cb\u30f3\u30b0(\u30d4\u30ea\u30aa\u30c9\u306e\u524a\u9664/\u5927\u6587\u5b57\u5909\u63db/\u6b63\u898f\u62bd\u51fa\u5024\u306e\u5909\u63db) \u00b6 # -*- coding: utf-8 -*- # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 text_data = [ \" Interrobang. By Aishwarya Henriette \" , \"Parking And Going. By Karl Gautier\" , \" Today Is The night. By Jarek Prakash \" ] # \u30db\u30ef\u30a4\u30c8\u30b9\u30da\u30fc\u30b9\u3092\u524a\u9664 strip_whitespace = [ string . strip () for string in text_data ] # \u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a strip_whitespace ########## # \u30d4\u30ea\u30aa\u30c9\u3092\u524a\u9664 remove_periods = [ string . replace ( \".\" , \"\" ) for string in strip_whitespace ] # \u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a remove_periods ########## # \u95a2\u6570\u3092\u5b9a\u7fa9 def capitalizer ( string : str ) -> str : return string . upper () # \u95a2\u6570\u3092\u9069\u7528 [ capitalizer ( string ) for string in remove_periods ] ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import re # \u95a2\u6570\u3092\u5b9a\u7fa9 def replace_letters_with_X ( string : str ) -> str : return re . sub ( r \"[a-zA-Z]\" , \"X\" , string ) # \u95a2\u6570\u3092\u9069\u7528 [ replace_letters_with_X ( string ) for string in remove_periods ] HTML\u306e\u30d1\u30fc\u30b9\u3068\u30af\u30ea\u30fc\u30cb\u30f3\u30b0 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from bs4 import BeautifulSoup # HTML\u30c6\u30ad\u30b9\u30c8\u3092\u4f5c\u6210 html = \"\"\" <div class='full_name'><span style='font-weight:bold'> Masego</span> Azra</div> \"\"\" # HTML\u3092\u30d1\u30fc\u30b9 soup = BeautifulSoup ( html , \"lxml\" ) # class\u304c\"full_name\"\u3068\u306a\u3063\u3066\u3044\u308bdiv\u3092\u898b\u3064\u3051\u3066\u3001\u305d\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a soup . find ( \"div\" , { \"class\" : \"full_name\" }) . text . strip () Cross_val_Scoring_method \u00b6 https://scikit-learn.org/stable/modules/model_evaluation.html \u4e3b\u6210\u5206\u5206\u6790 from sklearn.decomposition import PCA # \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306bPCA\u3092\u57cb\u3081\u8fbc\u3081\u3070\u81ea\u52d5\u7684\u306b\u6b21\u5143\u5727\u7e2e\u3057\u3066\u304f\u308c\u308b pca = PCA ( n_components = 10 , random_state = 1 ) # \u5b66\u7fd2\u6642\u306b\u81ea\u52d5\u7684\u306bPCA\u51e6\u7406\u304c\u65bd\u3055\u308c\u308b pca . fit ( X ) X = pca . transform ( X ) \u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0 \u00b6 cols = ( 'AC' , 'SOURCE' , 'QUADRANT' , 'CNDTN' , 'BATHRM_ALL' , 'ROOM_ALL' , 'AYB_GROUP' , 'SALEDATE_YEAR' ) # process columns, apply LabelEncoder to categorical features for c in cols : lbl = LabelEncoder () lbl . fit ( list ( train_df [ c ] . values )) train_df [ c ] = lbl . transform ( list ( train_df [ c ] . values )) # shape print ( 'Shape all_data: {} ' . format ( train_df . shape )) \u5024\u306e\u50be\u3044\u3066\u3044\u308b\u6570\u5024\u30c7\u30fc\u30bf\u306e\u7d5e\u308a\u8fbc\u307f\u3068\u5909\u63db \u00b6","title":"Scikit-learn Basic"},{"location":"sk_basic/#scikit-learn-basic","text":"","title":"Scikit-Learn Basic"},{"location":"sk_basic/#_1","text":"# -*- coding: utf-8 -*- # scikit-learn\u306edatasets\u3092\u30ed\u30fc\u30c9 from sklearn import datasets # digit\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30ed\u30fc\u30c9 digits = datasets . load_digits () # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = digits . data # \u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 target = digits . target # \u6700\u521d\u306e\u89b3\u6e2c\u3092\u8868\u793a features [ 0 ]","title":"\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u30ed\u30fc\u30c9"},{"location":"sk_basic/#_2","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.datasets import make_regression # \u7279\u5fb4\u91cf\u884c\u5217\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3001\u751f\u6210\u306b\u7528\u3044\u305f\u4fc2\u6570\u306e\u771f\u306e\u5024\u3092\u751f\u6210 features , target , coefficients = make_regression ( n_samples = 100 , n_features = 3 , n_informative = 3 , n_targets = 1 , noise = 0.0 , coef = True , random_state = 1 ) # \u7279\u5fb4\u91cf\u884c\u5217\u3068\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a print ( '\u7279\u5fb4\u91cf\u884c\u5217 \\n ' , features [: 3 ]) print ( '\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb \\n ' , target [: 3 ]) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.datasets import make_classification # \u7279\u5fb4\u91cf\u884c\u5217\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u751f\u6210 features , target = make_classification ( n_samples = 100 , n_features = 3 , n_informative = 3 , n_redundant = 0 , n_classes = 2 , weights = [ .25 , .75 ], random_state = 1 ) # \u7279\u5fb4\u91cf\u884c\u5217\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a print ( '\u7279\u5fb4\u91cf\u884c\u5217 \\n ' , features [: 3 ]) print ( '\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb \\n ' , target [: 3 ]) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.datasets import make_blobs # \u7279\u5fb4\u91cf\u884c\u5217\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u751f\u6210 features , target = make_blobs ( n_samples = 100 , n_features = 2 , centers = 3 , cluster_std = 0.5 , shuffle = True , random_state = 1 ) # \u7279\u5fb4\u91cf\u884c\u5217\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a print ( '\u7279\u5fb4\u91cf\u884c\u5217 \\n ' , features [: 3 ]) print ( '\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb \\n ' , target [: 3 ]) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import matplotlib.pyplot as plt # \u6563\u5e03\u30d7\u30ed\u30c3\u30c8\u3092\u8868\u793a plt . scatter ( features [:, 0 ], features [:, 1 ], c = target ) plt . show ()","title":"\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u306b\u3088\u308b\u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u306e\u4f5c\u6210"},{"location":"sk_basic/#_3","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn import preprocessing # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 feature = np . array ([[ - 500.5 ], [ - 100.1 ], [ 0 ], [ 100.1 ], [ 900.9 ]]) # \u30b9\u30b1\u30fc\u30eb\u5909\u63db\u5668\u3092\u4f5c\u6210 minmax_scale = preprocessing . MinMaxScaler ( feature_range = ( 0 , 1 )) # \u7279\u5fb4\u91cf\u3092\u30b9\u30b1\u30fc\u30eb\u5909\u63db scaled_feature = minmax_scale . fit_transform ( feature ) # \u7279\u5fb4\u91cf\u3092\u8868\u793a scaled_feature","title":"\u7279\u5fb4\u91cf\u306e\u30b9\u30b1\u30fc\u30eb\u5909\u63db"},{"location":"sk_basic/#_4","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn import preprocessing # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 x = np . array ([[ - 1000.1 ], [ - 200.2 ], [ 500.5 ], [ 600.6 ], [ 9000.9 ]]) # \u30b9\u30b1\u30fc\u30eb\u5909\u63db\u5668\u3092\u4f5c\u6210 scaler = preprocessing . StandardScaler () # \u7279\u5fb4\u91cf\u3092\u5909\u63db standardized = scaler . fit_transform ( x ) # \u7279\u5fb4\u91cf\u3092\u8868\u793a standardized ########## # \u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u3092\u8868\u793a print ( \"\u5e73\u5747:\" , round ( standardized . mean ())) print ( \"\u6a19\u6e96\u504f\u5dee:\" , standardized . std ()) ########## # \u30b9\u30b1\u30fc\u30eb\u5909\u63db\u5668\u3092\u4f5c\u6210 # \u5916\u308c\u5024\u304c\u591a\u3044\u5834\u5408\u306e\u5bfe\u51e6\uff08\u4e2d\u592e\u5024/\u5206\u4f4d\u6570\u3067\u30b9\u30b1\u30fc\u30ea\u30f3\u30b0\uff09 robust_scaler = preprocessing . RobustScaler () # \u7279\u5fb4\u91cf\u3092\u5909\u63db robust_scaler . fit_transform ( x ) #\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3078\u306e\u843d\u3068\u3057\u8fbc\u307f scaler = preprocessing . StandardScaler () standardized = scaler . fit_transform ( X ) X = pd . DataFrame ( columns = X_columns , data = standardized )","title":"\u7279\u5fb4\u91cf\u306e\u6a19\u6e96\u5316"},{"location":"sk_basic/#_5","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.preprocessing import Normalizer # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = np . array ([[ 0.5 , 0.5 ], [ 1.1 , 3.4 ], [ 1.5 , 20.2 ], [ 1.63 , 34.4 ], [ 10.9 , 3.3 ]]) # \u6b63\u898f\u5316\u5668\u3092\u4f5c\u6210 normalizer = Normalizer ( norm = \"l2\" ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u5909\u63db normalizer . transform ( features ) ########## # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u5909\u63db features_l2_norm = Normalizer ( norm = \"l2\" ) . transform ( features ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a features_l2_norm ########## # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u5909\u63db features_l1_norm = Normalizer ( norm = \"l1\" ) . transform ( features ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a features_l1_norm ########## # \u7dcf\u8a08\u3092\u8868\u793a print ( \"\u6700\u521d\u306e\u89b3\u6e2c\u5024\u306e\u5024\u306e\u7dcf\u8a08:\" , features_l1_norm [ 0 , 0 ] + features_l1_norm [ 0 , 1 ])","title":"\u7279\u5fb4\u91cf\u306e\u6b63\u898f\u5316"},{"location":"sk_basic/#_6","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.preprocessing import PolynomialFeatures # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = np . array ([[ 2 , 3 ], [ 2 , 3 ], [ 2 , 3 ]]) # PolynomialFeatures \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u6210 polynomial_interaction = PolynomialFeatures ( degree = 2 , include_bias = False ) # \u591a\u9805\u5f0f\u7279\u5fb4\u91cf\u3092\u4f5c\u6210 polynomial_interaction . fit_transform ( features ) ########## interaction = PolynomialFeatures ( degree = 2 , interaction_only = True , include_bias = False ) interaction . fit_transform ( features )","title":"\u7279\u5fb4\u91cf\u306e\u30b9\u30b1\u30fc\u30eb\u5909\u63db"},{"location":"sk_basic/#_7","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.preprocessing import FunctionTransformer # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = np . array ([[ 2 , 3 ], [ 2 , 3 ], [ 2 , 3 ]]) # \u7c21\u5358\u306a\u95a2\u6570\u3092\u5b9a\u7fa9 def add_ten ( x ): return x + 10 # \u5909\u63db\u5668\u3092\u4f5c\u6210 ten_transformer = FunctionTransformer ( add_ten ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u5909\u63db ten_transformer . transform ( features ) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # DataFrame\u3092\u4f5c\u6210 df = pd . DataFrame ( features , columns = [ \"feature_1\" , \"feature_2\" ]) # \u95a2\u6570\u3092\u9069\u7528 df . apply ( add_ten )","title":"\u72ec\u81ea\u306e\u7279\u5fb4\u91cf\u306e\u5909\u63db\u5668\u3092\u4f5c\u308b"},{"location":"sk_basic/#_8","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.covariance import EllipticEnvelope from sklearn.datasets import make_blobs # \u7c21\u5358\u306a\u4eba\u5de5\u30c7\u30fc\u30bf\u3092\u751f\u6210 features , _ = make_blobs ( n_samples = 10 , n_features = 2 , centers = 1 , random_state = 1 ) # \u6700\u521d\u306e\u7279\u5fb4\u91cf\u306e\u5024\u3092\u6975\u7aef\u306a\u5024\u306b\u7f6e\u63db features [ 0 , 0 ] = 10000 features [ 0 , 1 ] = 10000 # \u691c\u51fa\u5668\u3092\u4f5c\u6210 outlier_detector = EllipticEnvelope ( contamination = .1 ) # \u691c\u51fa\u5668\u3092\u8a13\u7df4 outlier_detector . fit ( features ) # \u5916\u308c\u5024\u3092\u4e88\u6e2c outlier_detector . predict ( features ) ########## # \u7279\u5fb4\u91cf\u30921\u3064\u4f5c\u6210 feature = features [:, 0 ] # \u5916\u308c\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8fd4\u3059\u95a2\u6570\u3092\u4f5c\u308b def indicies_of_outliers ( x ): q1 , q3 = np . percentile ( x , [ 25 , 75 ]) iqr = q3 - q1 lower_bound = q1 - ( iqr * 1.5 ) upper_bound = q3 + ( iqr * 1.5 ) return np . where (( x > upper_bound ) | ( x < lower_bound )) # \u95a2\u6570\u3092\u5b9f\u884c indicies_of_outliers ( feature )","title":"\u5916\u308c\u5024\u306e\u691c\u51fa\u3068\u524a\u9664"},{"location":"sk_basic/#_9","text":"\u5bfe\u6570\u5316\u3059\u308b\u3053\u3068\u3067\u5916\u308c\u5024\u306e\u5f71\u97ff\u304c\u5c0f\u3055\u304f\u306a\u308b # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # DataFrame\u3092\u4f5c\u6210 houses = pd . DataFrame () houses [ 'Price' ] = [ 534433 , 392333 , 293222 , 4322032 ] houses [ 'Bathrooms' ] = [ 2 , 3.5 , 2 , 116 ] houses [ 'Square_Feet' ] = [ 1500 , 2500 , 1500 , 48000 ] # \u89b3\u6e2c\u5024\u3092\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0 houses [ houses [ 'Bathrooms' ] < 20 ] ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u771f\u507d\u6761\u4ef6\u306b\u57fa\u3065\u3044\u3066\u7279\u5fb4\u91cf\u3092\u4f5c\u308b houses [ \"Outlier\" ] = np . where ( houses [ \"Bathrooms\" ] < 20 , 0 , 1 ) # \u30c7\u30fc\u30bf\u3092\u8868\u793a houses ########## # \u7279\u5fb4\u91cf\u3092\u5bfe\u6570\u306b\u3059\u308b houses [ \"Log_Of_Square_Feet\" ] = [ np . log ( x ) for x in houses [ \"Square_Feet\" ]] # \u30c7\u30fc\u30bf\u3092\u8868\u793a houses","title":"\u5916\u308c\u5024\u306e\u53d6\u6271\u3044\uff08\u524a\u9664\u3001\u5916\u308c\u5024\u306e\u30e9\u30d9\u30ea\u30f3\u30b0\u3001\u7279\u5fb4\u91cf\u3092\u5bfe\u6570\u5316\uff09"},{"location":"sk_basic/#_10","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.preprocessing import Binarizer # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 age = np . array ([[ 6 ], [ 12 ], [ 20 ], [ 36 ], [ 65 ]]) # \u4e8c\u5024\u5316\u5668\u3092\u4f5c\u6210 # Create binarizer binarizer = Binarizer ( 18 ) # \u7279\u5fb4\u91cf\u3092\u5909\u63db binarizer . fit_transform ( age ) ########## # \u7279\u5fb4\u91cf\u3092\u8907\u6570\u306e\u30d3\u30f3\u306b\u5206\u5272 np . digitize ( age , bins = [ 20 , 30 , 64 ]) ########## # \u7279\u5fb4\u91cf\u3092\u8907\u6570\u306e\u30d3\u30f3\u306b\u5206\u5272 np . digitize ( age , bins = [ 20 , 30 , 64 ], right = True ) ########## # \u7279\u5fb4\u91cf\u3092\u8907\u6570\u306e\u30d3\u30f3\u306b\u5206\u5272 np . digitize ( age , bins = [ 18 ])","title":"\u7279\u5fb4\u91cf\u306e\u96e2\u6563\u5316\uff08\u30d3\u30f3\u5316\uff09"},{"location":"sk_basic/#_11","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd from sklearn.datasets import make_blobs from sklearn.cluster import KMeans # \u4eba\u5de5\u7684\u306a\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features , _ = make_blobs ( n_samples = 50 , n_features = 2 , centers = 3 , random_state = 1 ) # DataFrame\u3092\u4f5c\u6210 dataframe = pd . DataFrame ( features , columns = [ \"feature_1\" , \"feature_2\" ]) # k-means\u30af\u30e9\u30b9\u30bf\u5206\u3051\u5668\u3092\u4f5c\u6210 clusterer = KMeans ( 3 , random_state = 0 ) # \u30af\u30e9\u30b9\u30bf\u5206\u3051\u5668\u3092\u8a13\u7df4 clusterer . fit ( features ) # \u30af\u30e9\u30b9\u30bf\u5206\u3051\u3092\u5b9f\u884c dataframe [ \"group\" ] = clusterer . predict ( features ) # \u89b3\u6e2c\u5024\u306e\u6700\u521d\u306e\u6570\u500b\u3092\u8868\u793a dataframe . head ( 5 )","title":"\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306b\u3088\u308b\u89b3\u6e2c\u5024\u306e\u30b0\u30eb\u30fc\u30d7\u5206\u3051"},{"location":"sk_basic/#_12","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = np . array ([[ 1.1 , 11.1 ], [ 2.2 , 22.2 ], [ 3.3 , 33.3 ], [ 4.4 , 44.4 ], [ np . nan , 55 ]]) # \u6b20\u640d\u5024\u306e\u306a\u3044(~ \u3067\u6761\u4ef6\u3092\u53cd\u8ee2\u3057\u3066\u3044\u308b)\u89b3\u6e2c\u5024\u3060\u3051\u3092\u6b8b\u3059 features [ ~ np . isnan ( features ) . any ( axis = 1 )] ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u30c7\u30fc\u30bf\u3092\u30ed\u30fc\u30c9 dataframe = pd . DataFrame ( features , columns = [ \"feature_1\" , \"feature_2\" ]) # \u6b20\u640d\u5024\u306e\u3042\u308b\u89b3\u6e2c\u5024\u3092\u524a\u9664 dataframe . dropna ()","title":"\u6b20\u640d\u5024\u304c\u3042\u308b\u89b3\u6e2c\u5024\u306e\u9664\u5916"},{"location":"sk_basic/#knn","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from fancyimpute import KNN from sklearn.preprocessing import StandardScaler from sklearn.datasets import make_blobs # \u4eba\u5de5\u7684\u306a\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features , _ = make_blobs ( n_samples = 1000 , n_features = 2 , random_state = 1 ) # \u7279\u5fb4\u91cf\u3092\u6a19\u6e96\u5316 scaler = StandardScaler () standardized_features = scaler . fit_transform ( features ) # \u6700\u521d\u306e\u7279\u5fb4\u91cf\u306e\u6700\u521d\u306e\u5024\u3092\u6b20\u640d\u5024\u306b\u7f6e\u63db true_value = standardized_features [ 0 , 0 ] standardized_features [ 0 , 0 ] = np . nan # \u7279\u5fb4\u91cf\u884c\u5217\u4e2d\u306e\u6b20\u640d\u5024\u3092\u88dc\u5b8c features_knn_imputed = KNN ( k = 5 , verbose = 0 ) . complete ( standardized_features ) # \u771f\u306e\u5024\u3068\u88dc\u5b8c\u3055\u308c\u305f\u5024\u3092\u6bd4\u8f03 print ( \"\u771f\u306e\u5024:\" , true_value ) print ( \"\u88dc\u5b8c\u3055\u308c\u305f\u5024:\" , features_knn_imputed [ 0 , 0 ]) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.preprocessing import Imputer # \u6b20\u640d\u5024\u88dc\u5b8c\u5668(imputer)\u3092\u4f5c\u308b # Create imputer mean_imputer = Imputer ( strategy = \"mean\" , axis = 0 ) # \u6b20\u640d\u5024\u3092\u88dc\u5b8c\u3059\u308b features_mean_imputed = mean_imputer . fit_transform ( features ) # \u771f\u306e\u5024\u3068\u88dc\u5b8c\u3055\u308c\u305f\u5024\u3092\u6bd4\u8f03 print ( \"\u771f\u306e\u5024:\" , true_value ) print ( \"\u88dc\u5b8c\u3055\u308c\u305f\u5024:\" , features_mean_imputed [ 0 , 0 ])","title":"\u6b20\u640d\u5024\u306e\u88dc\u5b8c\uff08kNN\u3067\u4e88\u6e2c\u3057\u3066\u88dc\u5b8c/\u6700\u983b\u5024/\u5e73\u5747\u5024\u7b49\u3067\u88dc\u5b8c)"},{"location":"sk_basic/#_13","text":"\u6570\u5024\u306e\u5927\u5c0f\u95a2\u4fc2\u304c\u306a\u3044\u5834\u5408\u306f\u6ce8\u610f\u3059\u308b\u3053\u3068 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.preprocessing import LabelBinarizer , MultiLabelBinarizer # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 feature = np . array ([[ \"Texas\" ], [ \"California\" ], [ \"Texas\" ], [ \"Delaware\" ], [ \"Texas\" ]]) # \u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u3092\u4f5c\u6210 one_hot = LabelBinarizer () # \u7279\u5fb4\u91cf\u3092\u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c9 one_hot . fit_transform ( feature ) ########## # \u7279\u5fb4\u91cf\u30af\u30e9\u30b9\u3092\u8868\u793a one_hot . classes_ ########## # \u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c9\u3055\u308c\u305f\u7279\u5fb4\u91cf\u3092\u9006\u5909\u63db one_hot . inverse_transform ( one_hot . transform ( feature )) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u7279\u5fb4\u91cf\u304b\u3089\u30c0\u30df\u30fc\u5909\u6570\u3092\u751f\u6210 pd . get_dummies ( feature [:, 0 ]) ########## # \u8907\u6570\u30af\u30e9\u30b9\u7279\u5fb4\u91cf\u3092\u4f5c\u6210 multiclass_feature = [( \"Texas\" , \"Florida\" ), ( \"California\" , \"Alabama\" ), ( \"Texas\" , \"Florida\" ), ( \"Delware\" , \"Florida\" ), ( \"Texas\" , \"Alabama\" )] # \u8907\u6570\u30af\u30e9\u30b9\u7528\u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c0\u3092\u4f5c\u6210 one_hot_multiclass = MultiLabelBinarizer () # \u8907\u6570\u30af\u30e9\u30b9\u7279\u5fb4\u91cf\u3092\u30ef\u30f3\u30db\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c9 one_hot_multiclass . fit_transform ( multiclass_feature ) ########## # \u30af\u30e9\u30b9\u3092\u8868\u793a one_hot_multiclass . classes","title":"\u540d\u7fa9\u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u91cf\u306e\u6570\u5024\u5316"},{"location":"sk_basic/#_14","text":"\u5024\u306e\u5f37\u5ea6\u306e\u9806\u5e8f\u304c\u660e\u78ba\u306b\u306a\u308b\u3088\u3046\u306b\u6570\u5024\u5909\u63db\u3059\u308b # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 dataframe = pd . DataFrame ({ \"Score\" : [ \"Low\" , \"Low\" , \"Medium\" , \"Medium\" , \"High\" ]}) # \u30de\u30c3\u30d7\u3092\u4f5c\u6210 scale_mapper = { \"Low\" : 1 , \"Medium\" : 2 , \"High\" : 3 } # \u7279\u5fb4\u91cf\u306e\u5024\u3092\u30de\u30c3\u30d7\u3092\u4f7f\u3063\u3066\u7f6e\u63db dataframe [ \"Score\" ] . replace ( scale_mapper ) ########## dataframe = pd . DataFrame ({ \"Score\" : [ \"Low\" , \"Low\" , \"Medium\" , \"Medium\" , \"High\" , \"Barely More Than Medium\" ]}) scale_mapper = { \"Low\" : 1 , \"Medium\" : 2 , \"Barely More Than Medium\" : 3 , \"High\" : 4 } dataframe [ \"Score\" ] . replace ( scale_mapper ) ########## scale_mapper = { \"Low\" : 1 , \"Medium\" : 2 , \"Barely More Than Medium\" : 2.1 , \"High\" : 3 } dataframe [ \"Score\" ] . replace ( scale_mapper )","title":"\u9806\u5e8f\u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u91cf\u306e\u6570\u5024\u5316"},{"location":"sk_basic/#_15","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.feature_extraction import DictVectorizer # \u8f9e\u66f8\u3092\u4f5c\u6210 data_dict = [{ \"Red\" : 2 , \"Blue\" : 4 }, { \"Red\" : 4 , \"Blue\" : 3 }, { \"Red\" : 1 , \"Yellow\" : 2 }, { \"Red\" : 2 , \"Yellow\" : 2 }] # \u8f9e\u66f8\u30d9\u30af\u30c8\u30eb\u5909\u63db\u5668\u3092\u4f5c\u6210 dictvectorizer = DictVectorizer ( sparse = False ) # \u8f9e\u66f8\u3092\u7279\u5fb4\u91cf\u884c\u5217\u306b\u5909\u63db features = dictvectorizer . fit_transform ( data_dict ) # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u8868\u793a features ########## # \u7279\u5fb4\u91cf\u306e\u540d\u524d\u3092\u53d6\u5f97 feature_names = dictvectorizer . get_feature_names () # \u7279\u5fb4\u91cf\u306e\u540d\u524d\u3092\u8868\u793a feature_names ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u7279\u5fb4\u91cf\u304b\u3089DataFrame\u3092\u4f5c\u6210 pd . DataFrame ( features , columns = feature_names ) ########## # 4\u3064\u306e\u6587\u66f8\u306b\u5bfe\u3059\u308b\u5358\u8a9e\u30ab\u30a6\u30f3\u30c8\u8f9e\u66f8\u3092\u4f5c\u6210 doc_1_word_count = { \"Red\" : 2 , \"Blue\" : 4 } doc_2_word_count = { \"Red\" : 4 , \"Blue\" : 3 } doc_3_word_count = { \"Red\" : 1 , \"Yellow\" : 2 } doc_4_word_count = { \"Red\" : 2 , \"Yellow\" : 2 } # \u30ea\u30b9\u30c8\u3092\u4f5c\u6210 doc_word_counts = [ doc_1_word_count , doc_2_word_count , doc_3_word_count , doc_4_word_count ] # \u5358\u8a9e\u30ab\u30a6\u30f3\u30c8\u8f9e\u66f8\u306e\u30ea\u30b9\u30c8\u3092\u7279\u5fb4\u91cf\u884c\u5217\u306b\u5909\u63db dictvectorizer . fit_transform ( doc_word_counts )","title":"\u7279\u5fb4\u91cf\u8f9e\u66f8\u306e\u6570\u5024\u5316"},{"location":"sk_basic/#_16","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.neighbors import KNeighborsClassifier # \u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u91cf\u3092\u6301\u3064\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 X = np . array ([[ 0 , 2.10 , 1.45 ], [ 1 , 1.18 , 1.33 ], [ 0 , 1.22 , 1.27 ], [ 1 , - 0.21 , - 1.19 ]]) # \u30ab\u30c6\u30b4\u30ea\u7279\u5fb4\u91cf\u306b\u6b20\u640d\u5024\u3092\u6301\u3064\u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 X_with_nan = np . array ([[ np . nan , 0.87 , 1.31 ], [ np . nan , - 0.67 , - 0.22 ]]) # KNN\u30af\u30e9\u30b9\u5206\u985e\u5668\u3092\u8a13\u7df4 clf = KNeighborsClassifier ( 3 , weights = 'distance' ) trained_model = clf . fit ( X [:, 1 :], X [:, 0 ]) # \u6b20\u640d\u5024\u306e\u30af\u30e9\u30b9\u3092\u4e88\u6e2c imputed_values = trained_model . predict ( X_with_nan [:, 1 :]) # \u4e88\u6e2c\u3055\u308c\u305f\u30af\u30e9\u30b9\u5024\u3068\u4ed6\u306e\u7279\u5fb4\u91cf\u3092\u7d50\u5408 X_with_imputed = np . hstack (( imputed_values . reshape ( - 1 , 1 ), X_with_nan [:, 1 :])) # 2\u3064\u306e\u7279\u5fb4\u91cf\u884c\u5217\u3092\u7d50\u5408 np . vstack (( X_with_imputed , X )) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from sklearn.preprocessing import Imputer # 2\u3064\u306e\u7279\u5fb4\u91cf\u884c\u5217\u3092\u7d50\u5408 X_complete = np . vstack (( X_with_nan , X )) imputer = Imputer ( strategy = 'most_frequent' , axis = 0 ) imputer . fit_transform ( X_complete )","title":"\u6b20\u640d\u30af\u30e9\u30b9\u5024\u306e\u88dc\u5b8c"},{"location":"sk_basic/#_17","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.datasets import load_iris # iris\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u30ed\u30fc\u30c9 iris = load_iris () # \u7279\u5fb4\u91cf\u884c\u5217\u3092\u4f5c\u6210 features = iris . data # \u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 target = iris . target # \u6700\u521d\u306e40\u306e\u89b3\u6e2c\u5024\u3092\u524a\u9664 features = features [ 40 :,:] target = target [ 40 :] # \u30af\u30e9\u30b90\u3067\u3042\u308b\u304b\u3069\u3046\u304b\u3092\u793a\u30592\u5024\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210 target = np . where (( target == 0 ), 0 , 1 ) # \u30d0\u30e9\u30f3\u30b9\u306e\u5d29\u308c\u305f\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u8868\u793a target ########## # \u91cd\u307f\u3092\u4f5c\u6210 weights = { 0 : .9 , 1 : 0.1 } # \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u30af\u30e9\u30b9\u5206\u985e\u5668\u3092\u3001\u91cd\u307f\u3092\u6307\u5b9a\u3057\u3066\u4f5c\u6210 RandomForestClassifier ( class_weight = weights ) ########## # \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u30af\u30e9\u30b9\u5206\u985e\u5668\u3092\u3001\u91cd\u307f\u3092balanced\u306b\u6307\u5b9a\u3057\u3066\u4f5c\u6210 RandomForestClassifier ( class_weight = \"balanced\" ) ########## # \u305d\u308c\u305e\u308c\u306e\u30af\u30e9\u30b9\u306e\u89b3\u6e2c\u5024\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u53d6\u5f97 i_class0 = np . where ( target == 0 )[ 0 ] i_class1 = np . where ( target == 1 )[ 0 ] # \u305d\u308c\u305e\u308c\u306e\u30af\u30e9\u30b9\u306e\u89b3\u6e2c\u5024\u6570\u3092\u8a08\u7b97 n_class0 = len ( i_class0 ) n_class1 = len ( i_class1 ) # \u30af\u30e9\u30b90\u306e\u305d\u308c\u305e\u308c\u306e\u89b3\u6e2c\u5024\u306b\u5bfe\u3057\u3066\u3001\u30e9\u30f3\u30c0\u30e0\u306b # \u30af\u30e9\u30b91\u304b\u3089\u975e\u5fa9\u5143\u62bd\u51fa i_class1_downsampled = np . random . choice ( i_class1 , size = n_class0 , replace = False ) # \u30af\u30e9\u30b90\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3068\u3001 # \u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u305f\u30af\u30e9\u30b91\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u7d50\u5408 np . hstack (( target [ i_class0 ], target [ i_class1_downsampled ])) ########## # \u30af\u30e9\u30b90\u306e\u7279\u5fb4\u91cf\u884c\u5217\u3068\u3001 # \u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u305f\u30af\u30e9\u30b91\u306e\u7279\u5fb4\u91cf\u884c\u5217\u3092\u7d50\u5408 np . vstack (( features [ i_class0 ,:], features [ i_class1_downsampled ,:]))[ 0 : 5 ] ########## # \u30af\u30e9\u30b91\u306e\u305d\u308c\u305e\u308c\u306e\u89b3\u6e2c\u5024\u306b\u5bfe\u3057\u3066\u3001\u30e9\u30f3\u30c0\u30e0\u306b\u30af\u30e9\u30b90\u304b\u3089\u5fa9\u5143\u62bd\u51fa i_class0_upsampled = np . random . choice ( i_class0 , size = n_class1 , replace = True ) # \u30af\u30e9\u30b90\u306e\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3055\u308c\u305f\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3068\u30af\u30e9\u30b91\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u30d9\u30af\u30c8\u30eb\u3092\u7d50\u5408 np . concatenate (( target [ i_class0_upsampled ], target [ i_class1 ])) ########## # \u30af\u30e9\u30b90\u3092\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u305f\u7279\u5fb4\u91cf\u884c\u5217\u3068\u3001\u30af\u30e9\u30b91\u306e\u7279\u5fb4\u91cf\u884c\u5217\u3092\u7d50\u5408 np . vstack (( features [ i_class0_upsampled ,:], features [ i_class1 ,:]))[ 0 : 5 ]","title":"\u4e0d\u5747\u8861\u306a\u30af\u30e9\u30b9\u306e\u53d6\u6271"},{"location":"sk_basic/#_18","text":"# -*- coding: utf-8 -*- # \u30c6\u30ad\u30b9\u30c8\u3092\u751f\u6210 text_data = [ \" Interrobang. By Aishwarya Henriette \" , \"Parking And Going. By Karl Gautier\" , \" Today Is The night. By Jarek Prakash \" ] # \u30db\u30ef\u30a4\u30c8\u30b9\u30da\u30fc\u30b9\u3092\u524a\u9664 strip_whitespace = [ string . strip () for string in text_data ] # \u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a strip_whitespace ########## # \u30d4\u30ea\u30aa\u30c9\u3092\u524a\u9664 remove_periods = [ string . replace ( \".\" , \"\" ) for string in strip_whitespace ] # \u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a remove_periods ########## # \u95a2\u6570\u3092\u5b9a\u7fa9 def capitalizer ( string : str ) -> str : return string . upper () # \u95a2\u6570\u3092\u9069\u7528 [ capitalizer ( string ) for string in remove_periods ] ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import re # \u95a2\u6570\u3092\u5b9a\u7fa9 def replace_letters_with_X ( string : str ) -> str : return re . sub ( r \"[a-zA-Z]\" , \"X\" , string ) # \u95a2\u6570\u3092\u9069\u7528 [ replace_letters_with_X ( string ) for string in remove_periods ]","title":"\u30c6\u30ad\u30b9\u30c8\u306e\u30af\u30ea\u30fc\u30cb\u30f3\u30b0(\u30d4\u30ea\u30aa\u30c9\u306e\u524a\u9664/\u5927\u6587\u5b57\u5909\u63db/\u6b63\u898f\u62bd\u51fa\u5024\u306e\u5909\u63db)"},{"location":"sk_basic/#html","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from bs4 import BeautifulSoup # HTML\u30c6\u30ad\u30b9\u30c8\u3092\u4f5c\u6210 html = \"\"\" <div class='full_name'><span style='font-weight:bold'> Masego</span> Azra</div> \"\"\" # HTML\u3092\u30d1\u30fc\u30b9 soup = BeautifulSoup ( html , \"lxml\" ) # class\u304c\"full_name\"\u3068\u306a\u3063\u3066\u3044\u308bdiv\u3092\u898b\u3064\u3051\u3066\u3001\u305d\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u8868\u793a soup . find ( \"div\" , { \"class\" : \"full_name\" }) . text . strip ()","title":"HTML\u306e\u30d1\u30fc\u30b9\u3068\u30af\u30ea\u30fc\u30cb\u30f3\u30b0"},{"location":"sk_basic/#cross_val_scoring_method","text":"https://scikit-learn.org/stable/modules/model_evaluation.html \u4e3b\u6210\u5206\u5206\u6790 from sklearn.decomposition import PCA # \u30d1\u30a4\u30d7\u30e9\u30a4\u30f3\u306bPCA\u3092\u57cb\u3081\u8fbc\u3081\u3070\u81ea\u52d5\u7684\u306b\u6b21\u5143\u5727\u7e2e\u3057\u3066\u304f\u308c\u308b pca = PCA ( n_components = 10 , random_state = 1 ) # \u5b66\u7fd2\u6642\u306b\u81ea\u52d5\u7684\u306bPCA\u51e6\u7406\u304c\u65bd\u3055\u308c\u308b pca . fit ( X ) X = pca . transform ( X )","title":"Cross_val_Scoring_method"},{"location":"sk_basic/#_19","text":"cols = ( 'AC' , 'SOURCE' , 'QUADRANT' , 'CNDTN' , 'BATHRM_ALL' , 'ROOM_ALL' , 'AYB_GROUP' , 'SALEDATE_YEAR' ) # process columns, apply LabelEncoder to categorical features for c in cols : lbl = LabelEncoder () lbl . fit ( list ( train_df [ c ] . values )) train_df [ c ] = lbl . transform ( list ( train_df [ c ] . values )) # shape print ( 'Shape all_data: {} ' . format ( train_df . shape ))","title":"\u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0"},{"location":"sk_basic/#_20","text":"","title":"\u5024\u306e\u50be\u3044\u3066\u3044\u308b\u6570\u5024\u30c7\u30fc\u30bf\u306e\u7d5e\u308a\u8fbc\u307f\u3068\u5909\u63db"},{"location":"stats/","text":"Stats Model \u00b6 \u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f \u00b6 import pandas as pd import numpy as np import random import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm import math from scipy import stats import re \u30c7\u30fc\u30bf\u5206\u5272 \u00b6 from sklearn.model_selection import train_test_split # \u30c7\u30fc\u30bf\u3092\u5206\u5272 X , Xt , y , yt = train_test_split ( df_dummy_train , df_dummy_test , random_state = 0 , test_size = 0.3 ) \u30e2\u30c7\u30eb\u4f5c\u6210 \u00b6 X = sm . add_constant ( X ) stats_model = sm . OLS ( y , X ) . fit () stats_model . summary () MAE\u78ba\u8a8d \u00b6 from sklearn.metrics import mean_absolute_error pred_y_ts = stats_model . predict ( Xt ) #pred_y_ts = scaler.inverse_transform(pred_y_ts) mae_ts = mean_absolute_error ( yt , pred_y_ts ) print ( \"Test_MAE: \" + str ( round ( mae_ts , 4 )))","title":"Stats Model"},{"location":"stats/#stats-model","text":"","title":"Stats Model"},{"location":"stats/#_1","text":"import pandas as pd import numpy as np import random import matplotlib.pyplot as plt import seaborn as sns import statsmodels.api as sm import math from scipy import stats import re","title":"\u5fc5\u8981\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u8aad\u307f\u8fbc\u307f"},{"location":"stats/#_2","text":"from sklearn.model_selection import train_test_split # \u30c7\u30fc\u30bf\u3092\u5206\u5272 X , Xt , y , yt = train_test_split ( df_dummy_train , df_dummy_test , random_state = 0 , test_size = 0.3 )","title":"\u30c7\u30fc\u30bf\u5206\u5272"},{"location":"stats/#_3","text":"X = sm . add_constant ( X ) stats_model = sm . OLS ( y , X ) . fit () stats_model . summary ()","title":"\u30e2\u30c7\u30eb\u4f5c\u6210"},{"location":"stats/#mae","text":"from sklearn.metrics import mean_absolute_error pred_y_ts = stats_model . predict ( Xt ) #pred_y_ts = scaler.inverse_transform(pred_y_ts) mae_ts = mean_absolute_error ( yt , pred_y_ts ) print ( \"Test_MAE: \" + str ( round ( mae_ts , 4 )))","title":"MAE\u78ba\u8a8d"},{"location":"timeseries/","text":"Time Series \u00b6","title":"Time Series"},{"location":"timeseries/#time-series","text":"","title":"Time Series"},{"location":"ts_basic/","text":"Time Series Basic \u00b6 \u6642\u9593\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0 \u00b6 def filter_time ( input_df : pd . DataFrame , year , start_month , end_month , start_date , end_date ): \"\"\"Get selected time duration DataFrame Args: df (pd.DataFrame): Original df year (int) : data_year start_month (int): start_month end_month (int): end_month start_date (int): start_date end_date (int): end_date Returns: output_df(pd.DataFrame): Filtered_DataFrame \"\"\" input_df . index = pd . to_datetime ( input_df . index ) start_datetime = datetime ( year , start_month , start_date ) end_datetime = datetime ( year , end_month , end_date ) output_df = input_df [( input_df . index > start_datetime ) & ( input_df . index < end_datetime )] return output_df # detailed filtering function of df def detailed_filter_time ( input_df : pd . DataFrame , year , start_month , end_month , start_date , end_date , start_hour , end_hour , start_min , end_min ): \"\"\"Get selected time duration DataFrame Args: df (pd.DataFrame): Original df year (int) : data_year start_month (int): start_month end_month (int): end_month start_date (int): start_date end_date (int): end_date start_hour (int): start_hour end_hour (int): end_hour start_min (int): start_min end_min (int): end_min Returns: output_df(pd.DataFrame): Detailed_Filtered_DataFrame \"\"\" df = input_df . copy () start_datetime = datetime ( year , start_month , start_date , start_hour , start_min ) end_datetime = datetime ( year , end_month , end_date , end_hour , end_min ) output_df = df [( df . index > start_datetime ) & ( df . index < end_datetime )] return output_df \u30b9\u30b1\u30fc\u30eb\u5316\u3057\u305f\u6642\u7cfb\u5217\u30c8\u30ec\u30f3\u30c9\u306e\u78ba\u8a8d \u00b6 def minmax_scaling ( input_df : pd . DataFrame , target_cols : list ): \"\"\"Get Sacaled DataFrame Args: input_df (pd.DataFrame): Original df target_cols (list): scaling target columns Returns: output_df(pd.DataFrame): scaled_DataFrame \"\"\" output_df = input_df . copy () features = output_df [ target_cols ] scaler = preprocessing . MinMaxScaler () . fit ( features . values ) features = scaler . transform ( features . values ) output_df [ target_cols ] = features return output_df def plot_scaled_trend ( input_df , tags : list , Normalize = True , font_size = 2 ): \"\"\"Get Scaled Data Trend Args: input_df (pd.DataFrame): Original df tags (list): target column name \"\"\" sns . set ( style = \"white\" , font_scale = font_size ) # Relation driver power vs ambient temp fig , axes = plt . subplots ( 1 , 1 , figsize = ( 20 , 8 ), sharey = True ) fig . suptitle ( 'Scaled Trend' ) if Normalize : # Scaling df_scaled = minmax_scaling ( input_df = input_df , target_cols = tags ) for tag in tags : sns . lineplot ( df_scaled . index , df_scaled [ tag ], label = str ( tag ), linewidth = 2 ) plt . ylabel ( \"Scaled Sensor Values\" ) plt . xlabel ( \"Date\" ) plt . legend ( loc = \"lower left\" ) else : df_scaled = input_df . copy () for tag in tags : sns . lineplot ( df_scaled . index , df_scaled [ tag ], label = str ( tag ), linewidth = 2 ) plt . ylabel ( \"Sensor Values\" ) plt . xlabel ( \"Date\" ) plt . legend ( loc = \"lower left\" ) fig . tight_layout ( pad = 3 , w_pad = 3 , h_pad = 5.0 ) \u6587\u5b57\u5217\u306e\u65e5\u6642\u30c7\u30fc\u30bf\u3078\u306e\u5909\u63db \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np import pandas as pd # \u6587\u5b57\u5217\u3092\u4f5c\u6210 date_strings = np . array ([ '03-04-2005 11:35 PM' , '23-05-2010 12:01 AM' , '04-09-2009 09:09 PM' ]) # \u65e5\u6642\u30c7\u30fc\u30bf\u306b\u5909\u63db [ pd . to_datetime ( date , format = ' %d -%m-%Y %I:%M %p' ) for date in date_strings ] ########## # \u65e5\u6642\u30c7\u30fc\u30bf\u306b\u5909\u63db [ pd . to_datetime ( date , format = \" %d -%m-%Y %I:%M %p\" , errors = \"coerce\" ) for date in date_strings ] \u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u306e\u53d6\u308a\u6271\u3044 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 pd . Timestamp ( '2017-05-01 06:00:00' , tz = 'Europe/London' ) ########## # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 date = pd . Timestamp ( '2017-05-01 06:00:00' ) # \u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u3092\u8a2d\u5b9a date_in_london = date . tz_localize ( 'Europe/London' ) # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u8868\u793a date_in_london ########## # \u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u3092\u5909\u66f4 date_in_london . tz_convert ( 'Africa/Abidjan' ) ########## # \u65e5\u6642\u30c7\u30fc\u30bf\u30923\u3064\u4f5c\u6210 dates = pd . Series ( pd . date_range ( '2/2/2002' , periods = 3 , freq = 'M' )) # \u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u3092\u8a2d\u5b9a dates . dt . tz_localize ( 'Africa/Abidjan' ) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from pytz import all_timezones # \u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u30922\u3064\u8868\u793a all_timezones [ 0 : 2 ] \u65e5\u4ed8\u3068\u6642\u9593\u306b\u3088\u308b\u9078\u629e \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210 dataframe = pd . DataFrame () # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 dataframe [ 'date' ] = pd . date_range ( '1/1/2001' , periods = 100000 , freq = 'H' ) # 2\u3064\u306e\u65e5\u6642\u306e\u9593\u306e\u89b3\u6e2c\u5024\u3092\u9078\u629e dataframe [( dataframe [ 'date' ] > '2002-1-1 01:00:00' ) & ( dataframe [ 'date' ] <= '2002-1-1 04:00:00' )] ########## # \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3057\u3066\u6307\u5b9a dataframe = dataframe . set_index ( dataframe [ 'date' ]) # 2\u3064\u306e\u65e5\u6642\u306e\u9593\u306e\u89b3\u6e2c\u5024\u3092\u9078\u629e dataframe . loc [ '2002-1-1 01:00:00' : '2002-1-1 04:00:00' ] \u65e5\u4ed8\u30c7\u30fc\u30bf\u3092\u8907\u6570\u306e\u7279\u5fb4\u91cf\u306b\u5206\u89e3 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210 dataframe = pd . DataFrame () # 150\u306e\u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u308b dataframe [ 'date' ] = pd . date_range ( '1/1/2001' , periods = 150 , freq = 'W' ) # \u5e74\u3001\u6708\u3001\u65e5\u3001\u6642\u3001\u5206\u3092\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f5c\u6210 dataframe [ 'year' ] = dataframe [ 'date' ] . dt . year dataframe [ 'month' ] = dataframe [ 'date' ] . dt . month dataframe [ 'day' ] = dataframe [ 'date' ] . dt . day dataframe [ 'hour' ] = dataframe [ 'date' ] . dt . hour dataframe [ 'minute' ] = dataframe [ 'date' ] . dt . minute # 3\u884c\u8868\u793a dataframe . head ( 3 ) \u65e5\u4ed8\u306e\u5dee\u306e\u7b97\u51fa \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 dates = pd . Series ( pd . date_range ( \"2/2/2002\" , periods = 3 , freq = \"M\" )) # \u66dc\u65e5\u3092\u8868\u793a dates . dt . weekday_name ########## # \u66dc\u65e5\u3092\u8868\u793a dates . dt . weekday \u66dc\u65e5\u306e\u7b97\u51fa \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 dates = pd . Series ( pd . date_range ( \"2/2/2002\" , periods = 3 , freq = \"M\" )) # \u66dc\u65e5\u3092\u8868\u793a dates . dt . weekday_name ########## # \u66dc\u65e5\u3092\u8868\u793a dates . dt . weekday \u6642\u9593\u9045\u308c\u7279\u5fb4\u91cf\u306e\u4f5c\u6210 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210 dataframe = pd . DataFrame () # \u65e5\u4ed8\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 dataframe [ \"dates\" ] = pd . date_range ( \"1/1/2001\" , periods = 5 , freq = \"D\" ) dataframe [ \"stock_price\" ] = [ 1.1 , 2.2 , 3.3 , 4.4 , 5.5 ] # 1\u884c\u5206\u30e9\u30b0\u306e\u3042\u308b(\u9045\u308c\u3066\u3044\u308b)\u5024\u3092\u4f5c\u6210 dataframe [ \"previous_days_stock_price\" ] = dataframe [ \"stock_price\" ] . shift ( 1 ) # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u8868\u793a dataframe \u79fb\u52d5\u6642\u9593\u7a93\u306e\u5229\u7528 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 time_index = pd . date_range ( \"01/01/2010\" , periods = 5 , freq = \"M\" ) # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210\u3057\u3001\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8a2d\u5b9a dataframe = pd . DataFrame ( index = time_index ) # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 dataframe [ \"Stock_Price\" ] = [ 1 , 2 , 3 , 4 , 5 ] # \u79fb\u52d5\u5e73\u5747\u3092\u8a08\u7b97 dataframe . rolling ( window = 2 ) . mean () \u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u4e2d\u306e\u6b20\u640d\u5024\u306e\u53d6\u308a\u6271\u3044 \u00b6 # -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd import numpy as np # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 time_index = pd . date_range ( \"01/01/2010\" , periods = 5 , freq = \"M\" ) # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210\u3057\u3001\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8a2d\u5b9a dataframe = pd . DataFrame ( index = time_index ) # \u6b20\u640d\u5024\u3092\u542b\u3080\u7279\u5fb4\u91cf\u3092\u4f5c\u6210 dataframe [ \"Sales\" ] = [ 1.0 , 2.0 , np . nan , np . nan , 5.0 ] # \u6b20\u640d\u5024\u3092\u5185\u633f\u3057\u3066\u88dc\u5b8c dataframe . interpolate () ########## # \u524d\u65b9\u88dc\u5b8c dataframe . ffill () ########## # \u5f8c\u65b9\u88dc\u5b8c dataframe . bfill () ########## # \u6b20\u640d\u5024\u3092\u5185\u633f dataframe . interpolate ( method = \"quadratic\" ) ########## # \u6b20\u640d\u5024\u3092\u5185\u633f dataframe . interpolate ( limit = 1 , limit_direction = \"forward\" )","title":"Time-series Basic"},{"location":"ts_basic/#time-series-basic","text":"","title":"Time Series Basic"},{"location":"ts_basic/#_1","text":"def filter_time ( input_df : pd . DataFrame , year , start_month , end_month , start_date , end_date ): \"\"\"Get selected time duration DataFrame Args: df (pd.DataFrame): Original df year (int) : data_year start_month (int): start_month end_month (int): end_month start_date (int): start_date end_date (int): end_date Returns: output_df(pd.DataFrame): Filtered_DataFrame \"\"\" input_df . index = pd . to_datetime ( input_df . index ) start_datetime = datetime ( year , start_month , start_date ) end_datetime = datetime ( year , end_month , end_date ) output_df = input_df [( input_df . index > start_datetime ) & ( input_df . index < end_datetime )] return output_df # detailed filtering function of df def detailed_filter_time ( input_df : pd . DataFrame , year , start_month , end_month , start_date , end_date , start_hour , end_hour , start_min , end_min ): \"\"\"Get selected time duration DataFrame Args: df (pd.DataFrame): Original df year (int) : data_year start_month (int): start_month end_month (int): end_month start_date (int): start_date end_date (int): end_date start_hour (int): start_hour end_hour (int): end_hour start_min (int): start_min end_min (int): end_min Returns: output_df(pd.DataFrame): Detailed_Filtered_DataFrame \"\"\" df = input_df . copy () start_datetime = datetime ( year , start_month , start_date , start_hour , start_min ) end_datetime = datetime ( year , end_month , end_date , end_hour , end_min ) output_df = df [( df . index > start_datetime ) & ( df . index < end_datetime )] return output_df","title":"\u6642\u9593\u30d5\u30a3\u30eb\u30bf\u30ea\u30f3\u30b0"},{"location":"ts_basic/#_2","text":"def minmax_scaling ( input_df : pd . DataFrame , target_cols : list ): \"\"\"Get Sacaled DataFrame Args: input_df (pd.DataFrame): Original df target_cols (list): scaling target columns Returns: output_df(pd.DataFrame): scaled_DataFrame \"\"\" output_df = input_df . copy () features = output_df [ target_cols ] scaler = preprocessing . MinMaxScaler () . fit ( features . values ) features = scaler . transform ( features . values ) output_df [ target_cols ] = features return output_df def plot_scaled_trend ( input_df , tags : list , Normalize = True , font_size = 2 ): \"\"\"Get Scaled Data Trend Args: input_df (pd.DataFrame): Original df tags (list): target column name \"\"\" sns . set ( style = \"white\" , font_scale = font_size ) # Relation driver power vs ambient temp fig , axes = plt . subplots ( 1 , 1 , figsize = ( 20 , 8 ), sharey = True ) fig . suptitle ( 'Scaled Trend' ) if Normalize : # Scaling df_scaled = minmax_scaling ( input_df = input_df , target_cols = tags ) for tag in tags : sns . lineplot ( df_scaled . index , df_scaled [ tag ], label = str ( tag ), linewidth = 2 ) plt . ylabel ( \"Scaled Sensor Values\" ) plt . xlabel ( \"Date\" ) plt . legend ( loc = \"lower left\" ) else : df_scaled = input_df . copy () for tag in tags : sns . lineplot ( df_scaled . index , df_scaled [ tag ], label = str ( tag ), linewidth = 2 ) plt . ylabel ( \"Sensor Values\" ) plt . xlabel ( \"Date\" ) plt . legend ( loc = \"lower left\" ) fig . tight_layout ( pad = 3 , w_pad = 3 , h_pad = 5.0 )","title":"\u30b9\u30b1\u30fc\u30eb\u5316\u3057\u305f\u6642\u7cfb\u5217\u30c8\u30ec\u30f3\u30c9\u306e\u78ba\u8a8d"},{"location":"ts_basic/#_3","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import numpy as np import pandas as pd # \u6587\u5b57\u5217\u3092\u4f5c\u6210 date_strings = np . array ([ '03-04-2005 11:35 PM' , '23-05-2010 12:01 AM' , '04-09-2009 09:09 PM' ]) # \u65e5\u6642\u30c7\u30fc\u30bf\u306b\u5909\u63db [ pd . to_datetime ( date , format = ' %d -%m-%Y %I:%M %p' ) for date in date_strings ] ########## # \u65e5\u6642\u30c7\u30fc\u30bf\u306b\u5909\u63db [ pd . to_datetime ( date , format = \" %d -%m-%Y %I:%M %p\" , errors = \"coerce\" ) for date in date_strings ]","title":"\u6587\u5b57\u5217\u306e\u65e5\u6642\u30c7\u30fc\u30bf\u3078\u306e\u5909\u63db"},{"location":"ts_basic/#_4","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 pd . Timestamp ( '2017-05-01 06:00:00' , tz = 'Europe/London' ) ########## # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 date = pd . Timestamp ( '2017-05-01 06:00:00' ) # \u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u3092\u8a2d\u5b9a date_in_london = date . tz_localize ( 'Europe/London' ) # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u8868\u793a date_in_london ########## # \u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u3092\u5909\u66f4 date_in_london . tz_convert ( 'Africa/Abidjan' ) ########## # \u65e5\u6642\u30c7\u30fc\u30bf\u30923\u3064\u4f5c\u6210 dates = pd . Series ( pd . date_range ( '2/2/2002' , periods = 3 , freq = 'M' )) # \u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u3092\u8a2d\u5b9a dates . dt . tz_localize ( 'Africa/Abidjan' ) ########## # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 from pytz import all_timezones # \u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u30922\u3064\u8868\u793a all_timezones [ 0 : 2 ]","title":"\u30bf\u30a4\u30e0\u30be\u30fc\u30f3\u306e\u53d6\u308a\u6271\u3044"},{"location":"ts_basic/#_5","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210 dataframe = pd . DataFrame () # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 dataframe [ 'date' ] = pd . date_range ( '1/1/2001' , periods = 100000 , freq = 'H' ) # 2\u3064\u306e\u65e5\u6642\u306e\u9593\u306e\u89b3\u6e2c\u5024\u3092\u9078\u629e dataframe [( dataframe [ 'date' ] > '2002-1-1 01:00:00' ) & ( dataframe [ 'date' ] <= '2002-1-1 04:00:00' )] ########## # \u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3068\u3057\u3066\u6307\u5b9a dataframe = dataframe . set_index ( dataframe [ 'date' ]) # 2\u3064\u306e\u65e5\u6642\u306e\u9593\u306e\u89b3\u6e2c\u5024\u3092\u9078\u629e dataframe . loc [ '2002-1-1 01:00:00' : '2002-1-1 04:00:00' ]","title":"\u65e5\u4ed8\u3068\u6642\u9593\u306b\u3088\u308b\u9078\u629e"},{"location":"ts_basic/#_6","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210 dataframe = pd . DataFrame () # 150\u306e\u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u308b dataframe [ 'date' ] = pd . date_range ( '1/1/2001' , periods = 150 , freq = 'W' ) # \u5e74\u3001\u6708\u3001\u65e5\u3001\u6642\u3001\u5206\u3092\u7279\u5fb4\u91cf\u3068\u3057\u3066\u4f5c\u6210 dataframe [ 'year' ] = dataframe [ 'date' ] . dt . year dataframe [ 'month' ] = dataframe [ 'date' ] . dt . month dataframe [ 'day' ] = dataframe [ 'date' ] . dt . day dataframe [ 'hour' ] = dataframe [ 'date' ] . dt . hour dataframe [ 'minute' ] = dataframe [ 'date' ] . dt . minute # 3\u884c\u8868\u793a dataframe . head ( 3 )","title":"\u65e5\u4ed8\u30c7\u30fc\u30bf\u3092\u8907\u6570\u306e\u7279\u5fb4\u91cf\u306b\u5206\u89e3"},{"location":"ts_basic/#_7","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 dates = pd . Series ( pd . date_range ( \"2/2/2002\" , periods = 3 , freq = \"M\" )) # \u66dc\u65e5\u3092\u8868\u793a dates . dt . weekday_name ########## # \u66dc\u65e5\u3092\u8868\u793a dates . dt . weekday","title":"\u65e5\u4ed8\u306e\u5dee\u306e\u7b97\u51fa"},{"location":"ts_basic/#_8","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 dates = pd . Series ( pd . date_range ( \"2/2/2002\" , periods = 3 , freq = \"M\" )) # \u66dc\u65e5\u3092\u8868\u793a dates . dt . weekday_name ########## # \u66dc\u65e5\u3092\u8868\u793a dates . dt . weekday","title":"\u66dc\u65e5\u306e\u7b97\u51fa"},{"location":"ts_basic/#_9","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210 dataframe = pd . DataFrame () # \u65e5\u4ed8\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 dataframe [ \"dates\" ] = pd . date_range ( \"1/1/2001\" , periods = 5 , freq = \"D\" ) dataframe [ \"stock_price\" ] = [ 1.1 , 2.2 , 3.3 , 4.4 , 5.5 ] # 1\u884c\u5206\u30e9\u30b0\u306e\u3042\u308b(\u9045\u308c\u3066\u3044\u308b)\u5024\u3092\u4f5c\u6210 dataframe [ \"previous_days_stock_price\" ] = dataframe [ \"stock_price\" ] . shift ( 1 ) # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u8868\u793a dataframe","title":"\u6642\u9593\u9045\u308c\u7279\u5fb4\u91cf\u306e\u4f5c\u6210"},{"location":"ts_basic/#_10","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 time_index = pd . date_range ( \"01/01/2010\" , periods = 5 , freq = \"M\" ) # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210\u3057\u3001\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8a2d\u5b9a dataframe = pd . DataFrame ( index = time_index ) # \u7279\u5fb4\u91cf\u3092\u4f5c\u6210 dataframe [ \"Stock_Price\" ] = [ 1 , 2 , 3 , 4 , 5 ] # \u79fb\u52d5\u5e73\u5747\u3092\u8a08\u7b97 dataframe . rolling ( window = 2 ) . mean ()","title":"\u79fb\u52d5\u6642\u9593\u7a93\u306e\u5229\u7528"},{"location":"ts_basic/#_11","text":"# -*- coding: utf-8 -*- # \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30ed\u30fc\u30c9 import pandas as pd import numpy as np # \u65e5\u6642\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 time_index = pd . date_range ( \"01/01/2010\" , periods = 5 , freq = \"M\" ) # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u3092\u4f5c\u6210\u3057\u3001\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u8a2d\u5b9a dataframe = pd . DataFrame ( index = time_index ) # \u6b20\u640d\u5024\u3092\u542b\u3080\u7279\u5fb4\u91cf\u3092\u4f5c\u6210 dataframe [ \"Sales\" ] = [ 1.0 , 2.0 , np . nan , np . nan , 5.0 ] # \u6b20\u640d\u5024\u3092\u5185\u633f\u3057\u3066\u88dc\u5b8c dataframe . interpolate () ########## # \u524d\u65b9\u88dc\u5b8c dataframe . ffill () ########## # \u5f8c\u65b9\u88dc\u5b8c dataframe . bfill () ########## # \u6b20\u640d\u5024\u3092\u5185\u633f dataframe . interpolate ( method = \"quadratic\" ) ########## # \u6b20\u640d\u5024\u3092\u5185\u633f dataframe . interpolate ( limit = 1 , limit_direction = \"forward\" )","title":"\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u4e2d\u306e\u6b20\u640d\u5024\u306e\u53d6\u308a\u6271\u3044"},{"location":"visualize_basic/","text":"Matplot/Seaborn Basic \u00b6 \u8868\u793a\u57fa\u672c\u8a2d\u5b9a \u00b6 sns . set ( font_scale = 1.1 ) sns . set_style ( 'white' ) fig = plt . figure ( figsize = ( 30 , 5 )) plt . figure ( figsize = ( 8 , 8 )) \u8907\u6570\u30b0\u30e9\u30d5\u306e\u63cf\u753b \u00b6 congress80_dem = congress80 [ congress80 [ 'party' ] == 'Democrat' ] .copy () congress80_rep = congress80 [ congress80 [ 'party' ] == 'Republican' ] .copy () #\u30b0\u30e9\u30d5\u306e\u63cf\u753b\u7bc4\u56f2\u3092\u8a2d\u5b9a\u3059\u308b fig, ax = plt.subplots () plt.xlim ( -1.5, 2 ) plt.ylim ( -1.5, 1 .5 ) # Democrat\u306e\u6563\u5e03\u56f3\u3092\u4f5c\u308b plt.scatter ( congress80_dem [ 'dwnom1' ] , congress80_dem [ 'dwnom2' ] , c = \"r\" , alpha = 0 .3 ) # Republican\u306e\u6563\u5e03\u56f3\u3092\u4f5c\u308b plt.scatter ( congress80_rep [ 'dwnom1' ] , congress80_rep [ 'dwnom2' ] , c = \"b\" , alpha = 0 .3 ) \u76f8\u95a2\u4fc2\u6570\u306e\u53ef\u8996\u5316 \u00b6 train_df . corr () . style . background_gradient ( axis = None ) \u5217\u306b\u9805\u76ee\u5225\u306e\u30b0\u30e9\u30d5\u3092\u540c\u6642\u306b\u8868\u793a\u3059\u308b \u00b6 sns.relplot ( data = ASFR_graph, x = 'age' , y = 'ASFR' , col = \"period\" , hue = 'country' , kind = 'line' ) x\u8ef8\u30e9\u30d9\u30eb\u306e\u56de\u8ee2 \u00b6 fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = 'home' , y = 'y' , data = train_new ) labels = ax . get_xticklabels () plt . setp ( labels , rotation = 90 ); \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e\u8868\u793a\uff08\u8907\u6570\u30b0\u30e9\u30d5\u3092\u91cd\u306d\u3066\u8868\u793a\uff09 \u00b6 sns . distplot ( df [ \"Temperature\" ], bins = 20 , label = \"2018\" ) sns . distplot ( df [ \"Temperature\" ], bins = 20 , label = \"2019\" ) plt . legend ( loc = 0 ) plt . title ( \"Temperature\" ) sns . despine ( left = True ) \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e\u8868\u793a\uff08\u6a2a\u306b\u4e26\u3079\u3066\u8868\u793a\uff09 \u00b6 sns . set ( font_scale = 1.1 ) sns . set_style ( 'white' ) fig = plt . figure ( figsize = ( 30 , 5 )) temp = [ 'temp_0' , \"temp_1\" , \"temp_2\" ] for i in range ( len ( temp )): le = fig . add_subplot ( 1 , 3 , i + 1 ) sns . distplot ( df [ temp [ i ]], bins = 20 ) plt . title ( \"Temperature\" ) sns . despine ( left = True ) \u6563\u5e03\u56f3\u306e\u8868\u793a\uff08\u6a2a\u306b\u8868\u793a/\u57fa\u6e96\u7dda\u306e\u8868\u793a\uff09 \u00b6 sns . set ( font_scale = 1.3 ) sns . set_style ( 'white' ) sns . set_style ( { \"xtick.major.size\" : 8 , \"ytick.major.size\" : 8 }) g = sns . FacetGrid ( df , col = \"GROUP\" , height = 8 , legend_out = True ) g . map_dataframe ( sns . scatterplot , x = \"ratio\" , y = \"temp\" , hue = \"GROUP\" ) g . set_titles ( col_template = \" {col_name} \" , row_template = \" {row_name} \" ) g . set ( xlim = ( 4 , 10 ), ylim = ( - 150 , - 140 ), xlabel = \"label\" , ylabel = \"label\" ) g . add_legend () # HMB\u306e\u5024\u7834\u7dda\u8a18\u8f09 for ( row_val ), ax in g . axes_dict . items (): ax . axhline ( y = 5 , linestyle = '--' , color = \"r\" ), ax . axvline ( x = 5 , linestyle = '--' , color = \"r\" ) \u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u8868\u793a\u95a2\u6570 \u00b6 def point_view ( df ): sns . set ( font_scale = 1.3 ) #sns.set_style('white') fig = plt . figure ( figsize = ( 23 , 4 )) f = sns . lineplot ( data = df , x = df . index , y = 'Temperature' , color = \"y\" , alpha = .8 , label = 'Temperature' ) ax1 . xaxis . set_major_locator ( mdates . HourLocator ( byhour = None , interval = 1 , tz = None )) ax1 . xaxis . set_major_formatter ( mdates . DateFormatter ( \"%H:%M\" )) #xaxis_ = ax1.xaxis #xaxis_.set_major_formatter(DateFormatter('%H:%M:%')) ax1 . set_ylabel ( 'Temperature' ) labels = ax1 . get_xticklabels () plt . setp ( labels , rotation = 45 ); \u30dc\u30c3\u30af\u30b9\u30d7\u30ed\u30c3\u30c8\u306e\u8868\u793a \u00b6 train_new [ \"match_flag\" ] = train_new [ \"match\" ] . str [: - 3 ] fig , ax = plt . subplots ( figsize = ( 20 , 10 )) sns . boxplot ( x = 'match_flag' , y = 'y' , data = train_new , sym = \"\" ) labels = ax . get_xticklabels () plt . setp ( labels , rotation = 90 ); yy\u30d7\u30ed\u30c3\u30c8\u306e\u8868\u793a \u00b6 # yyplot \u4f5c\u6210\u95a2\u6570 def yyplot ( y_obs , y_pred ): yvalues = np . concatenate ([ y_obs , y_pred ]) ymin , ymax , yrange = np . amin ( yvalues ), np . amax ( yvalues ), np . ptp ( yvalues ) fig = plt . figure ( figsize = ( 8 , 8 )) plt . scatter ( y_obs , y_pred ) plt . plot ([ ymin - yrange * 0.01 , ymax + yrange * 0.01 ], [ ymin - yrange * 0.01 , ymax + yrange * 0.01 ]) plt . xlim ( ymin - yrange * 0.01 , ymax + yrange * 0.01 ) plt . ylim ( ymin - yrange * 0.01 , ymax + yrange * 0.01 ) plt . xlabel ( 'y_observed' , fontsize = 24 ) plt . ylabel ( 'y_predicted' , fontsize = 24 ) plt . title ( 'Observed-Predicted Plot' , fontsize = 24 ) plt . tick_params ( labelsize = 16 ) plt . show () return fig Folium\u306b\u3088\u308b\u5730\u56f3\u4e0a\u306e\u53ef\u8996\u5316 \u00b6 import folium def visualize_locations ( df , zoom = 10 ): # \u56f3\u306e\u5927\u304d\u3055\u3092\u6307\u5b9a\u3059\u308b\u3002 f = folium . Figure ( width = 1000 , height = 500 ) # \u521d\u671f\u8868\u793a\u306e\u4e2d\u5fc3\u306e\u5ea7\u6a19\u3092\u6307\u5b9a\u3057\u3066\u5730\u56f3\u3092\u4f5c\u6210\u3059\u308b\u3002 center_lat = 38.9 center_lon =- 77 m = folium . Map ([ center_lat , center_lon ], zoom_start = zoom ) . add_to ( f ) # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u5168\u3066\u306e\u884c\u306e\u30de\u30fc\u30ab\u30fc\u3092\u4f5c\u6210\u3059\u308b\u3002 for i in range ( 0 , len ( df )): folium . Marker ( location = [ df [ \"LATITUDE_GROUP\" ][ i ], df [ \"LONGITUDE_GROUP\" ][ i ]]) . add_to ( m ) #for i in range(0,len(df)): #folium.Marker(location=[df[\"LATITUDE_GROUP\"][i],df[\"LONGITUDE_GROUP\"][i]], #icon=folium.Icon(color=\"orange\", icon=\"info-sign\")).add_to(m) return m visualize_locations ( temp_df_fol )","title":"Matplot/Seaborn Basic"},{"location":"visualize_basic/#matplotseaborn-basic","text":"","title":"Matplot/Seaborn Basic"},{"location":"visualize_basic/#_1","text":"sns . set ( font_scale = 1.1 ) sns . set_style ( 'white' ) fig = plt . figure ( figsize = ( 30 , 5 )) plt . figure ( figsize = ( 8 , 8 ))","title":"\u8868\u793a\u57fa\u672c\u8a2d\u5b9a"},{"location":"visualize_basic/#_2","text":"congress80_dem = congress80 [ congress80 [ 'party' ] == 'Democrat' ] .copy () congress80_rep = congress80 [ congress80 [ 'party' ] == 'Republican' ] .copy () #\u30b0\u30e9\u30d5\u306e\u63cf\u753b\u7bc4\u56f2\u3092\u8a2d\u5b9a\u3059\u308b fig, ax = plt.subplots () plt.xlim ( -1.5, 2 ) plt.ylim ( -1.5, 1 .5 ) # Democrat\u306e\u6563\u5e03\u56f3\u3092\u4f5c\u308b plt.scatter ( congress80_dem [ 'dwnom1' ] , congress80_dem [ 'dwnom2' ] , c = \"r\" , alpha = 0 .3 ) # Republican\u306e\u6563\u5e03\u56f3\u3092\u4f5c\u308b plt.scatter ( congress80_rep [ 'dwnom1' ] , congress80_rep [ 'dwnom2' ] , c = \"b\" , alpha = 0 .3 )","title":"\u8907\u6570\u30b0\u30e9\u30d5\u306e\u63cf\u753b"},{"location":"visualize_basic/#_3","text":"train_df . corr () . style . background_gradient ( axis = None )","title":"\u76f8\u95a2\u4fc2\u6570\u306e\u53ef\u8996\u5316"},{"location":"visualize_basic/#_4","text":"sns.relplot ( data = ASFR_graph, x = 'age' , y = 'ASFR' , col = \"period\" , hue = 'country' , kind = 'line' )","title":"\u5217\u306b\u9805\u76ee\u5225\u306e\u30b0\u30e9\u30d5\u3092\u540c\u6642\u306b\u8868\u793a\u3059\u308b"},{"location":"visualize_basic/#x","text":"fig , ax = plt . subplots ( figsize = ( 10 , 10 )) sns . boxplot ( x = 'home' , y = 'y' , data = train_new ) labels = ax . get_xticklabels () plt . setp ( labels , rotation = 90 );","title":"x\u8ef8\u30e9\u30d9\u30eb\u306e\u56de\u8ee2"},{"location":"visualize_basic/#_5","text":"sns . distplot ( df [ \"Temperature\" ], bins = 20 , label = \"2018\" ) sns . distplot ( df [ \"Temperature\" ], bins = 20 , label = \"2019\" ) plt . legend ( loc = 0 ) plt . title ( \"Temperature\" ) sns . despine ( left = True )","title":"\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e\u8868\u793a\uff08\u8907\u6570\u30b0\u30e9\u30d5\u3092\u91cd\u306d\u3066\u8868\u793a\uff09"},{"location":"visualize_basic/#_6","text":"sns . set ( font_scale = 1.1 ) sns . set_style ( 'white' ) fig = plt . figure ( figsize = ( 30 , 5 )) temp = [ 'temp_0' , \"temp_1\" , \"temp_2\" ] for i in range ( len ( temp )): le = fig . add_subplot ( 1 , 3 , i + 1 ) sns . distplot ( df [ temp [ i ]], bins = 20 ) plt . title ( \"Temperature\" ) sns . despine ( left = True )","title":"\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e\u8868\u793a\uff08\u6a2a\u306b\u4e26\u3079\u3066\u8868\u793a\uff09"},{"location":"visualize_basic/#_7","text":"sns . set ( font_scale = 1.3 ) sns . set_style ( 'white' ) sns . set_style ( { \"xtick.major.size\" : 8 , \"ytick.major.size\" : 8 }) g = sns . FacetGrid ( df , col = \"GROUP\" , height = 8 , legend_out = True ) g . map_dataframe ( sns . scatterplot , x = \"ratio\" , y = \"temp\" , hue = \"GROUP\" ) g . set_titles ( col_template = \" {col_name} \" , row_template = \" {row_name} \" ) g . set ( xlim = ( 4 , 10 ), ylim = ( - 150 , - 140 ), xlabel = \"label\" , ylabel = \"label\" ) g . add_legend () # HMB\u306e\u5024\u7834\u7dda\u8a18\u8f09 for ( row_val ), ax in g . axes_dict . items (): ax . axhline ( y = 5 , linestyle = '--' , color = \"r\" ), ax . axvline ( x = 5 , linestyle = '--' , color = \"r\" )","title":"\u6563\u5e03\u56f3\u306e\u8868\u793a\uff08\u6a2a\u306b\u8868\u793a/\u57fa\u6e96\u7dda\u306e\u8868\u793a\uff09"},{"location":"visualize_basic/#_8","text":"def point_view ( df ): sns . set ( font_scale = 1.3 ) #sns.set_style('white') fig = plt . figure ( figsize = ( 23 , 4 )) f = sns . lineplot ( data = df , x = df . index , y = 'Temperature' , color = \"y\" , alpha = .8 , label = 'Temperature' ) ax1 . xaxis . set_major_locator ( mdates . HourLocator ( byhour = None , interval = 1 , tz = None )) ax1 . xaxis . set_major_formatter ( mdates . DateFormatter ( \"%H:%M\" )) #xaxis_ = ax1.xaxis #xaxis_.set_major_formatter(DateFormatter('%H:%M:%')) ax1 . set_ylabel ( 'Temperature' ) labels = ax1 . get_xticklabels () plt . setp ( labels , rotation = 45 );","title":"\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u8868\u793a\u95a2\u6570"},{"location":"visualize_basic/#_9","text":"train_new [ \"match_flag\" ] = train_new [ \"match\" ] . str [: - 3 ] fig , ax = plt . subplots ( figsize = ( 20 , 10 )) sns . boxplot ( x = 'match_flag' , y = 'y' , data = train_new , sym = \"\" ) labels = ax . get_xticklabels () plt . setp ( labels , rotation = 90 );","title":"\u30dc\u30c3\u30af\u30b9\u30d7\u30ed\u30c3\u30c8\u306e\u8868\u793a"},{"location":"visualize_basic/#yy","text":"# yyplot \u4f5c\u6210\u95a2\u6570 def yyplot ( y_obs , y_pred ): yvalues = np . concatenate ([ y_obs , y_pred ]) ymin , ymax , yrange = np . amin ( yvalues ), np . amax ( yvalues ), np . ptp ( yvalues ) fig = plt . figure ( figsize = ( 8 , 8 )) plt . scatter ( y_obs , y_pred ) plt . plot ([ ymin - yrange * 0.01 , ymax + yrange * 0.01 ], [ ymin - yrange * 0.01 , ymax + yrange * 0.01 ]) plt . xlim ( ymin - yrange * 0.01 , ymax + yrange * 0.01 ) plt . ylim ( ymin - yrange * 0.01 , ymax + yrange * 0.01 ) plt . xlabel ( 'y_observed' , fontsize = 24 ) plt . ylabel ( 'y_predicted' , fontsize = 24 ) plt . title ( 'Observed-Predicted Plot' , fontsize = 24 ) plt . tick_params ( labelsize = 16 ) plt . show () return fig","title":"yy\u30d7\u30ed\u30c3\u30c8\u306e\u8868\u793a"},{"location":"visualize_basic/#folium","text":"import folium def visualize_locations ( df , zoom = 10 ): # \u56f3\u306e\u5927\u304d\u3055\u3092\u6307\u5b9a\u3059\u308b\u3002 f = folium . Figure ( width = 1000 , height = 500 ) # \u521d\u671f\u8868\u793a\u306e\u4e2d\u5fc3\u306e\u5ea7\u6a19\u3092\u6307\u5b9a\u3057\u3066\u5730\u56f3\u3092\u4f5c\u6210\u3059\u308b\u3002 center_lat = 38.9 center_lon =- 77 m = folium . Map ([ center_lat , center_lon ], zoom_start = zoom ) . add_to ( f ) # \u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306e\u5168\u3066\u306e\u884c\u306e\u30de\u30fc\u30ab\u30fc\u3092\u4f5c\u6210\u3059\u308b\u3002 for i in range ( 0 , len ( df )): folium . Marker ( location = [ df [ \"LATITUDE_GROUP\" ][ i ], df [ \"LONGITUDE_GROUP\" ][ i ]]) . add_to ( m ) #for i in range(0,len(df)): #folium.Marker(location=[df[\"LATITUDE_GROUP\"][i],df[\"LONGITUDE_GROUP\"][i]], #icon=folium.Icon(color=\"orange\", icon=\"info-sign\")).add_to(m) return m visualize_locations ( temp_df_fol )","title":"Folium\u306b\u3088\u308b\u5730\u56f3\u4e0a\u306e\u53ef\u8996\u5316"},{"location":"web_scrp/","text":"Web Scrping \u00b6 gmail\u9001\u4fe1\u3000\u30b3\u30de\u30f3\u30c9 \u00b6 import smtplib import os from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.application import MIMEApplication def send_mail_attach ( from_addr , passward , to_address , subject , body , file_path = None ): stmp_server = \"smtp.gmail.com\" stmp_port = 587 stmp_user = from_addr stmp_password = passward to_address = to_address from_address = stmp_user subject = subject contents = { 'body' : body } body = \"\"\" <html> <body> <p> {body} </p> </body> </html>\"\"\" . format ( ** contents ) filepath = file_path filename = os . path . basename ( filepath ) msg = MIMEMultipart () msg [ \"Subject\" ] = subject msg [ \"From\" ] = from_address msg [ \"To\" ] = to_address msg . attach ( MIMEText ( body , \"html\" )) with open ( filepath , \"rb\" ) as f : mb = MIMEApplication ( f . read ()) mb . add_header ( \"Content-Disposition\" , \"attachment\" , filename = filename ) msg . attach ( mb ) s = smtplib . SMTP ( stmp_server , stmp_port ) s . starttls () s . login ( stmp_user , stmp_password ) s . sendmail ( from_address , to_address , msg . as_string ()) s . quit () \u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u5b9f\u884c \u30b3\u30de\u30f3\u30c9 \u00b6 pip install schedule import schedule import time # \u5b9f\u884cjob\u95a2\u6570 def job (): print ( \"job\u5b9f\u884c\" #1\u5206\u6bce\u306ejob\u5b9f\u884c\u3092\u767b\u9332 schedule . every ( 1 ) . minutes . do ( job ) #1\u6642\u9593\u6bce\u306ejob\u5b9f\u884c\u3092\u767b\u9332 schedule . every ( 1 ) . hours . do ( job ) #AM11:00\u306ejob\u5b9f\u884c\u3092\u767b\u9332 schedule . every () . day . at ( \"11:00\" ) . do ( job ) #\u65e5\u66dc\u65e5\u306ejob\u5b9f\u884c\u3092\u767b\u9332 schedule . every () . sunday . do ( job ) #\u6c34\u66dc\u65e513:15\u306ejob\u5b9f\u884c\u3092\u767b\u9332 schedule . every () . wednesday . at ( \"13:15\" ) . do ( job ) # job\u306e\u5b9f\u884c\u76e3\u8996\u3001\u6307\u5b9a\u6642\u9593\u306b\u306a\u3063\u305f\u3089job\u95a2\u6570\u3092\u5b9f\u884c while True : schedule . run_pending () time . sleep ( 1 )","title":"Web Scriping"},{"location":"web_scrp/#web-scrping","text":"","title":"Web Scrping"},{"location":"web_scrp/#gmail","text":"import smtplib import os from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.application import MIMEApplication def send_mail_attach ( from_addr , passward , to_address , subject , body , file_path = None ): stmp_server = \"smtp.gmail.com\" stmp_port = 587 stmp_user = from_addr stmp_password = passward to_address = to_address from_address = stmp_user subject = subject contents = { 'body' : body } body = \"\"\" <html> <body> <p> {body} </p> </body> </html>\"\"\" . format ( ** contents ) filepath = file_path filename = os . path . basename ( filepath ) msg = MIMEMultipart () msg [ \"Subject\" ] = subject msg [ \"From\" ] = from_address msg [ \"To\" ] = to_address msg . attach ( MIMEText ( body , \"html\" )) with open ( filepath , \"rb\" ) as f : mb = MIMEApplication ( f . read ()) mb . add_header ( \"Content-Disposition\" , \"attachment\" , filename = filename ) msg . attach ( mb ) s = smtplib . SMTP ( stmp_server , stmp_port ) s . starttls () s . login ( stmp_user , stmp_password ) s . sendmail ( from_address , to_address , msg . as_string ()) s . quit ()","title":"gmail\u9001\u4fe1\u3000\u30b3\u30de\u30f3\u30c9"},{"location":"web_scrp/#_1","text":"pip install schedule import schedule import time # \u5b9f\u884cjob\u95a2\u6570 def job (): print ( \"job\u5b9f\u884c\" #1\u5206\u6bce\u306ejob\u5b9f\u884c\u3092\u767b\u9332 schedule . every ( 1 ) . minutes . do ( job ) #1\u6642\u9593\u6bce\u306ejob\u5b9f\u884c\u3092\u767b\u9332 schedule . every ( 1 ) . hours . do ( job ) #AM11:00\u306ejob\u5b9f\u884c\u3092\u767b\u9332 schedule . every () . day . at ( \"11:00\" ) . do ( job ) #\u65e5\u66dc\u65e5\u306ejob\u5b9f\u884c\u3092\u767b\u9332 schedule . every () . sunday . do ( job ) #\u6c34\u66dc\u65e513:15\u306ejob\u5b9f\u884c\u3092\u767b\u9332 schedule . every () . wednesday . at ( \"13:15\" ) . do ( job ) # job\u306e\u5b9f\u884c\u76e3\u8996\u3001\u6307\u5b9a\u6642\u9593\u306b\u306a\u3063\u305f\u3089job\u95a2\u6570\u3092\u5b9f\u884c while True : schedule . run_pending () time . sleep ( 1 )","title":"\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u5b9f\u884c \u30b3\u30de\u30f3\u30c9"}]}